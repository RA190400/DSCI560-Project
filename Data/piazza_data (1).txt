DSCI 552: Machine Learning for Data
Science (Fall 2023)
Units: 4
Instructor: Mohammad Reza Rajati, PhD
PHE 412
rajati@usc.edu { Include DSCI 552 in subject.
Oce Hours: Right after the lecture, by appointment
Webpage: Personal Homepage at Intelligent Decision Analysis
TA(s): Will be introduced on Piazza.
Lecture 1: Monday, Wednesday, 12:00 pm {1:50 pm SGM 123 & Online
Lecture 2: Monday, Wednesday, 3:30 pm {5:20 pm THH 201 & Online
Webpages: Piazza Class Page for discussions, announcements, and course materials
and USC DEN Class Page for exams and grades
and GitHub for code submission
{ All HWs, handouts, solutions will be posted in PDF format
{Student has the responsibility to stay current with webpage material
Prerequisite: Prior courses in multivariate calculus, linear algebra, probability, and statistics.
{ This course is a prerequisite to DSCI 558.
Other Requirements: Computer programming skills.
Using Python is mandatory.
Students must know Python or must be willing to learn it.
Tentative Grading: Assignments 45%
Midterm 1 20%
Midterm 2 25%
Final Project 10%
Participation on Piazza* 5%
Letter Grade Distribution:
93.00 A 73.00 - 76.99 C
90.00 - 92.99 A- 70.00 - 72.99 C-
87.00 - 89.99 B+ 67.00 - 69.99 D+
83.00 - 86.99 B 63.00 - 66.99 D
80.00 - 82.99 B- 60.00 - 62.99 D-
77.00 - 79.99 C+ 59.99 F2 DSCI 552 Syllabus { August 23, 2023
Disclaimer: Although the instructor does not expect this syllabus to drastically change, he
reserves every right to change this syllabus any time in the semester.
Note on e-mail vs. Piazza: If you have a question about the material or logistics of the class
and wish to ask it electronically, please post it on the piazza page (not e-mail). Often times, if one
student has a question/comment, other also have a similar question/comment. Use private Piazza
posts with the professor, TA, graders only for issues that are specic to your individually (e.g., a
scheduling issue or grade issue). Minimize the use of email to the course sta and only use it when
absolutely necessary .
Catalogue Description: Practical applications of machine learning techniques to real-world
problems. Uses in data mining and recommendation systems and for building adaptive user inter-
faces.
Course Description: This is a foundational course with the primary application to data analyt-
ics, but is intended to be accessible both to students from technical backgrounds such as computer
science, computer engineering, electrical engineering, or mathematics; and to students from less
technical backgrounds such as business administration, communication, accounting, various medi-
cal specializations including preventative medicine and personalized medicine, genomics, and man-
agement information systems. A basic understanding of engineering and/or technology principles
is needed, as well as basic programming skills, sucient mathematical background in probability,
statistics, and linear algebra.
Course Objectives: Upon successful completion of this course a student will
Broadly understand major algorithms used in machine learning.
Understand supervised and unsupervised learning techniques.
Understand regression methods.
Understand resampling methods, including cross-validation and bootstrap.
Understand decision trees, dimensionality reduction, regularization, clustering, and kernel
methods.
Understand hidden Markov models and graphical models.
Understand feedforward and recurrent neural networks and deep learning.
Exam Dates:
Midterm 1 (in-person): Friday October 20, 10:00 AM-11:50 AM. (May be changed to a a
dierent hour on the same day)
Midterm 2 (in-person): Friday, December 1, 10:00 AM - 11:50 AM (May be changed to a
dierent hour on the same day)DSCI 552 Syllabus { August 23, 2023 3
Final Project Due: Monday, December 11, 4:00 PM. Grace period : the project can be
submitted until 11:59 PM of the same day with 30% penalty. Any change in the project after
the deadline is considered late submission. One second late is late. The project is graded
based on when it was submitted, not when it was nished. Homework late days cannot be
used for the project.
Textbooks:
Required Textbook:
1. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction
to Statistical Learning with Applications in R , Springer, 2021. (ISLR)
Available at https://web.stanford.edu/ ~hastie/ISLRv2_website.pdf
Recommended Textbooks:
1. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An Introduction
to Statistical Learning with Applications in Python , Springer, 2023.
Available at https://hastie.su.domains/ISLP/ISLP_website.pdf
2.Applied Predictive Modeling , 1stEdition
Authors: Max Kuhn and Kjell Johnson; Springer; 2016. ISBN-13: 978-1-4614-6848-6
3.Machine Learning: A Concise Introduction , 1stEdition
Author: Steven W. Knox; Wiley; 2018. ISBN-13: 978-1-119-43919-6
4.The Elements of Statistical Learning: Data Mining, Inference, and Prediction , 2ndEdi-
tion
Authors: Trevor Hastie, Robert Tibshirani, and Jerome Friedman; Springer; 2008.
(ESL) ISBN-13: 978-0387848570
5.Machine Learning: An Algorithmic Perspective , 2ndEdition
Author: Stephen Marsland; CRC Press; 2014. ISBN-13: 978-1-4614-7137-0
6.Deep Learning , 1stEdition
Authors: Ian Goodfellow, Yoshua Bengio, and Aaron Courville; MIT Press; 2016. (DL)
ISBN-13: 978-0262035613
7.Neural Networks and Learning Machines , 3rdEdition
Author: Simon Haykin; Pearson; 2008. ISBN-13: 978-0131471399
8.Neural Networks and Deep Learning: A Textbook , 1stEdition
Authors: Charu Aggrawal; Springer; 2018. ISBN-13: 978-3319944623
9.Introduction to Machine Learning , 2ndEdition
Author: Ethem Alpaydine; MIT Press; 2010. (AL) ISBN-13: 978-8120350786
10.Machine Learning , 1stEdition
Authors: Tom M. Mitchell; McGraw-Hill Education; 1997. ISBN-13: 978-00704280724 DSCI 552 Syllabus { August 23, 2023
Grading Policies:
The letter grade distribution table guarantees the minimum grade each student will receive
based on their nal score. When appropriate, relative performance measures will be used to
assign the nal grade, at the discretion of the instructor.
{Final grades are non-negotiable and are assigned at the discretion of the instructor. If
you cannot accept this condition, you should not enroll in this course.
Your lowest homework grade and half of your second lowest homework grade will be dropped
from the nal grade. For example, if you received 90, 85, 10, 95, 65, 80, 100, 100 your home-
work score will be0:565+80+85+90+95+100+100
6:5= 89:62 instead of10+65+80+85+90+95+100+100
8=
78:13. This policy makes up for missing assignments because of heavy workload, sickness,
etc. Remember that if you miss an assignment because of heavy workload in other courses
and then miss another one because of sickness, only the second assignment's grade will be
completely dropped from your score. Be aware of this when you decide not to submit an
assignment, because later you may become sick.
Homework 0 will not be graded.
*Participation on Piazza has up to 5% extra credit, which is granted on a competitive basis
at the discretion of the instructor .
Homework Policy
{Homework is assigned on an approximately biweekly basis. Homework due dates are
mentioned in the course outline, so mark your calendars. A three-day grace period can
be used for each homework with 10% penalty per day. Any change in homework after
the deadline makes it a late submission. Absolutely no late homework will be accepted
after the grace period. A late assignment results in a zero grade.
{Late Days: No late homework will be accepted after the three day grace period. One
second after the deadline is considered late. However, students are allowed to use sixlate
days for homework for any reason (including sickness, family emergencies, overwhelming
workload, exams, etc) without incurring the 10% penalty . Beyond that, no individual
extension will be granted to anyone for any reason whatsoever.
Example : A student can submit six assignments, one day late each, without any penalty.
Or three assignments, two days late each, without penalty, or two assignments three days
late each. A student cannot use four late days for one assignment, and two late days for
another assignment. An assignment submitted four days late will receive a zero grade,
although its grade will be dropped as the lowest homework grade, according to the above
grading policies.
{Use your six late days strategically and only if you absolutely need them. Always
remember that later in the semester, you might become sick or have heavy workload in
other courses and might need to use your late days.
{Assignments are project-style ; therefore, we do not provide solutions to the assignments.
This is a rm rule.
{Poor internet connection, failing to upload properly, or similar issues are NOT acceptable
reasons for late submissions. If you want to make sure that you do not have suchDSCI 552 Syllabus { August 23, 2023 5
problems, submit homework eight hours earlier than the deadline. Please do not ask the
instructor to make individual exceptions.
{Homework is graded based on when it was submitted, not when it was nished.
{Homework solutions and simulation results should be typed or scanned using scanners
or mobile scanner applications like CamScan and uploaded (photos taken by cell-phone
cameras and in formats other than pdf will NOT be accepted). Programs and simulation
results have to be uploaded on GitHub as well.
{Students are encouraged to discuss homework problems with one another, but each
student must do their own work and submit individual solutions written/ coded in
their own hand. Copying the solutions or submitting identical homework sets is written
evidence of cheating. The penalty ranges from F on the homework or exam, to an F
in the course, to recommended expulsion. One important (but not exclusive) instance
of cheating is having access to other students' solutions. Claims of \being inspired"
by other students' codes, or using them as \sample code" are not acceptable. Asking
questions from your peers and exchanging tips about coding are highly encouraged and
should not be confused with outright cheating.
{Posting the homework assignments and their solutions to online forums or sharing them
with other students is strictly prohibited and infringes the copyright of the instructor.
Instances will be reported to USC ocials as academic dishonesty for disciplinary action.
Exam Policy
{ Make-up Exams: No make-up exams will be given. If you cannot make the above
dates due to a class schedule conict or personal matter, you must drop the class. In
the case of a required business trip or a medical emergency, a signed letter from your
manager or physician has to be submitted. This letter must include the contact of your
physician or manager.
{An excused absence supported by documents in the rst midterm can be made up by
using the second midterm's grade in lieu of the rst midterm. An excused absence in
the second midterm results in an IN (incomplete) grade.
{Exams will be closed book and notes. Calculators are allowed but computers and cell-
phones or using any devices that have internet capability are not allowed, except for
writing the solutions or being proctored are not allowed. One letter size cheat sheet
(back and front) is allowed for Midterm 1. Two letter size cheat sheets (back and front)
are allowed for Midterm 2.
{All exams are cumulative, with an emphasis on material presented since the last exam.
{For several reasons, including unauthorized circulation of previous exams, we DO NOT
provide exam solutions. This is a rm rule.
{For several reasons, including the dicult logistics of dealing with a large class, we
may not be able to hold a regrading session for the exams. Please make sure that you
understand this rule when you take this course.
Project
{The nal project is more like a slightly extended Homework that will be assigned after
Midterm 2 as the nal summative experience.6 DSCI 552 Syllabus { August 23, 2023
{The project topic and steps will be provided to students, similar to homework assign-
ments.
{Projects must be nished individually .
{A short grace period of a few hours after the project deadline will be given to students
for 30% penalty. Late submissions will be graded zero. One second late is late.
{Project is graded based on when it was submitted, not when it was nished.
{Homework late days cannot be used for project in any circumstances.
Attendance:
{Students are required to attend all the lectures and discussion sessions and actively par-
ticipate in class discussions. Use of cellphones and laptops is prohibited in the classroom.
If you need your electronic devices to take notes, you should discuss with the instructor
at the beginning of the semester.
Important Notes :
Textbooks are secondary to the lecture notes and homework assignments.
Handouts and course material will be distributed.
Please use your USC email to register on Piazza and to contact the instructor and TAs.DSCI 552 Syllabus { August 23, 2023 7
Monday Wednesday
Aug 21st 1
Introduction to Statistical Learning
(ISLR Chs.1,2, ESL Chs.1,2)
Motivation: Big Data
Supervised vs. Unsupervised Learning23rd 2
Introduction to Statistical Learning
(ISLR Chs.1,2, ESL Chs.1,2)
Regression, Classication
The Regression Function
Nearest Neighbors
28th 3
Introduction to Statistical Learning
(ISLR Chs.1,2, ESL Chs.1,2)
Model Assessment
The Bias-Variance Trade-o
No Free Lunch Theorem30th 4
Linear Regression (ISLR Ch.3, ESL Ch. 3)
Estimating Coecients
Estimating the Accuracy of Coecients
Sep 4th
Labor Day6th 5
Linear Regression (ISLR Ch.3, ESL Ch. 3)
Variable Selection and Hypothesis Testing
Multiple Regression
Analysis of Variance and the F Test
11th 6
Linear Regression (ISLR Ch.3, ESL Ch. 3)
Stepwise Variable Selection
Qualitative Variables13th 7
Classication (ISLR Ch. 4, ESL Ch. 4)
Multi-class and Multi-label Classication
Logistic Regression
Class Imbalance
Hypothesis Testing and Variable Selection
18th 8
Classication (ISLR Ch. 4, ESL Ch. 4)
Subsampling and Upsampling
SMOTE
Multinomial Regression
Bayesian Linear Discriminant Analysis20th 9
Classication (ISLR Ch. 4, ESL Ch. 4)
Measures for Evaluating Classiers
Quadratic Discriminant Analysis*
Comparison with K-Nearest Neighbors
The Na ve Bayes' Classier
Text Classication
Feature Creation for Text Data
Handling Missing Data
25th 10
Resampling Methods (ISLR Ch. 5, ESL
Ch. 7)
Model Assessment
Validation Set Approach
Cross-Validation
The Bias-Variance Trade-o for
Cross-Validation
The Bootstrap
Bootstrap Condence Intervals27th 11
Linear Model Selection and
Regularization (ISLR Ch.6, ESL Ch. 3)
Subset Selection
AIC, BIC, and Adjusted R2)
Shrinkage Methods
Ridge Regression8 DSCI 552 Syllabus { August 23, 2023
Monday Wednesday
Oct 2nd 12
Linear Model Selection and
Regularization (ISLR Ch.6, ESL Ch. 3)
The LASSO
Elastic Net
Dimension Reduction Methods*4th 13
Tree-based Methods (ISLR Ch. 8, ESL
Chs. 9, 10)
Regression and Classication Trees
Cost Complexity Pruning
9th 14
Tree-based Methods (ISLR Ch. 8, ESL
Chs. 9, 10, 16)
Bagging, Random Forests, and Boosting11th 15
Support Vector Machines (ISLR Ch. 9,
ESL Ch. 12)
Maximal Margin Classier
Support Vector Classiers
16th 16
Support Vector Machines (ISLR Ch. 9,
ESL Ch. 12)
The Kernel Trick
Support Vector Machines
L1 Regularized SVMs
Multi-class and Multilabel Classication
The Vapnik-Chervonenkis Dimension*
Support Vector Regression18th 17
Unsupervised Learning (ISLR Ch. 12,
ESL Ch. 14)
K-Means Clustering
Hierarchical Clustering
23rd 18
Unsupervised Learning (ISLR Ch. 12,
ESL Ch. 14)
Practical Issues in Clustering25th 19
Unsupervised Learning (ISLR Ch. 12,
ESL Ch. 14)
Principal Component Analysis
Anomaly Detection*
Association Rules*
Mixture Models and Soft K-Means*
30th 20
Active and Semi-Supervised Learning
Semi-Supervised Learning
Self-Training
Co-Training
Yarowsky Algorithm
Renements
Active vs. Passive Learning
Stream-Based vs. Pool-Based Active Learning
Query Selection StrategiesNov 1st 21
Neural Networks and Deep Learning
(ISLR Ch. 10, ESL Ch. 11, DL Ch. 6)
The Perceptron
Feedforward Neural Networks
Backpropagation and Gradient Descent
OverttingDSCI 552 Syllabus { August 23, 2023 9
Monday Wednesday
6th 22
Neural Networks and Deep Learning
(DL Chs. 6, 7)
Autoencoders and Deep Feedforward Neural
Networks
Regularization
Early Stopping and Dropout
Adversarial Training*8th 23
Neural Networks and Deep Learning
(ISLR Ch. 12, DL Chs. 9, 10)
Convolutional Neural Networks
Sequence Modeling
Recurrent Neural Networks
13th 24
Neural Networks and Deep Learning
(ISLR Ch. 12, DL Ch. 10)
Sequence-to-Sequence Modeling*
Long Short Term Memory (LSTM) Neural
Networks15th 25
Hidden Markov Models (AL Ch. 15)
Principles
The Viterbi Algorithm
20th 26
Reinforcement Learning *
Denitions
Task-Reward-Policy Formulation
Total Discounted Future Reward
Optimal Policy
Value Function
Q-Function
The Bellman Equation
Q-Learning
Exploration- Exploitation
Temporal Dierence Learning
Extensions to Stochastic Environments and
Rewards
Deep Reinforcement Learning22nd
Thanksgiving Break
27th 27
Fuzzy Systems*
Fuzzy Sets
Set Operations
T-norms, T-conorms, and Fuzzy complements
Cylindrical Extensions and Fuzzy Relations
Fuzzy If-Then Rules as Association Rules29th 28
Fuzzy Systems*
Inference from Fuzzy Rules
Fuzzication and Defuzzication
Learning Fuzzy Rules from Examples
The Wang-Mendel Algorithm
Fuzzy C-Means Clustering
Notes:
Items marked by * will be covered only if time permits.10 DSCI 552 Syllabus { August 23, 2023
Homework Due Dates & Exams
Friday
Aug 25th 1
-
Sep 1st 2
-
8th 3
Homework 0 Due (not graded)
15th 4
Homework 1 Due
22nd 5
Homework 2 Due
29th 6
-
Oct 6th 7
Homework 3 Due
13th 8
Homework 4 Due (Moved to Monday Oct. 16)
20th 9
[Midterm 1]
27th 10
Homework 5 Due
Nov 3rd 11
Homework 6 Due
10th 12
Homework 7 Due (Moved to Monday Nov. 13)
17th 13
-
24th 14
Homework 8 Due (Moved to Monday Nov. 28)
Dec 1st 15
[Midterm 2]DSCI 552 Syllabus { August 23, 2023 11
Statement on Academic Conduct and Support Systems
Academic Conduct:
Plagiarism { presenting someone else's ideas as your own, either verbatim or recast in your own
words { is a serious academic oense with serious consequences. Please familiarize yourself with the
discussion of plagiarism in SCampus in Part B, Section 11, \Behavior Violating University Stan-
dards" policy.usc.edu/scampus-part-b. Other forms of academic dishonesty are equally unaccept-
able. See additional information in SCampus and university policies on Research and Scholarship
Misconduct.
Students and Disability Accommodations:
USC welcomes students with disabilities into all of the University's educational programs. The
Oce of Student Accessibility Services (OSAS) is responsible for the determination of appropriate
accommodations for students who encounter disability-related barriers. Once a student has com-
pleted the OSAS process (registration, initial appointment, and submitted documentation) and
accommodations are determined to be reasonable and appropriate, a Letter of Accommodation
(LOA) will be available to generate for each course. The LOA must be given to each course in-
structor by the student and followed up with a discussion. This should be done as early in the
semester as possible as accommodations are not retroactive. More information can be found at
osas.usc.edu. You may contact OSAS at (213) 740-0776 or via email at osasfrontdesk@usc.edu.
Support Systems:
Counseling and Mental Health - (213) 740-9355 { 24/7 on call
studenthealth.usc.edu/counseling
Free and condential mental health treatment for students, including short-term psychotherapy,
group counseling, stress tness workshops, and crisis intervention.
National Suicide Prevention Lifeline - 1 (800) 273-8255 { 24/7 on call
suicidepreventionlifeline.org
Free and condential emotional support to people in suicidal crisis or emotional distress 24 hours
a day, 7 days a week.
Relationship and Sexual Violence Prevention Services (RSVP) - (213) 740-9355(WELL), press
\0" after hours { 24/7 on call
studenthealth.usc.edu/sexual-assault
Free and condential therapy services, workshops, and training for situations related to gender-
based harm.
Oce for Equity, Equal Opportunity, and Title IX (EEO-TIX) - (213) 740-5086
eeotix.usc.edu
Information about how to get help or help someone aected by harassment or discrimination, rights
of protected classes, reporting options, and additional resources for students, faculty, sta, visitors,
and applicants.
Reporting Incidents of Bias or Harassment - (213) 740-5086 or (213) 821-829812 DSCI 552 Syllabus { August 23, 2023
usc-advocate.symplicity.com/care report
Avenue to report incidents of bias, hate crimes, and microaggressions to the Oce for Equity, Equal
Opportunity, and Title for appropriate investigation, supportive measures, and response.
The Oce of Student Accessibility Services (OSAS) - (213) 740-0776
osas.usc.edu
OSAS ensures equal access for students with disabilities through providing academic accommoda-
tions and auxiliary aids in accordance with federal laws and university policy.
USC Campus Support and Intervention - (213) 821-4710
campussupport.usc.edu
Assists students and families in resolving complex personal, nancial, and academic issues adversely
aecting their success as a student.
Diversity, Equity and Inclusion - (213) 740-2101
diversity.usc.edu
Information on events, programs and training, the Provost's Diversity and Inclusion Council, Diver-
sity Liaisons for each academic school, chronology, participation, and various resources for students.
USC Emergency - UPC: (213) 740-4321, HSC: (323) 442-1000 { 24/7 on call
dps.usc.edu,emergency.usc.edu
Emergency assistance and avenue to report a crime. Latest updates regarding safety, including
ways in which instruction will be continued if an ocially declared emergency makes travel to
campus infeasible.
USC Department of Public Safety - UPC: (213) 740-6000, HSC: (323) 442-120 { 24/7 on call
dps.usc.edu Non-emergency assistance or information.
Oce of the Ombuds - (213) 821-9556 (UPC) / (323-442-0382 (HSC)
ombuds.usc.edu
A safe and condential place to share your USC-related issues with a University Ombuds who will
work with you to explore options or paths to manage your concern.
Occupational Therapy Faculty Practice - (323) 442-3340 or otfp@med.usc.edu
chan.usc.edu/otfp
Condential Lifestyle Redesign services for USC students to support health promoting habits and
routines that enhance quality of life and academic performance.post1: Hi Class, I updated the letter grades on USC Grading and Roster System. I also updated the letter grade distribution in @1300. Please calculate your letter grade carefully and let me know any discrepancies. Hopefully everything is fixed now. Regards, M R Rajati
post1-comment1: Thanks for the quick update
post1-comment2: Hi professor, my grade was not updated on OASIS. Could you check about that?
post1-comment2-reply1: please open a regrade request
post2: Hi, Letter grades must be available to students on Grade Report or OASIS. I calculated your total score using the following formula: $$0.45\times HW + 0.2\times Midterm1 +0.25\times Midterm2 +0.1\times Project$$ where HW is the (sum of your homework scores minus the lowest homework score minus half of the second lowest score) divided by (6.5) . Then I used the following thresholds to calculate your letter grade: B-?B60B+69A-81A86 Here is the distribution of letter grades: A312A-44B+50B6 Only one student benefitted from the 5% extra credit that is assigned based on Piazza Participation and they were notified. Super important, please read twice: Please note that in such a large class, whatever threshold I use, there will be students whose score is less than one percent short of receiving the next letter grade. Please DO NOT request changing the thresholds because you are only 0.7% or 0.3% away from, say, ...an A-. If I change the threshold, there will be people who are 0.2% away from, say, ...an A- according to the new threshold. The thresholds were devised to make the gap between recipients of letter grades as large as possible, and those gaps are still small. Please remember that I have graded this class extremely leniently, and its average is larger than 3.8, which is above an A-. Also, we will NOT entertain HW and Exam regrading requests to compensate some students' 0.3% gap from an...A-. The regrading period has already ended and your grades will be submitted to the registrar in a couple of days. If you have reason to believe that I miscalculated your letter grade (this is different from a regrading request), please contact me by Tuesday December 19, 12:00 Noon PST. It has been a pleasure to teach this class and I believe it was yet another very successful offering of DSCI 552. Happy holidays! Wish everyone a happy and healthy new year! Regards, M R Rajati 
post2-comment1: Hello, if I have a question regarding the miscalculation of my final grade, should I make a post on Piazza, or I can email Professor Rajati directly?
post2-comment1-reply1: The professor will update it soon. Please be patient.
post3: Hi Class, The response rate of course evaluations is still low. Please do me a favor and fill in your course evaluations, as they will be closing tomorrow. Thanks! Regards, M R Rajati
post4: Hi Class, You can find the final project of DSCI 552 here: https://www.dropbox.com/sh/38zygdpo6u7alwy/AAB1PJuhqF-58GZJ-oD4xl5Ya?dl=0 It's due Monday Dec 11, 4:00 PM PST. If submission is done between 4:00 PM and 11:59 PM the same day, you will incur 30% penalty. Submissions received after 11:59 PM will receive a zero. Regards, M R Rajati
post4-comment1: Are we using the 2.6GB `data.zip`?
post4-comment1-reply1: that is the only one present in https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi:10.48577/jpl.QJ9PYA
post4-comment1-reply2: I'm assuming I have to unzip the 2.6GB file, correct?
post4-comment1-reply3: I have downloaded and unzipped the 2.6GB file (which takes a long time) and there you can find the labels and tiles (JSON and png files). So yes.
post4-comment1-reply4: how long did it take you? file explorer kept crashing when I tried to extract it but it's finally started to extract successfully one of the folders ('_MACOSX')...
post4-comment1-reply5: Downloading the zip file didn't take too long but extracting the data took like an hour (maybe a bit more)
post4-comment1-reply6: We have two data.zip files of different sizes. which one to use ?The one uploaded in the dropbox is of size 1.4 GB and the file on dataverse is of size 2.6GB
post4-comment1-reply7: I noticed that too. What is the difference?
post4-comment1-reply8: Its the same dataset. The one present in dropbox was downloaded a few days back. Since then, new data has been added to the original dataset. 
post4-comment2: which dataset we are using? 
post4-comment2-reply1: the data.zip present in the link
post4-comment2-reply2: 2.6GB file and the .txt files for train, val, test will be given to us in the future
post4-comment3: The instructions say (1. (b) ii.): "The dataset includes files for splitting the data into train, test and validation.However, you will be provided by an improved version of those files when arepo is created:A. train_source_images.txtB. test_source_images.txtC. val_source_images.txt" However, there is currently no link to create a repo for the final project at this post @42. When can we expect to access these txt files?
post4-comment3-reply1: soon^tm
post4-comment3-reply2: it is now updated!
post4-comment4: It said There are 214 subframes and a total of 119920 tiles. But, there are total of 414 folders named PSP_ and ESP_. If I open each folder, it has labels and tiles folders. Are we using all those 414 folders for the project?
post4-comment4-reply1: I suspect once we get the train, test, and source .txt files it will only include the referenced 214, but can't know for sure until to repo is accessible.
post4-comment4-reply2: It makes sense, thank you!
post4-comment4-reply3:  Why don’t you start setting up the pipeline assuming you have the train, validation, and test set? You can randomly split the data, for example. M R Rajati’s phone 
post4-comment5: I do not have access to the dataset website cause I am in China. Could anyone share the dataset with me? Thanks a lot for your help.
post4-comment5-reply1: I can share it with you over slack. Does that work?
post4-comment5-reply2: That's good, but I am unfamiliar with Slack, could you please guide me to get the dataset?
post4-comment6: Can we use the code from the data_prep_examples file present in the link for data: https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi:10.48577/jpl.QJ9PYA
post4-comment6-reply1: You can use it. But it has lots of errors. You will need to edit the code to properly process the data. You are welcome to do that. We have edited that code to properly process the inputs. We would be sharing the updated data prep file today.
post4-comment7: How do we upload this data to our repository? There's a timeout error with regular "git add", so I tried it with git lfs, but still got this error message: "Uploading LFS objects: 0% (0/1), 0 B | 0 B/s, done.batch response: This repository is over its data quota. Account responsible for LFS bandwidth should purchase more data packs to restore access.error: failed to push some refs"
post4-comment7-reply1: There is no need
post4-comment7-reply2: So, just to clarify, we do not need to upload the dataset, but rather we only upload the notebook where we code the solutions?
post4-comment7-reply3: Yes you don’t need to upload the dataset.
post4-comment8: When we can expect to get the train, test and validation txt files?
post4-comment8-reply1: it was published yesterday, just create ur repo
post4-comment8-reply2:  Start creating the pipeline by choosing your own train/validation/ test randomly if you can. That’s a small thing. I could have asked you to do it randomly. I was promised they will post it today. M R Rajati’s phone 
post4-comment9: can we use Torch instead of Keras , is there any penalty for the dl framework choice?
post4-comment9-reply1: You may, but the grading will be based on Keras so be aware that this may make the evaluation of your project harder, slower and potentially less accurate even though the CPs try our best. So we would prefer you to use Keras if it's all the same!
post4-comment10: For question 1c(i), do we need to create functions for regularization, cropping, random zooming, rotating, flipping, contrasting, and translation? Sorry, I did not understand this question very well. 
post4-comment10-reply1: you can use opencv as mentioned for this task
post4-comment10-reply2: Perfect. But do we need to actually crop, zoom, rotate, etc. the images in this step, or is it more for other steps? If we do need to crop, zoom, rotate, etc. is it with random values? Thank you so much and sorry for bothering
post4-comment11: how long does it take you guys to train each epoch? mine train really slow even after reducing the complexity to minimum
post4-comment11-reply1: try increasing batch size, it shouldnt be too bad
post4-comment12: The jpl link to download dataset is taking very long time to load in the web browser https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi:10.48577/jpl.QJ9PYA 
post4-comment12-reply1: prob getting overloaded
post4-comment12-reply2: Hi, I am facing the same issue and it's taking way too long to load. Is it possible to upload the 2.6 GB data.zip from the link to dropbox
post4-comment13: Can we use the data.zip file that is present in the dropbox instead of downloading from https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi:10.48577/jpl.QJ9PYA ?
post4-comment13-reply1: same question.
post4-comment13-reply2: @1096
post4-comment14: When grading, will the entire repo be cloned and the notebook ran? I'm asking because I pushed all the data to my repo before seeing that we didn't need to and so in my notebook I used a relative path to get the data. Therefore, if you guys just take my notebook and run it, the data may be in a different relative path and then the notebook wouldn't be able to run. However, if the grading is just looking at our notebook/results, I am good. 
post4-comment14-reply1: +1
post4-comment14-reply2: Of course it is best if you CAN upload the data, but you should always use relative path as @40 indicates.TLDR: you are good!
post4-comment15: Do I need to create a new notebook and follow the homework submission rules? I.e. submit a file Nandi\_Soumyaroop\_final_project.ipynb, or can I just work off of the dataPreprocessingNotebook that was in the repo and continue from there?
post4-comment15-reply1: just change the notebook name
post4-comment16: i wanna cry, my crappy intel mac takes 50 min / epoch * 20 I don't have GPU acceleration can i just submit the code without running the whole code is that fine?
post4-comment16-reply1: you can maybe try google colab i also have intel and dont have GPU accel so i used that instead but extracting all the data from the zip file takes a long time 
post5: Hi Class, The course evaluations are open now, but the response rate is low. Please do me a favor and fill in your course evaluations. Thanks! Regards, M R Rajati
post5-comment1: Where can we submit the course evaluations? Is that one on the blackboard? 
post5-comment1-reply1:  You must have received an invitation email. I can resend it. M R Rajati’s phone 
post5-comment1-reply2: I found the email that I didn't read, sorry about the confusion.
post5-comment1-reply3: can you please tell the email subject
post5-comment1-reply4: USC Learning Experience Evaluations
post5-comment1-reply5: Thanks 
post5-comment2: can through with Course Evaluation in BlackBoard
post6: Hi Class, Dr. Mark Wronkiewicz from JPL will be our guest speaker on Monday, Nov. 27, online. I think it is a good idea to do the whole session online, as it might be hard to capture student questions in SGM 123. https://ml.jpl.nasa.gov/members/mark-wronkiewicz.html Here is the message I received from him: I’ll plan on talking for ~1 hour about JPL science-focused projects (and connect with your class concepts wherever I can). I’m hoping the students are engaged enough to answer questions and will stay as long as they want to discuss. Regards, M R Rajati
post6-comment1: Is this going to be held during the class hours? I wanted to know since there is a joint session too. Can you please clarify.
post6-comment1-reply1: Joint session is 20th, this is 27th
post6-comment1-reply2: My bad 😬 messed up dates in my mind. Thank you though!!
post6-comment2: Is this going to happen during both the morning and afternoon lectures? And, if this is totally online, will that mean there's no office hours on the 27th?
post6-comment3: What time is the JPL session today ?
post6-comment3-reply1:  12:00 noon. M R Rajati’s phone 
post6-comment3-reply2: thank u professor
post6-comment4: what is the zoom meeting link?
post6-comment4-reply1: you will have to go to DEN USC. online lecture link
post7: Hi Class, I have decided to hold a joint session on Monday Nov 20. In the 12-1:50 session, I will present HMMs, and it will be likely a short lecture. In the 3:30-5:20 lecture, I will present Reinforcement Learning, but I WILL NOT HOLD YOU RESPONSIBLE for it in the midterm. Everyone is welcome to join the afternoon session and its recording will be uploaded on D2L as well. Regards, M R Rajati
post7-comment1: Zoom Meeting Link: https://usc.zoom.us/j/94051898406?pwd=K2wrcWxFem5KYVhDdkt3Mk5XcE5EUT09 
post7-comment2: Monday Nov. 20 is the correct date.
post7-comment3: Hi Professor, the recording of Reinforcement Learning lecture isn't on D2L yet (but this Monday's lecture is already there.) Could you please check on that? Thank you!
post7-comment3-reply1:  I was told it was uploaded in Week 13? M R Rajati’s phone 
post7-comment3-reply2: I found it in Week 13. Thank you!
post8: Hi Class, To see your standing in DSCI 552, calculate the following grade: 0.45×Midterm1+0.55×HW in which HW is the average of your homework scores so far. That is an estimate of your performance, assuming that you perform similarly in Midterm2 and your future assignments and the project. You can drop your lowest HW score and play what-if games too. Then, compare your score with the thresholds I used in another offering of this course to have an idea where you stand in this class: B- 39 B 58 B+ 68 A- 80 A 86.8 In that class, here is the number of students who received each grade: A 29 A- 10 B+ 2 B 0 B- 0 C+ 2 C 0 Please note that I need to have all elements of your score and the performance of the class to determine your letter grade. The above scenario is a hypothetical scenario based on approximately 50% of your grade, and projecting it to the future as well as using the thresholds used for a different class and different exam. My experience is that it usually offers a good estimate of the standing of the students. Regards, M R Rajati
post8-comment1:  The thresholds here are totally different with that in syllabus. If finally my score is 75. According to the thresholds here, it is B+. But according to the thresholds in syllabus, it is C, Which one should I refer to? 
post8-comment1-reply1: The only thing that is guaranteed is the table in the syllabus. That shows the MINIMUM grade you will receive.I cannot guarantee anything else. However, I have never used the table in the syllabus in DSCI 552. If you read the above, you will see that I insist that the above scenario is a hypothetical scenario and you cannot make assumptions whatsoever. To reiterate:Please note that I need to have all elements of your score and the performance of the class to determine your letter grade. 
post9:  Hi Class, The first midterm of DSCI 552 will be held on Friday, October 20, 8:00 AM PDT. The location of Midterm 1 is: On Campus Students in 12-1:50 Section: THH 101DEN Students: OnlineOn Campus Students in 3:30-5:20 Section: THH 201All Students with OSAS extra time accommodations who don't want to take their exam in the test center: VPD 106 Please bring your laptops and cellphones with a Scanner App (Such as CamScanner) installed on the phone to the exam room, even if you take the exam with the OSAS test center. You will upload a scanned version of your exam on D2L after the exam time is up. Also, please bring your USC picture ID to the exam room and be ready to show it to the proctors. If you have OSAS accommodations and have not posted your letter on Piazza yet, please make a private post to ALL INSTRUCTORS and use OSAS tag ASAP. If you have done so before, you do not need to do anything. For DEN students who are not in Los Angeles area, the exam will be posted in the Assignment Section of D2L/DEN. You will be proctored on DEN and you should make every effort to be on DEN 15 minutes before your exam starts. You should have your USC ID or any official picture ID ready and show it to your proctor prior to the start of the exam. Please do not use headphones (earplugs to avoid noise are OK) during the exam and be ready to unmute if your proctor asks. DEN students who take the exam online will write their answers on sheets of paper and scan them and upload a PDF on DEN/D2L, or will just upload the PDF of the answers they wrote using their tablet. They will be given 15 minutes after the exam to upload the exam on the assignment section of DEN/D2L. Using electronic devices for anything other than exam calculations or writing the solutions of the exam or connecting to DEN/D2L is prohibited. On-Campus Students and DEN students who take the exam on campus will write their solutions on paper and after they are done, they scan the solutions and upload them on D2L. Please stop writing at the end of the exam, otherwise you run the risk of not being able to submit your solutions. (DO NOT take that risk). The exam is closed-book and notes. You can bring ONE letter-sized cheat sheet (back and front) to the exam. It can be handwritten or typed, but it cannot be electronic. You should use a hard copy. Calculators ARE ALLOWED (and probably needed) in the exam. A simple scientific calculator (~$10-$20) suffices. Graphical calculators are not allowed. The exam will be on the material presented in Lessons 1-5. Appendices that WERE TAUGHT IN CLASS are fair game for the exam. I have posted two sample exams for your practice. Please note that you should not make any assumption about the theme and the level of the questions in your midterm based on the sample exams. You can expect around 6 questions in the midterm, and you may expect any type of question. The exam will last for around 100 minutes. Please arrive early, because the time of the exam cannot be extended and we must leave the room 10 minutes before the next lecture starts. Please make sure that you get enough sleep the night before the exam. Also, you need to practice as many sample problems (from your textbook and the posted sample exam) as possible. Reading the solutions is NOT ENOUGH. You have to actually sit down and work out your own solutions. A lot of people have problems with time management in exams. That can be remedied with working on a lot of sample problems. Good Luck, M. R. Rajati 
post9-comment1: Hi Professor,I wanted to know that for on-campus student, how much time we will get for uploading our paper to D2L after finishing the paper in 100 minutes. Will we get at least 10 or 15 min to scan and upload to D2L as the one Den student have after 100 minutes of exam?Thanks
post9-comment1-reply1: Yes, reasonable time will be given to the students for uploading.
post9-comment2: Are the sample exams you posted the ones already in @12 or are there new ones?
post9-comment2-reply1: they are in @12
post9-comment3: If the exam asks for values based off a t-distribution or something else that requires a table, will the tables be provided?
post9-comment3-reply1: yes it will be
post9-comment4: Is the total score of the 6 questions-test we take have 120points and we cannot exceed 100/100?
post9-comment4-reply1: Make no assumptions whatsoever. None.
post9-comment5: Lessons 1-5 mentioned above correspond to ISLR chapters 1 to 5. chapter 6 is not coming in the midterm?
post9-comment5-reply1: Lessons refer to slides, which are the primary resource for this course.
post9-comment5-reply2: Is Lesson 5 up to Model selection?
post9-comment5-reply3: If you look at the slides, they are named Lesson X - Topic name, and yes Lesson 5 is named model selection
post9-comment6: For DEN Local students, which classroom should we use?
post9-comment6-reply1: Same as 12-1:50 section, unless you have OSAS accommodations, in which case, you must go to VPD. -------------------------------------------------------------- Mohammad Reza Rajati, PhD Senior Lecturer, Thomas Lord Department of Computer Science University of Southern California 
post9-comment6-reply2: Thank you
post9-comment7: Just to confirm, for the DEN student exam proctoring on Zoom, do we use the same Zoom link as we use for lecture? That is the only one that I can find on D2L.
post9-comment7-reply1: @Siddharth Aggarwal Can you please confirm this? Thank you!
post9-comment7-reply2: Yes, as i communicated in OH, its the same class link. If there are any issues, I'll post the changes on piazza and notify everyone 
post9-comment7-reply3: Thank you!
post9-comment8: Just to confirm, is THH the humanities building on the opposite side of campus from where lecture normally is?
post9-comment8-reply1:  I am inclined to say yes, but you must double check with USC maps: maps.usc.edu M R Rajati’s phone 
post9-comment9: Are we expected to bring blue books or scratch paper, or will these be provided?
post9-comment9-reply1: We are only allowed a cheat sheet from what I know. Usually, we will have some extra space at the end of the question paper for any scratch work.
post9-comment10: For DEN students, are we being proctored on the same zoom link used for class?
post9-comment10-reply1: yeah just use that one for now, if anything changes we will inform you
post9-comment10-reply2: Waiting in the waiting room 
post9-comment10-reply3: It says for me "waiting for host to start the meeting"
post9-comment10-reply4: sweet 
post9-comment10-reply5: no problem, it hasnt been opened
post9-comment10-reply6: no assignment for midterm upload yet
post9-comment10-reply7: Can you please join this zoom link : https://usc.zoom.us/j/97040902730
post10: Hi Class, There are no rooms for our midterm exams at 10-11:50 AM at USC. According to the syllabus, I will change the hours of the midterm exams to 8 AM-9:50 AM, but the dates stay the same. Both of the exams will be IN PERSON. This decision is a last resort, and it makes sure that we can have the exam for all students at the same place. Regards, M R Rajati
post10-comment1: What does that mean BOTH? Can we take a midterm at 10-11:50 too?
post10-comment1-reply1: it means both midterm 1 and 2
post10-comment1-reply2:  We have two midterms. Both were at 10. Now they’re at 8. M R Rajati’s phone 
post10-comment2: Just to confirm, midterm will be held on the 20th of October right?
post10-comment2-reply1: the date DID NOT change, refer to the syllabus for all dates
post10-comment3: Where will the midterms be held? I'm a local DEN student so am unfamiliar with the campus
post10-comment3-reply1: @533
post11: From syllabus: If you have a question about the material or logistics of the class and wish to ask it electronically, please post it on the piazza page (not e-mail). Oftentimes, if one student has a question/comment, others also have a similar question/comment. Make public posts in these cases (you may post them anonymously). Use private Piazza posts with the professor, TA, and graders only for issues that are specific to you individually (e.g., a scheduling issue or grade issue). Try minimizing the use of email by the course staff. Following the above process also makes sure that you receive credit for your Piazza contributions.
post12: Hi everyone,I am sharing a recorded tutorial session.This covers homework 0 and the basic Anaconda and GitHub setup (clone, commit, push, pull, etc). Recording link: https://usc.zoom.us/rec/share/QYn1l2yrjh_9SsaFA3Do93rDzStSJvtfsWOQxzj2iYZejXn-XUVdy2X3Yp2KDRlU.OFSYx_hJQ9JR4DUZ?startTime=1674190963000 Slides and HW0 link: https://github.com/woojeongjin/HW0 Best, Woojeong
post12-comment1: Reading the csv file using pandas, shouldn't we use skiprows = [2] in order to skip the second row ? skiprows=[1] skips the first row instead of second.
post12-comment1-reply1: yes skiprows = [2] skips the second row
post12-comment2:  Create a Python dictionary object whose keys are the headers of the dataframe created in the read_csv() exercise and values are Python list objects that contain data corresponding to the headers. Do we have to consider playerID column which has been set as an index column as the key for the dictionary as well ? 
post12-comment2-reply1: Yes, in the solved homework notebook, playerID column is considered as a key. 
post12-comment2-reply2: Actually making PlayerID an index column was just a part of pandas section after that you might use the normal data frame to work along other tasks. Basically you have to convert the whole data frame in a dictionary format which should match the dataset of converted back to data frame itself.
post12-comment3: I know I'm being nit-picky here, but this part of the assignment reads: "Select the id of the players who are registered in ATL and HOU and whose salary is higher than one million.", which suggests we want to find the players who were a part of ATL team and HOU team at some point (assuming the players can switch teams). The solution, however, returns the ids of players who were a part of ATL or HOU. If we want to follow the spec/instructions precisely, then grouping players by ATL and HOU teams and counting the number of teams for each player would probably be desired: players_in_atl_or_hou = df[(df['teamID'].isin(['ATL', 'HOU'])) & (df['salary'] > 1000000)] players_and_distinct_teams = players_in_atl_or_hou.groupby('playerID')['teamID'].nunique() players_in_both_teams = players_and_distinct_teams[players_and_distinct_teams == 2] players_in_both_teams.index.tolist() We get a much shorter list of players: ['bournmi01', 'hamptmi01', 'wagnebi02'] 
post12-comment3-reply1: If you feel this can help you practice more go for it. But yes I agree the wording here can be a bit confusing.Its kinda like saying "students in USC and UCLA", would you interoperate that in the same way?Once again proven that english is harder than machine learning sometimes, but ultimately its not super important since this assignment is just aimed to help you warm up. If you find any other wording related questions we will make sure that a clear answer is given.
post12-comment4: This video seems to be out of date and I can't open it. Could you please upload it againThis video seems to be out of date and I can't open it. Could you please upload it again?
post12-comment4-reply1: Did you log in to Zoom? I have no problem accessing the video with the link. Could you elaborate what your issue is?
post13: Hi all, Here is the office hour schedule, any changes will be updated and posted on Piazza. In-person OH will be located at the SAL computer lab if not specified. Day/TimeCP/TAZoom LinkMonday 10-12 amSoumyarooplink (pwd: dsci552)Monday 6-8 pmChristopher Myau Zoom Link (dsci552) Monday 8-10 pmSarth Kanani Sarth Kanani Tuesday 10-12 amKaustubh KothawaleZoom Link(dsci552)Tuesday 12 -2 pmTanmay JainZoom Link (dsci522)Tuesday 2-4 pmDarshan VishwanathZoom LinkTuesday 4-6 pmVishal SinghZoom LinkWednesday 8-10 amAbhishek AjmeraZoom LinkWednesday 10-12 amTarunbir GambhirZoom Link (pwd: dsci552)Wednesday 2-4 pmIan WuZoom LinkWednesday 6-8 pmKshitij PatelKshitij OHWednesday 8-10 pmWoojeong JinlinkThursday 8-10 amAabha RanadelinkThursday 10-12 amChangxun (Shawn) Lilink (pw: dsci552)Thursday 12-2 pmParth KapadialinkThursday 2-4 pmSrivatsav GunisettySrivatsav OH LinkThursday 4-6 pmVansh Rajesh JainVansh OH linkThursday 6-8 pmQasim SiddiquiZoom (pwd: dsci552)Thursday 8-10 pmSiddharth AggarwallinkFriday 8-10 amSiddharth ByaleSiddharth OHFriday 10-12 amGrace KimGrace (pw: dsci552)Friday 12-2 pmDaniel Pereira da CostaZoom linkFriday 2-4 pmZhuo ChenPassword: DSCI552Friday 4-6 pmPrayas DixitPrayas OH LinkFriday 6-8 pmVedanvitaVedanvita OHFriday 8-10 pmShreyash ZanjalShreyash OH linkFriday 12-2 pmYaqi HulinkFriday 12:30-2:30 pmPiyush DeepPiyush Deep's Office HourFriday 10-12 amHassan ShahHassan Shah LinkFriday 2-4 pmVibhav DeshpandeVibhav's LinkFriday 4-6 pmKirthika GurumurthyKirthika OH Link 
post13-comment1: My In-Person OHs will be in the Baum Student Center in RTH unless said otherwise
post13-comment2: My In-Person OHs will be in Baum Student Center in RTH
post13-comment3: My In-Person OHs will be in the Baum Student Center in RTH 
post13-comment4: My in-person office hours will be in the basement of Leavy Library 
post13-comment5: Hi, my in person office hours will be in RTH 219.
post13-comment6: Is Aabha having OH this week? It seems as though some CPs are and some aren't, so is there any way to know who is?
post13-comment6-reply1: Refer to the schedule on top, unless said otherwise there are no OHs scheduled
post13-comment6-reply2: I don't have OH scheduled this week. But feel free to make a post on Piazza if you have any questions.
post14:  #pin
post14-comment1: Edit: Posted in wrong section.
post15: Hi all, Here is the link for Homework submission. Future homework submission links will be provided here after the deadline for the previous homework. (Note: Homework 0 is not graded and you don’t have to make a submission. But you can use the link for test purposes) Homework 0: https://classroom.github.com/a/Y-v87gZB Homework 1: https://classroom.github.com/a/Ud4Ja_W5 Homework 2: https://classroom.github.com/a/pxAvblRv Homework 3: https://classroom.github.com/a/nUhZZjC1 Homework 4: https://classroom.github.com/a/c4YzY-PT Homework 5: https://classroom.github.com/a/N46wag9k Homework 6: https://classroom.github.com/a/jMvl29md Homework 7: https://classroom.github.com/a/2C6tffKD Homework 8: https://classroom.github.com/a/Bz-pWU7u Final Project: https://classroom.github.com/a/J0DgMjNz 
post15-comment1: Hello! Do we have to upload something in assignments submission in D2L? Thanks!
post15-comment1-reply1: no everything is on github, D2L is just for grades
post15-comment2: Heads up there is a waiting period of maximum 10 days for Github to confirm that you are a student that can enroll in the classroom
post15-comment3: Just for clarification, can we submit homeworks until 11:59pm of the same date listed in the syllabus? Or, is it 4pm like the final project?
post15-comment3-reply1: 11:59
post15-comment3-reply2: Thank you!
post15-comment4: Hi, a few general doubts: 1) How do I verify that the .ipynb file that I have put up on the github is working fine, and will also run as expected for the TAs as well? 2) If the package that I have used in my file is not installed in the TAs' device, and they are unable to run my file due to this, will that be an issue on my part? 3) Also, if the version of the package that the TA has, is somehow incompatible with the version of the package that we have used in the submission, is there anything that the students can do about this? Thank you!
post15-comment4-reply1: 1. Just make sure it works for you thats all you can do, no need to stress2. Thats why there is the optional requriment.txt file @403. once again if you are super concerned then use the optional requirement.txt @40
post15-comment5: can we get the HW6 submission link published whenever someone has an opportunity? Thanks, James
post15-comment5-reply1: same way here..if we can get the link of HW7 i'd appreciate it!
post15-comment5-reply2: +1 for HW7 link.
post15-comment5-reply3: I just published the link for HW7
post15-comment6: For Final Project do we have to fork the repository and then upload? As I did not have an option to upload in main branch in my case.
post16: Hello everyone, Please enter your Github Usernames (the one that you will use to submit HWs) and email in the following spreadsheet: https://docs.google.com/spreadsheets/d/1y3QHIa0Eo--Vb2NI-Qdn5A9TPP4YHgMhP4S9agDqJxk/edit?usp=sharing You can access this Google sheet only if you log in with your USC email. Suggestion: Create a Github account using your USC email ID and submit homework using that account. Note: If you do not provide your Github Usernames, we will not be able to grade your homework. Also, please check if your name is not present in the spreadsheet even though you have registered for the course. If not, comment below or message privately and mention your Github username. If you are not registered in the course, your name will not be present. In that case, contact the department/myViterbi for D-clearance and register first.
post16-comment1: Is it mandatory to create a new github account using usc email? or can I use the one which I already have created with my personal email.
post16-comment1-reply1: it is not mandatory, you are welcome to use your own account
post16-comment1-reply2: In which column can I update the email id to use my personal github account ?
post16-comment1-reply3: You just put in your github ID, you are not changing your email, the email is for identification now github
post16-comment2: Hello, I just realized I didn't add my id in the sheet, I just added that now, is that okay as I already submitted HW1?
post16-comment2-reply1: you are all good
post16-comment3: Hello, I just realized I didn't add my github id in the excel sheet above, I just added that now, is that okay as I already submitted HW1? 
post16-comment3-reply1: you are all good
post17: Homework Assignments Submission Rules (Absolutely important) Hello Class, 1. For all homework assignments, we will use the GitHub classroom for programming assignments’ submission. TAs/CPs will post an invitation link for each assignment before the due day. For submission, all you need to do is click that link to accept that assignment, and it will redirect you to a private repository for that programming assignment. Please use only one GitHub account for submission during the whole semester. 2. For all assignments, when submitting, please organize your code and data file as the directory structure shown in the figure below (P.S. The top-level folder is the root of the GitHub repository). Also, please use a relative path when loading your data file so that TAs can execute your Jupyter Notebook files without changing everything. Also, please avoid using “” for specifying a path. No matter you are specifying an absolute path or a relative path, using backslash is always a bad idea since it cannot be recognized correctly in platforms other than Windows. Instead, one should always use “/”, e.g. “C:/Users/user1/Desktop”, “/usr/bin”, or “../data/notebooks”. Using the following figure as an example, the correct path should be …/data/vertebral_column_data/column_2C.dat. Do not use absolute path otherwise there is a high probability that your TAs/CPs will fail to execute your code. Points will be deducted if your TAs/CPs fail to execute your code because of not using a correct relative path in your assignments. readme, gitignore and requirements.txt are optional. Thank you for your cooperation. 3. For each programming assignment, please use markdown cells to indicate which question you are solving. Also, use markdown cells to report your answers if the result cannot be explicitly output by your code. Thank you for your cooperation. 4. For each programming assignment, please use the “cell -> run all” function provided by the Jupyter Notebook to sequentially testing all code is working correctly. Otherwise, you may lose some points because there may be some unseen exceptions when TAs executing your code. For example, you declare a variable in a very beginning cell. Then you use that variable in the following cells. A few hours later, you think that this variable is no longer useful and you delete it. However, this variable has already been in memory. Thus, the autocomplete mechanism may push you to use that variable again and again unless you restart your notebook and finally find that variable is deprecated. 5. Please use the local Jupyter Notebook for finishing your assignment. In the previous semester, some students used the Google Colab for finishing assignments and then copied the code for the Google Colab notebook to a local Jupyter Notebook file and submitted it without testing. In this case, some unexpected exceptions may arise and the TAs have to take out some points from your assignments. 6. Please include your Name, Github Username and USC ID in the first cell of the Jupyter Notebook file for each assignment. Name your jupyter file as Lastname_Firstname_HW(i).ipynb, for example, my HW0 should look like Nandi_Soumyaroop_HW0.ipynb- Additional Instructions: A few suggestions for a cleaner submission: The below points are suggestions and good practices and not rules. 1. Try to organize all imports together (preferably in the first cell)This can help in understanding all the dependencies required to run your notebook. This might also help avoid problems that arise when cells run out of order during development. 2. References and CitationsTry to maintain a list of links/documents that you referred to while researching any algorithm at the bottom of your notebook in a markdown cell so as to support any decisions/techniques used to solve the problems in the assignments. This is also good practice so that you have a collection of links/documents that you referred for a specific problem and might save time when required in the future. 3. Plots and figures try to label (and add index) your plots wherever possible to convey information quickly and in a concise manner. Sometimes combining a few plots (eg. using subplots in matplotlib) can make your submission drastically more presentable and will help you in learning data visualization techniques using python (which is a very useful skill) Note: Once you upload your homework, you don't need to follow up with the TAs, checking if your homework was submitted. There will be a timestamp of your submission in Github and as long as it is within the submission deadline, you don't need to worry.Always submit a copy of your .ipynb file where the code has been executed in each cell. Please comment below if there are any questions.
post17-comment1: Do we need to convert our hw repo from private to public?
post17-comment1-reply1: no, that would cause other students to see your own
post17-comment1-reply2: Do we need to create a new branch or just add a new ipynb file to the main branch?
post17-comment1-reply3: @40 ALWAYS submit in main branch. Ideally you should be practicing good git with proper version control, which means working in a dev branch and then merging into main.
post17-comment1-reply4: OK, thanks a lot. I am a little confused because of the hw rules. As the picture showed below, it seems that the hw's ipynb file is under branch "notebook" which is a branch of main. 
post17-comment1-reply5: The picture shown is your folder structure, it is not related to git
post17-comment1-reply6: Make sense! I think it is only related to my local space right and have nothing to do with Homework Submission on github, right?
post17-comment1-reply7: You final submission and your local space should both have this same file structure
post17-comment1-reply8: Sorry, I want to make sure that is this structure like this is OK? Or I need to construct a data / notebook folder？ 
post17-comment1-reply9: I see, yes you should have data/notebook folder
post17-comment1-reply10: OK, got it, thanks.
post17-comment1-reply11: Like this? 
post17-comment1-reply12: yes thats good
post17-comment1-reply13: I have the same folder structure as the one shown above, but when reading the data file, I think the correct path is '../data/vertebral_column_data/column_2C.dat' instead of '…/data/vertebral_column_data/column_2C.dat' ? (not sure but just want to confirm) Thanks!
post17-comment1-reply14: You are right. Having "../" (with 2 periods, not 3) means to go up one directory. Your notebook would not run if the relative path is incorrect. You can also clone your repo into a new directory if you are worried that your relative paths do not work.
post17-comment1-reply15: you cant have ... anyway, refer to @84
post17-comment2: Do we lose all the possible points for a homework if the relative path is wrong? (I am just realizing that I forgot to update my path variable in the notebook after creating the "data" folder above the "vertebral_column_data" folder in the repository. I am aware that it was my responsibility to update variables, I am just wondering if I should expect to get 0 points.)
post17-comment2-reply1: If a student does not follow the submission instruction on the Piazza post @40, a deduction of 10% will be given for HW1, and will receive 0 from HW2 onwards if they do not follow the submission instruction. 
post18: Hi Class, The location of the 3:30-5:20 section of DSCI 552 has permanently moved to THH 201. Regards, M R Rajati
post18-comment1: I'm in THH201 and class was supposed to start 20 minutes ago Is class still happening?
post18-comment1-reply1: The class did happen, idk if you went to the wrong classroom, but THH is located near the leavey libaray and right next to trousdale
post19: https://www.dropbox.com/scl/fo/z7ybcaxtorjgyirn4p0r6/h?rlkey=jrw2r05ltirdnkotyuj92bj8m&dl=0 
post20: https://www.dropbox.com/scl/fo/28233d8epazdzkr49v6wj/h?rlkey=adwos8nlc07dgw9xb8pqwv05h&dl=0 
post21: https://www.dropbox.com/scl/fo/8bl7razrri7l8pgyebdcn/h?rlkey=6pd1f4sj67l0ummlun8jyxenp&dl=0 
post21-comment1: Hi, Where can I find the deadlines for homework?
post21-comment1-reply1: Hi! You can find them in the syllabus!
post21-comment1-reply2:  Please watch the first lecture carefully. Thanks. M R Rajati’s phone 
post22: https://www.dropbox.com/scl/fo/8jugp9yufrs8xcwbvkhuj/h?rlkey=1yale6qyxscurgerfskqtuzrx&dl=0 
post23: https://www.dropbox.com/scl/fo/bzcdr20it75x35tg65x7p/h?rlkey=6wd4839idy42uujo3vesbrsqi&dl=0 
post24: https://www.dropbox.com/scl/fo/j4otn3s9dncglsnz8rvlv/h?rlkey=m6d6bahon84fzzfyw0y3wxvf3&dl=0 
post25: Hi Class, Due to unforeseen situations including flooding of SGM 123, we must hold the first lecture of DSCI 552 online. As a result, I am cancelling the afternoon class, and I request that students in the 3:30-5:20 section join the 12:00 noon lecture on DEN today or watch its recording. All the lectures of 12-1:50 session will be recorded by DEN and made available to students in both sections. I apologize for any inconvenience that this may cause. Regards, M R Rajati
post26: Message from Dr. Satish Kumar: I will be teaching a special topics course "DSCI-599: Optimization Techniques for Data Science" next semester. The syllabus is here: https://web-app.usc.edu/soc/syllabus/20241/32453.pdf Kindly forward this email to your students who you think may benefit from the course. If they need any help with d-clearance, they can reach out to me directly. 
post26-comment1: Can we have their email address? Nvm found it from syllabus! 
post26-comment1-reply1:  tkskwork@gmail.com M R Rajati’s phone 
post27: Hi, Please do not send miscalculation messages. It seems that homework scores were miscalculated and I must update everyone's grade. I will make a post as soon as I am done with this. Sorry about any inconvenience this may cause. Regards, M R Rajati
post28: Hi, the syllabus said Participation in Piazza has up to 5% extra credit and it's granted on a competitive basis at the discretion of the instructor. Can we know how do instructor give us the extra credit?
post28-comment1-reply1: The top student will be granted extra points (if they need any for an A)
post29: It seems my homework 7 has not yet been graded but homework 8 has.
post29-comment1-reply1: <strong attention="jzmycefvu826td">@Soumyaroop Nandi</strong> 
post29-comment2: Updated HW7. I see no submission for HW8
post29-comment2-reply1: Please let me know what the updated grades are here.
post29-comment2-reply2: The grader had graded HW7 for this student, but did not update/publish on Den. I published it now. Final Grade: 791.b. ii. (-1) Not performed with standardized attributes1.b. iv. (-1) Smote applied before CV. (-2) Report your conclusions about the classifiers you trained - missing2. (-10) Not performed Monte Carlo Simulation. (-2)You need to show the best k and majority lable triplet for each iteration in Monte Calro Simulation. (-2) You need to calculate the average hamming score for all 50 iterations of Monte Carlo Simulation. (-2) SD and mean missing 3. (-1) Graphs partially correct
post30: I am wondering if the standing will decrease than this which is posted after midterm. B- 39 B 58 B+ 68 A- 80 A 86.8 Thank you.
post30-comment1-reply2: You will know soon^TM
post31: Hi, Just wanted to ask and get an idea about when can we expect to get the final letter grade? Thanks in advance!
post31-comment1-reply2: They said it will be on tomorrow 
post31-comment2-reply2: Soon^TM (a couple more days)
post31-comment3: Where can we check our final letter grade?
post31-comment3-reply1: +1
post32: Dear Instructors, I apologize for reaching out so late. I am writing to inquire about the possibility of a reevaluation of my answer to question 4. I know I made some mistakes after the cluster CD, I still show serval steps on cluster distances . I would be grateful if you could consider giving more grade on this part. Thank you for your time and understanding. The best
post32-comment1: Checking 
post32-comment1-reply1: @Kirthika Gurumurthy can give you some more details
post32-comment2: I have updated your marks based on the steps you’ve shown.
post32-comment2-reply1: Thank you so much! Hope you have a nice holiday!
post33: Has hw8 been graded?
post33-comment1-reply1: Not yet. The grades should be up in a day or two <div><br /></div>
post33-comment2: Still have not seen any grade for HW8 recorded - wanted to make sure I didn’t miss anything? 
post33-comment2-reply1: We are regrading urs, but it should not change your grade since the lowest HW is dropped anyway
post34: I checked my grade for hw8 but found it was 0/100. I checked my github and my work was submitted. I don't know what happened. Can anyone help to give an explanation?
post34-comment1-reply1: @1073 @1253
post35: Hello, Just found a possible solution to get rid of my consistent val_accuracy issue for the EfficientNetB0 model (keep getting the same val_accuracy of 0.3218/0.6782, or the validation score does not improve as expected). In my load_and_preprocess() function, there’s a line of code like this: # Normalize the image to the [0, 1] range img = img / 255.0 On the websites below, some discussions suggest that there might be a bug or issue with EfficientNet in relation to the normalization process. Reference:https://stackoverflow.com/questions/67260853/why-would-validation-loss-be-exceptionally-high-while-fitting-with-efficientnethttps://github.com/tensorflow/tensorflow/issues/48103 After I removed the above line of code, my EfficientNetB0 worked correctly. I have tested the model with different parameters, and the issue never came out again. Although it is overdue, I hope it can be of help to others facing similar issues.
post36: I have been running the the file and now it's on VGG16. Can I submit the the project a bit late without penalty?
post36-comment1: They said no so i'd get a file in before 4 anyway 
post36-comment1-reply1: How about submitting the mid_running file and after finishing running, submit the completed file? 
post36-comment1-reply2: I'd at least have something and then wait for an instructor to clarify
post36-comment2-reply2: an instructor has confirmed that there is no extension on the deadline here @1232
post36-comment3-reply2: just submit whatever you have before the deadline
post37: Do we need to upload the data directory on github ? Could we upload a data.zip instead of data directory on github?
post37-comment1-reply2: No
post37-comment2: Hi, hope I didn't misunderstood. Does this means we don't need to upload data on github?Thanks!
post37-comment2-reply1: i believe other posts have stated this so yes 
post37-comment2-reply2: No we don’t have to upload data folder.We need to upload our notebook.Uploading checkpoints/.h5 files are appreciated but not required.^according to the piazza posts I have read so far
post37-comment3-reply2: <a href="/class/1192">@1192</a> <a href="/class/1037">@1037</a> @1198 please read previous posts
post38: Do we need to submit best_model.h5 file?
post38-comment1-reply2: Best if you can
post38-comment2: But when I upload it, this error shows up: The push operation includes a file which exceeds GitHub's file size restriction of 100MB. Please remove the file from history and try again.
post38-comment2-reply1: if you can I did not include it due to these errors 
post39: Replacing the training data with the augmented data I get better precision and accuracy. Which one should I use for training : 1) (Only Augmented Data: better results) or 2) (Augmented + Training data) ?
post39-comment1: whole point is to augment data so that we get more training examples so Ig 2? But just use whatever works cuz my sht just crashes when I use augmented data and I still don't know why 
post39-comment1-reply1: tough
post39-comment1-reply2: tell me about it 
post39-comment2-reply2: No free lunch, whatever works
post40: Since we are not uploading the data, should we use the relative path still for our data folder? Or is there a name for the folder we should use that will work for the person running the code? I am using a relative path but I am thinking since I might have named my folder something different that whoever is grading. Thank you!
post40-comment1-reply2: <a href="https://piazza.com/class/lll6cacyxjfg3?cid=1151">@1151</a> <a href="/class/1017_f14"></a><a href="https://piazza.com/class/lll6cacyxjfg3?cid=1017">@1017</a>_f14 Don&#39;t worry about the folder name
post41: github is saying I cant upload due to a lock file? I looked it up online and it says there should be a file I can delete but I dont see it. does anyone else have this issue?
post41-comment1-reply2: <md>I think I have run into that problem before, it's a hidden file that you'll have to take care of. I believe following this solved it: https://stackoverflow.com/questions/9282632/git-index-lock-file-exists-when-i-try-to-commit-but-i-cannot-delete-the-file</md>
post41-comment2-reply2: usually its bc u have a file open somewhere so its locked, just close all files or restart
post42: Hi all, Can we extend the final project ddl for 2 hours, I spend almost 24 hours running the VGG16 model for one time, and I had run it for 3 times, in order to make it converge. But I still need 6 hours to finish the final round. Thank you!
post42-comment1-reply2: <div> <div> <div> <div> <div> <div> <div> <div> <div> <div> <div> <div> <p>You can refer @1177 to relatively speed up execution time and test your models.</p> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div>
post42-comment1-reply2: <div> <div> <div> <div> <div> <div> <div> <div> <div> <div> <div> <div> <p>You can also refer @1177 to relatively speed up execution time and test your models.</p> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div> </div>
post42-comment2: Hi, I have used maxpooling, I think the main reason should be the huge amount of data, I have about one hundred thousand pics and I set the 20 epochs for the last model, and I am using cpu to train the model. Of course I have tried to use colab pro to train it, but it reported an error I didn't solve it, so I run it locally. 
post42-comment3-reply2: There is no extension, thats why you start early
post43: can we just push the project code to GitHub like we have done with the homework? or we upload to den
post43-comment1-reply2: We have been using git this entire semester, please use git
post44: Can we submit two separate notebooks one for CNN+MLP and one for Transfer Learning?
post44-comment1-reply2: Yes
post44-comment2-reply2: <a href="/class/lll6cacyxjfg3/post/1147">@1147</a> <a href="/class/lll6cacyxjfg3/post/1175">@1175</a> @1189
post45: is transfer learning supposed to be 3 models and therefore three plots? or are we combining the pretrained models and therefore should have one plot?
post45-comment1-reply2: 3 separate models, 3 separate plots 
post46: Hi, just wanted to know, are the hw-8 grades released yet?
post46-comment1-reply2: They said most likely the 12th
post47: We have published the final grades for midterm 2. Let us know if anyone has any problems before 13th December.
post47-comment1: Can we get a split for the marks distribution for each question? How many marks each sub-question is worth?
post47-comment1-reply1: @12 already up since last night
post47-comment2: Just curious but will there be solutions provided?
post47-comment2-reply1: @12 already up since last night
post48:  Above is a snippet of the original data-loading code in the project repository; if you look at the second line, 'tf_data_train' is passed to create a test dataset; shouldn't it be 'tf_data_test'?
post48-comment1-reply1: Yes it is a bug you can change it 
post48-comment2: It was corrected in @1025.
post49: Hi, I just want to confirm this what repository looks like after I submitted the final notebook.(the notebook folder has the notebook I finished for the final project.Thanks)
post49-comment1-reply1: The notebooks are supposed to be all in the notebook folder (hence why its named notebook), but not gonna ask you to fix it because rerunning would take too long
post50: I'm getting the exact same precision, recall, and f1 scores for both transfer learning and the base model, which tells me that transfer learning isn't doing anything, but I'm not sure why. Here's the definition of the pre-trained model efficient_model = tfk.applications.efficientnet.EfficientNetB0( weights="imagenet", input_shape=(299, 299, 3), include_top=False) efficient_model.trainable = False And what's supposed to be the transfer learning model: model = tf.keras.Sequential( [ efficient_model, #tfk.layers.GlobalAveragePooling2D(), tf.keras.layers.Conv2D(32, (3,3), padding='same', activation="relu"), tf.keras.layers.MaxPooling2D((2, 2), strides=2), tf.keras.layers.Conv2D(64, (3,3), padding='same', activation="relu"), tf.keras.layers.MaxPooling2D((2, 2), strides=2), tf.keras.layers.Conv2D(128, (3,3), padding='same', activation="relu"), tf.keras.layers.MaxPooling2D((2, 2), strides=2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(256, activation="relu", kernel_regularizer=tfk.regularizers.L2()), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dropout(0.3), tf.keras.layers.Dense(1, activation="softmax") ] ) This is identical to the base model, save for the first layer being "efficient_model"
post50-comment1-reply1: <p>You aren&#39;t supposed to have the CNNs in the transfer learning models! You&#39;re taking the features from the pretrained models and transforming them into your own features using your own CNN layers, which is why you&#39;re getting similar results! your model should look like </p> <p></p> <p>```</p> <p>efficient_model </p> <p>Flatten()</p> <p>...the rest of what you have...</p> <p>```</p>
post50-comment2: Not sure I fully understand. So, we're supposed to pass our images through the pre-trained models, flatten the outputs, then pass those outputs through data augmentation and the separate CNN model we used previously?
post50-comment2-reply1: No, first augment the data, then pass them through pretrained models, flatten the outputs and pass it through your MLP layer (I think most people are using 1 hidden layer and an output layer). The CNN layers convert your image into features. The pretrained models do the same, so you're _replacing_ the CNN layer with the pretrained models and keeping everything else the same (or similar, I believe you can tune hyperparameters). 
post50-comment2-reply2: Got it. Thanks for the clarification!
post51: the training set shape is 299,299,3 but my augmented training set is 32, 299, 299, 3 Is there a way to combine them to use for the 3-layer cnn? or do I just use the initial training set thank you!
post51-comment1-reply2: You can unbatch the augmented set, concatenate the two and then if required again batch the new dataset using .batch(32) method.
post51-comment1-reply2: You can use the .batch(32) method on the training set to get that shape. It creates batches of 32 of the training set
post51-comment2: this will reduce my augmented set from 32,299,299,3 to 299, 299, 3? thanks!
post51-comment2-reply1: yes
post51-comment2-reply2: Yes that’s correct. 
post52: 
post52-comment1-reply2: Prob around the 16th
post53: I have generated the classification report on tf_dataset_train instead of tf_augmented_train. Will I have to rerun it and generate it on tf_augmented_train? Or is this fine?
post53-comment1-reply2: I think it should be fine, especially because the classification report should be on unaugmented data for validation and test sets 
post54: Can anyone specify on what factors our project will be graded on?
post54-comment1-reply2: <md>@1108</md>
post55: 
post55-comment1-reply2: Soon^TM
post55-comment2-reply2: Will it be released today?
post56: Well, the title says it all. Any guidance can help me prepare as my weight files are above 450MB. Thanks!
post56-comment1-reply2: No
post57: I'm already using Colab Pro with GPU acceleration and its still crashing? My System RAM maxes out i believe I've already reduced the amount of augmented data Im adding but it still just crashes out Any suggestions? I believe it has to do something with my augmented data as it runs fine when just using the raw train data 
post57-comment1: Would it be ok to first just run everything without the augmentation to get some outputs and just turn in what I can with the augmentations before the deadline? 
post57-comment1-reply1: always welcome to. you will just lose some points 
post58: Hi, if I used Colab for training, do I need to change paths back to "./data" or "./train_source_images.txt"？Also, do we need to upload data folder? 
post58-comment1: I believe data folder is not needed the paths I think they said they'll be lenient on 
post58-comment1-reply1: Got it. So we best leave paths as they are and don't upload the data folder?
post58-comment1-reply2: Some posts have said to change some haven't I saw this so I think that would be fine? @1151
post58-comment2-reply2: <md>@1151 @1017_f7 @1192 @1037</md>
post58-comment2-reply2: <md>@1151 @1017_f7</md>
post59: I am facing an issue where the model consistently predicts the same class for everything for all 3 models, showing no signs of learning, as the accuracy remains constant. I've checked all the posts but haven't found a solution. It would be great if anyone who has resolved this issue could offer some advice.
post59-comment1-reply2: Whats your dense and output layer parameters?
post59-comment2: I used Dense(300, activation='relu', kernel_regularizer=l2(0.0001)), Dropout(0.3), Dense(2, activation='softmax') # Assuming binary classification 
post59-comment3: Seems ok probably check the data pre processing code error shld be there. And check if you have downloaded the updated dataset data.zip shld be 2.6gb
post60: Do we need to upload data to the github repository for the final project? The size is very large, and it's taking a while, so I wanted to make sure. Thanks!
post60-comment1-reply2: no,I think
post60-comment2: @1017 no need
post60-comment3-reply2: <a href="/class/1037">@1192 @1037</a> please read previous posts
post60-comment3-reply2: <a href="/class/1037">@1092 @1037</a> please read previous posts
post60-comment3-reply2: <a href="/class/1037">@1137 @1037</a> please read previous posts
post61: I'm getting the following error because of which the precision, recall and F-1 score are coming out to be zeroUndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result))Does anyone know how to resolve this?
post61-comment1-reply2: <md>@1101</md>
post62: Hi, Are we supposed to give Precision, Recall, and accuracy for all the train, test, and val datasets ?
post62-comment1-reply2: d.v? yeah
post63: For my VGG16 model, the ETA for epoch 1 is saying 19 hours. Any tips for solving this? I have had about one and a half hours per epoch for all the other ones. Also, when we submit, if we are not submitting with the data, should we still keep the relative path to the folder where we have the data and assume that if it will be run for grading, the person running it will update it to the correct path on their computer?
post63-comment1-reply2: <p>You can use collab pro with GPU accleration</p> <p><a href="/class/1151">@1151</a> <a href="/class/1017_f14">@1017_f14</a></p>
post64: WARNING:root:data: Did not find designated split in train/validate/test list. WARNING:root:__MACOSX: Did not find designated split in train/validate/test list. I'm getting this error when running the load_and_preprocess method. Any ideas on how to fix this?
post64-comment1-reply2: I am also getting this error. Has anyone solved this?
post64-comment2: maybe we can discuss together and figure it out?
post64-comment2-reply1: sure I dont mind we can do that. @y3gh18 - instagram. could you reach out to me on there
post65: I am getting this error when predicting labels of the test dataset using any of the transfer learning models: ValueError: can only convert an array of size 1 to a Python scalar Is someone facing the same issue?
post66: Hello! Do we have to push the data to GitHub while making a submission? Also, the file structure for the final project remains the same as for homework? Please confirmThanks!
post66-comment1-reply1: So far I have seen that we do not need to push the data and that it&#39;s alright to maintain two jupyter notebooks for each part. I personally uploaded Verma_Shiva_Part(c).ipynb and Verma_Shiva_Part(d).ipynb 
post66-comment2-reply1: @1037 please read previous posts
post67: Hello instructors, For EfficientNetB0, I’m getting the same val_accuracy per epoch. I’ve checked my layers, fine tuned parameters for a long time, but it still results to the same accuracy constantly. However, if I change my dropout rate by 10%, everything works fine. Can I use a dropout rate other than 30% in this model ? I cannot find any solution apart from this.
post67-comment1-reply1: Following the instructions would be preferred, but If you are at a major roadblock and doing something different would help you progress through it you should always do so
post68: I am getting following plot while running the efficient net model. Obviously something is wrong here, if someone is facing similar issues then please let me know 
post68-comment1: Many students are having the same problem where the model keeps predicting the same class for everything and doesn’t learn anything. I don’t think a solution has been found / posted yet! 
post68-comment2-reply1: Same here. I&#39;m getting zero improvement and it&#39;s taking too long for one epoch. 🥲
post68-comment2-reply1: Same
post68-comment3: Same problem here!
post69: Are we allowed to submit two separate notebooks? My colab environment always automatically disconnect due to excessive runtime. So I completed CNN+MLP model in one notebook, and transfer learning part in another. 
post69-comment1-reply1: <p>I believe it was mentioned that we can </p> <p></p> <p></p>
post69-comment2: https://piazza.com/class/lll6cacyxjfg3/post/1147
post69-comment3-reply1: @1147 @1175
post70: is 1.d.i actually asking us to do anything or just giving context? 
post70-comment1: Just letting us know how to do it i think 
post70-comment1-reply1: thanks!
post70-comment1-reply2: same with 1.d.iii ?
post70-comment1-reply3: I would say thats what your "last" layer would be to "add onto" the pre-existing models
post70-comment1-reply4: and use the outputs of thepenultimate layer in the original pre-trained model as the features extractedfrom each image. in 1.d.iii - when it says original pre-trained model, is that talking about the cnn model we made?
post70-comment2-reply4: The first part is giving context, but &#34;For these pre-trained networks, you will only train the last fully connected layer, and will freeze all layers before them (i.e. we do not change their parameters during training) and use the outputs of the penultimate layer in the original pre-trained model as the features extracted from each image&#34; you should implement
post71: After adding augmentation layers in the CNN + MLP model, the error rate of train dataset decrease gradually while the val error rate change sharply. Is it normal? 
post71-comment1: Yeah same, I am having kind off consistently decreasing (not significantly) loss but accuracy is spiking in some weird manner. I dont know whats going on! Please if anyone can guide. 
post71-comment2-reply4: This behavior is dependent on the train-val split used and is normal. There would be a general trend followed by the validation error if you let it run for multiple epochs and smoothen it out, but that is not required for this project.
post71-comment3-reply4: I believe this happens due to a large learning rate. You can probably decrease the learning rate while doing model.compile(optimizer=Adam(learning_rate=0.002) for e.g. I tested this and got a smoother validation accuracy
post71-comment3-reply4: I belive this happena due a large learning rate. You can probably decrease the learning rate while doing model.compile(optimizer=Adam(learning_rate=0.002) for e.g. I tested this and got a smoother validation accuracy
post72: The epoch bar is not there anymore when running my model?
post72-comment1-reply4: <md>You can try specifying the "verbose" parameter since that's what controls printing.</md>
post72-comment2: i did try that i set it to 1 but all I see is epoch 1 and then nothing 
post72-comment2-reply1: How many epochs did you specify to run? The default is 1
post72-comment2-reply2: I specified 100
post72-comment2-reply3: only thing I can think of is I did change some parameters but I'm not sure why that would cause this issue
post72-comment2-reply4: well this is weird it works for my transfer learning model maybe ill try remounting to see if that does anything 
post72-comment2-reply5: I believe that once you see epoch 1, it means 'verbose' should have been set to True. It took me some time to see the training history after it showed Epoch 1. Maybe you can wait for a while, or adjust the parameters to reduce the model complexity and check if it works for a simpler model. Are you using Tensorflow GPU?
post72-comment2-reply6: could it be im doing my data augmentation incorrectly? As Im running on GPU for colab 
post72-comment2-reply7: Maybe try to simplify the image augmentation? 
post72-comment2-reply8: how would I simplify it I feel like what I'm doing is already fairly bare bones
post73: Hi, I tried to train the CNN model, but met the ValueError like this image. Does anyone met similar problem? 
post73-comment1-reply8: It’s happening because your model doesn’t recognise the shape of your inputs. Try to explicitly set the input shape inside your model to something like (299,299,3) for example and see how it goes. 
post73-comment2: Check this one! It helps @1027
post74: Why is the precision and recall coming 0.0 and 0.0 for background class? Is there any way to improve it? I have following layers in my model A pre-trained convolutional base (EfficientNetB0) for feature extraction.Global Average Pooling layer to reduce spatial dimensions.A dense layer with 512 neurons and ReLU activation.Batch Normalization layer for normalization.Dropout layer for regularization.Output dense layer with softmax activation for multi-class classification. 
post74-comment1-reply8: It’s probably because your model is recognising only one class as a label to classify your images. Did you use categorical_crossentropy by chance? If so, change it to sparse_categorical_crossentropy.
post74-comment2: I used sparse_categorical_crossentropy and I still got the same results...Any other suggestions or are these normal results?
post75: I have been trying to get good accuracy with keras but have not been unable to do so, hence I used Pytorch and it worked much better, can I submit both files ?
post75-comment1-reply8: <p>I guess you can try to improve your model with Keras, because TA mentioned that the grading will be based on the Keras. @1017_f9</p> <p></p>
post75-comment2: I have been trying to do the same but in case it does not work out, should i just upload Torch or both ?
post75-comment2-reply1: I have the same situation with you, I just uploaded the Torch part. which is successful. 
post76: I completed the whole project in google colab. For the final submission, is it ok to download the code and upload to github? The code contains specific lines to mount the drive, which jupyter notebook does not need. Should I be commenting out the mount/unzip code for the final submission? My code also doesnt contain anything specific to jupyter notebook because I did the whole project in colab. can I just submit my colab notebook as is? Thanks ! 
post76-comment1-reply1: I think it should be okay to submit the colab notebook. I believe the grading will be mainly based on the code where we train the model and report the result.
post76-comment2: can a TA confirm? Thanks ! 
post76-comment3-reply1: @1153
post77:  Hi, I printed out the Precision, Recall, and F1 score for VGG16 model. But the result was confusing. I wonder if anyone got the same result as me? Or what should I do to adjust the code for correct model training? 
post78: I think we need a flatten layer between CNN and the dense layer. Do we need it between the pre-trained model and the final layer? Thanks.
post78-comment1-reply1: For me, I did not use flatten layer. Instead, I used GlobalAveragePooling2D(), which takes the place of Flatten() by converting the 3D output of the last convolutional layer to a 1D tensor.
post78-comment2: Thank you. I also tried ResNet50 without flatten layer or the layer you mentioned will result in error. (Though there was no error with EfficientNetB0). 
post79: Hi, I have two questions regarding the project instructions: 1) When it says "Keep the network parameters that have the lowest validation error", does it mean save the model that has the lowest validation error? 2) "Report Precision, Recall, and F1 score for your model". Is this for the model that has the lowest validation error? Thank you.
post79-comment1-reply1: 1. From my understanding, you need to save all 4 models while making sure each model has low validation error (you can do that using checkpoints)<div>2. I think this applies to all models </div>
post79-comment2-reply1: yes and yes
post80: Can I use input_shape=(100, 100, 3) instead of (299, 299, 3)? The latter is causing significant delays, and I'm exploring a faster alternative.
post80-comment1-reply1: I don’t think it’s a good idea to further reduce image size as it is already a bit small and each image contains very little data. Someone please correct me if I’m wrong here. 
post80-comment2: There are other tricks to reducing the number of parameters that are more typical of CNNs, like max pooling layers. Using those should reduce your runtime if used appropriately.
post80-comment2-reply1: That said, image downsampling is a very easy and effective way to reduce your computational load. And at the end of the day, if your model works, it works. Who's to say the model trained on 10000x10000 images is better than the one you did with 100x100 if the classification accuracy is only a few points higher, the training time is measured in dog years and you needed some alien-tech GPUs to do it?
post80-comment3: Could a TA please confirm if it is allowed to use the downsampled shape for training?"
post80-comment3-reply1: You are allowed to resize if it helps in making computation easier.
post81: Does anyone know how to solve this error in EfficientNetB0. Also can you specify if we are supposed to resize the images to (224,224,3) for this network?This is my network: base_model = EfficientNetB0( weights='imagenet', # Load weights pre-trained on ImageNet. input_shape=(224, 224, 3), include_top=False) # Freeze all layers in the base model for layer in base_model.layers: layer.trainable = False # Create a new model on top of the pre-trained base model model = models.Sequential([ base_model, layers.Dense(128, activation='relu'), layers.BatchNormalization(), layers.Dropout(0.3), layers.Dense(2, activation='softmax') # Assuming a binary classification task ]) # Compile the model model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Display the model summary model.summary() history = model.fit( tf_dataset_train_combined, # Replace with your training dataset validation_data=tf_dataset_val, # Replace with your validation dataset epochs=30, callbacks=[EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True)], batch_size=5 ) 
post81-comment1-reply1: <p>When in doubt, read documentation:</p> <p><a href="https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#:~:text=This%20model%20takes%20input%20images%20of%20shape%20%28224%2C%20224%2C%203%29%2C%20and%20the%20input%20data%20should%20be%20in%20the%20range%20%5B0%2C%20255%5D.%20Normalization%20is%20included%20as%20part%20of%20the%20model." target="_blank" rel="noopener noreferrer">https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#:~:text=This%20model%20takes%20input%20images%20of%20shape%20(224%2C%20224%2C%203)%2C%20and%20the%20input%20data%20should%20be%20in%20the%20range%20%5B0%2C%20255%5D.%20Normalization%20is%20included%20as%20part%20of%20the%20model.</a></p>
post81-comment2-reply1: <p>The EfficientNetB0 does support (299,299,3).</p> <p>However for the ResNet50 and VGG16, you would need to resize your images to [224, 224]</p>
post81-comment3: We could also use include_top = False and input_shape = (299,299,3) parameters? as an alternate solution to resizing the images to [224,224]?https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50#:~:text=for%20the%20model.-,input_shape,-optional%20shape%20tuple
post82: Hi instructors, Is it fine if I make the submission in the following format? 1. notebook 1 - CNN+ MLP model 2. notebook 2 - Transfer Learning - EfficientNet80 model 3. notebook 3 - Transfer Learning - ResNet50 model 4. notebook 4 - Transfer Learning - VGG16 model 5. README.txt - Explaining the contents of each file in the submission Of course, each notebook will have the data preprocessing part This helps me a lot since I have been facing kernel dying issues on Colab 
post82-comment1-reply1: I believe you should put all the implementation into one notebook since you need to compare all models and its results at the end.
post82-comment2: We can have different notebooks as confirmed by the instructor: https://piazza.com/class/lll6cacyxjfg3/post/1147
post82-comment3-reply1: @1147
post82-comment3-reply1: Thats fine
post82-comment4: Just confirming, @1147 talks about having two notebooks, one for cnn+mlp and the other for three transfer learning models. The way I’ve done it has one notebook per model , that’s 4 notebooks , with the last notebook having all the comparison details. Can an instructor confirm if this is fine or should I redo all my transfer learning models into a single notebook?Thanks!!
post82-comment4-reply1: I don't see any reason why this wouldn't be fine! 
post83: I am getting its length as 137 in google colab and when I ran in jupyter notebook it was 928. Any one with any suggestions?
post83-comment1-reply1: If you are using colab consider doing flush_and_unmount <br /><br />I was also having this issue but after unmounting one time i havent had that problem anymore 
post84: I've tried different combinations of data augmentation and the MLP layer but I'm only ever getting the same accuracy. I know a lot of other students are facing the same problem without any solution. Any tips on how to fix this? 
post84-comment1: Same issue with the EfficientNetB0 model, but the other models worked fine.
post84-comment1-reply1: Did you end up fixing it for efficientnetb0? 
post84-comment2-reply1: @1165
post85: I've seen similar posts, but haven't found a resolution. I've been tuning parameters, but during epoch training for transfer learning the validation accuracy remains the same. Any ideas to get out of this? Thanks.
post85-comment1: Same issue here, any solution? 
post85-comment2-reply1: @1165
post86: I just wanna know do we need to add the dataset also in the repo? or just the code file in the repo. Thank you!!
post86-comment1: just notebook code file
post86-comment1-reply1: thank you
post86-comment2-reply1: @1017
post87: The validation loss fluctuates after some epochs, which happens before 20 epochs (as mentioned in the question). So, should I do the early stopping before reaching 20 epochs if training loss and validation loss converge? PS: From previous posts, I observed different answers.
post87-comment1-reply1: <p>Well the assignments states as least 20 epochs so I assume no </p> <p></p> <p>prob just use start_from_epoch and set it to what they want </p> <p></p> <p>20 for CNN </p> <p></p> <p>and at least 10 during transfer</p> <p></p>
post87-comment2: 20 for CNN, then expected is 20 epochs; what is the point in using early stopping?
post87-comment2-reply1: well I think your supposed to set epochs to a larger number and then use early stopping after 20 iterations supposedly the loss may fluctuate a lot before we get to 20?
post87-comment3: According to @1114, the instructor clearly stated that epochs should be set to minimum requirement, but it is ok if it early stops before the minimum requirement.
post87-comment4-reply1: @1114
post88: Hi, I have used dataset in data.zip from the dropbox link. After pre-processing, the length of the train, test and val datasets turns out to be the following - Can someone please verify if this is correct? Thanks in advance!
post88-comment1-reply1: yeah, I also got the same.
post89: How to suppress this warning ? 
post89-comment1-reply1: <md>If it's in tensor flow, you can suppress warnings with ``` tf.get_logger().setLevel('ERROR') ```</md>
post90: For the transfer learning question, I wanted to confirm the following:- We add custom layers to EfficientNetB0, ResNet50, and VGG16 models, running all 3 on training data and using val data for early stopping beyond 10 epochs. - Based on the training / validation error, we choose the best model and run that on test data- report the training & test metrics of precision, recall, f1
post90-comment1: For 3. Report the precision, recall, and f1. I guess TA mentioned that we should report for all datasets. @1049
post90-comment2-reply1: <p>Early stop @1114</p> <p>Yeah train val for choosing transfer method</p> <p>Yeah for everything</p>
post91: If this is taking a long time Is it safe to interrupt the cell and just kill the runtime 
post91-comment1-reply1: someone said previously that this took them an hr or so
post92: Similar problem with post @1036 & @ 1125. I tried increasing the learning rate from default 0.001 to 0.01 and 0.1, doesn't help. I also tried decreasing the learning rate to 0.0001, still getting this: Anyone knows what other parameter I should tune that will help? Thank you!
post92-comment1: Also, could a TA answer if this is acceptable or not? 
post92-comment1-reply1: I guess not. It means the model isn't learning...
post92-comment1-reply2: Lol you're right, any luck yet? Nothing for me :( 
post92-comment1-reply3: I feel I have tried all parameters combinations for EfficientNetB0 but still no improvement
post92-comment2-reply3: As long as you are using the proper methods, remember no free lunch, nothing is guarateed
post93: Hi, I have finished till (d).v, and was moving towards the comparison part. Just wanted to confirm 2 doubts: 1) We need to present the comparison for the testing dataset metrics that we obtained, right? 2) Also, while capturing the classification report for transfer learning, I realized later that I only printed the classification report, but did not store its results in any variable (which I could've used later for the comparison purpose). So, is it okay if for comparison, I refer to these printed outputs of classification report? Thanks in advance!
post93-comment1-reply3: <p>1. yeah</p> <p>2. as long as you have the results and clearly say where they are its ok</p>
post94:  I noticed that for the img_list,lablel_list for tf_dataset_set is made from tf_dataset_train. Shouldn't it be from tf_dataset_test?
post94-comment1-reply3: Yes change it to test. The TA specified in @1025 that it is a typo.
post95: Hi, can someone please confirm what is the length of their train, test and validation datasets after training and augmentation? Thank you!
post95-comment1-reply3: Train - 442<div>Test - 247</div><div>Val - 250</div>
post95-comment2-reply3: Since not everyone has the same augmentation there is no answer
post95-comment3: My pre-augmentation has more data than the student answer?
post95-comment3-reply1: although it may depend on batch size
post96: I have 2 doubts: 1. I am training my model in colab and my path to the folder containing images is data_path = '/content/data'. So, Do I need to change it later while submitting my file?2. Can I submit 4 different colab notebooks for all 4 models in my project? Since the runtime gets disconnected and all my training is lost.
post96-comment1: I have mounted my drive and then unzipped the data in the colab instance otherwise the I/O took a lot of time and affected runtime, so during submission do we comment and keep our data extraction code in it? or how do we handle submission in such a case?
post96-comment2-reply1: @1151 @1017_f14
post96-comment3: So, its okay to submit multiple colab notebooks right? 
post96-comment3-reply1: It is okay. But make sure to properly mention which notebook contains which results so that its easy for your grader to figure out what you have done.
post97: I have been trying to perform hyperparameter tuning for quite sometime now. I have tried a couple of combinations. But my accuracies lie between 60-65% at best. I have read posts where people have got accuracies above 80. Should this be a reason for concern?Any tips/ suggestions that I could try to increase the accuracy is highly appreciated.
post97-comment1-reply1: <md>I got 93% test accuracy. WIth following parameters, > Agumentation : brightness, contrast, rotate only. (no zoom, no flipping), I believe HIRISE images rarely get different zoom levels and flipped. > optimizer = Adam(learning\_rate=0.00001) ``` model = Sequential([ # 1 Conv2D(32, (3, 3), padding='same', input_shape=(299, 299, 3), kernel_regularizer=l2(0.1)), BatchNormalization(), ReLU(), MaxPooling2D(pool_size=(2, 2)), # 2 Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(0.1)), BatchNormalization(), ReLU(), MaxPooling2D(pool_size=(2, 2)), # 3 Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(0.1)), BatchNormalization(), ReLU(), MaxPooling2D(pool_size=(2, 2)), Flatten(), Dense(128, activation='relu', kernel_regularizer=l2(0.1)), Dropout(0.3), Dense(2, activation='softmax') # for binary classification with softmax ]) ``` Ran 20 epochs, it didn't stopped for patience set to 7.</md>
post97-comment1-reply1: <md>I got 93% test accuracy. WIth following parameters, > Agumentation : brightness, contrast, rotate only. (no zoom, no flipping) > optimizer = Adam(learning\_rate=0.00001) ``` model = Sequential([ # 1 Conv2D(32, (3, 3), padding='same', input_shape=(299, 299, 3), kernel_regularizer=l2(0.1)), BatchNormalization(), ReLU(), MaxPooling2D(pool_size=(2, 2)), # 2 Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(0.1)), BatchNormalization(), ReLU(), MaxPooling2D(pool_size=(2, 2)), # 3 Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(0.1)), BatchNormalization(), ReLU(), MaxPooling2D(pool_size=(2, 2)), Flatten(), Dense(128, activation='relu', kernel_regularizer=l2(0.1)), Dropout(0.3), Dense(2, activation='softmax') # for binary classification with softmax ]) ``` Ran 20 epochs, it didn't stopped for patience set to 7.</md>
post97-comment2: Thanks for sharing! What's the percentage of data you treat with augmentation？I think I'm treating with 40%. My kernel numbers are like yours but other hyperparams different. I only got 83% now.
post97-comment2-reply1: I treated all of the training datas.
post97-comment2-reply2: Your learning rate is much lower than mine. I'm going to tune that part next. Btw, why did you use regularizers in all CNN layers? Thanks again.
post97-comment2-reply3: May refer to @1107_f7 Good luck!
post97-comment2-reply4: could u share how ur generating ur classification report? I am able to achieve high test accuracy while using evaluate function. But i get much lesser scores when i use model.predict .
post97-comment2-reply5: Same! That happens for me too. 
post97-comment2-reply6: Hope this helps, ``` def evaluate_model(model, dataset, show_output=True): y_true = [] y_pred = [] for images, labels in dataset: predictions = model.predict(images, verbose=0) # Does not print progress. y_true.extend(labels.numpy()) y_pred.extend(np.argmax(predictions, axis=1)) report = classification_report(y_true, y_pred) return report train_report = evaluate_model(best_model, tf_dataset_train) val_report = evaluate_model(best_model, tf_dataset_val) test_report = evaluate_model(best_model, tf_dataset_test) ```
post97-comment2-reply7: I'm able to replicate this now. Thanks!
post97-comment2-reply8: Have you used one-hot encoding for the labels of all the datasets? (train, val, and test)
post97-comment2-reply9: Thanks for the help, it worked!
post97-comment2-reply10: I'm having the same problem. The accuracy of the output when the model was training topped out at around 0.9, but when I took the trained model for a report, the accuracy dropped back down to around 0.5, can anyone give any help?
post97-comment2-reply11: How are you calculating it? I was calculating it myself and had the exact same problem, but with the code above (Thanks @Seungil), my results are very similar (not exactly the same, but +- 10%) as compared to the values during training. 
post98: Sometimes when I am running, my computer says theres and issue and immediately begins restarting. And without saving work. Does anyone know how to not have this happen? I havent been able to figure it out from looking online.
post98-comment1-reply11: Sounds like your computer is broken or overloaded somewhere, use collab
post99:  Matrix size-incompatible: In[0]: [32,102400], In[1]: [175232,256] [[{{node sequential_6/dense_6/Relu}}]] [Op:__inference_train_function_205539] I am getting this error and I have included Flatten() before dense layers. Anyone could suggest any solution?
post99-comment1: Maybe something to do with how you augment your data?
post99-comment1-reply1: Sorry, I didn't include specifics. I got this error after data augmentation. During C(ii), when I was training 3 layer CNN.
post99-comment1-reply2: hard to tell without the code but from other posts possibly when you crop you need to resize most likely a shape issue -> there is another post that explains what they did to fix it One was with the preprocessing code and another was them direcrtly setting the shape before training
post99-comment1-reply3: I checked that, the code during preprocessing is giving me black images if I am rescaling img = img / 255.0.
post99-comment1-reply4: im not sure what your using but I was using OpenCV resize function 
post99-comment1-reply5: did you figure this out? having same issue
post100: so i've been running on jupyter and keep getting resource errors even on the first epoch of the first time I'm trying to train my model. I've finally decided to try google collab but I'm not getting the same training data set that I got in jupyter even after mounting drive and using the supposed files correctly and can't seem to figure out why my training data from the ORIGINALLY PROVIDED CODE only has 2 images. Does anyone have any tips for using collab/ had a similar issue?
post100-comment1: I'm also dealing with this issue 
post100-comment1-reply1: So I think I may have fixed it? But I unflushed and remounted my google drive and then reextracted everything I also decided to do this in a different notebook from my code for the project The data in the drive seems to be correct but now unflushing is taking a long time (1+ hours ) so there is that issue 
post101: Question says - to perform rotate, flip, zoom and so on .. can we skip like 1 or 2 of them for example - is it ok if i perform just flip and rotate and not zoom ?
post101-comment1: @1043
post101-comment1-reply1: Thanks
post102: I disconnected my runtime to rerun my code on Collab and now the total data I have has dropped considerably? I really don't understand why this is happening 
post102-comment1: It's hard to say without more info. Where is the data that you are accessing in collab? Did you upload the data to drive or are you pulling it from somewhere?
post102-comment1-reply1: I downloaded the zip file from the website and then uploaded it to drive as I was told using the zip file from the project-folder was incorrect Then I extracted the data through colab and then it was working fine I tried to disconnect the runtime to rerun everything and now the datasets are all way smaller than before 
post102-comment1-reply2: Did you figure out how to fix this? 
post102-comment1-reply3: ended up remounting my drive after flushing it
post103: I've noticed that during the training process, both my training and validation datasets achieve accuracy rates of 0.8 or even higher. However, when I use my trained model to make predictions on the training and validation sets, the accuracy drops to around 0.5. This seems weird, and I can't find a solution. Can anyone offer some help?
post103-comment1: @1146
post103-comment1-reply1: @1141
post103-comment1-reply2: Sry, I didn't find some very useful information.
post103-comment2-reply2: <md>Try using the evaluate function in the model, it should produce consistent results with the batched data.</md>
post103-comment3: can anybody offer some tips?
post104: How exactly would we submit if we ran on colab? As the relative pathing would be different i think? Do we just run all like we do for jupyter and then download the notebook and then push it like usual? Just would like some guidance on this part 
post104-comment1-reply2: <md>I think you can run all on collab to generate the outputs and download the notebook with the outputs. Then maybe comment out the Google collab specific code and make sure your code could run without error if it was run locally. That does not mean you have to run it locally, just that it could run locally if someone wanted to.</md>
post104-comment1-reply2: <md>I think you can run all on collab to generate the outputs and download the notebook with the outputs. Then maybe comment out the Google collab specific code and make sure your code could run without error if it was run locally.</md>
post104-comment2-reply2: Its just as normal submission (the ran notebook from collab), @1151 for path
post104-comment2-reply2: Its just as normal submission, @1151 for path
post105: Hello, I understand that we used relative paths for homework, but I was wondering if I will lose marks for not including relative paths in my project notebook. I am using Colab, and have already reached the GPU limit on my two accounts. Re-running the notebook would take an entire day, and I don't have access to GPUs on my accounts. It's causing a real problem for me. I hope you understand.
post105-comment1-reply2: If its collab it is ok, we understand
post105-comment2: Thank you!
post106: In the Pre-processing code provided, there seems to be a small error in the test data generation part, as highlighted below in the image. The highlighted part should be tf_data_test. Can anyone confirm this? 
post106-comment1-reply2: <p>That is correct.</p> <p></p> <p>It has been confirmed by an instructor here: @1025</p>
post106-comment1-reply2: <p>That is correct.</p> <p></p> <p>It has been confirmed here: @1025</p>
post106-comment1-reply2: That is correct.
post107: I've paid for Colab+ ($10 subscription), and it says it's running out of memory space for building the model (which I had done locally but took 8+ hours). Anyone have a solution here? I've posted the error and settings below: 
post107-comment1-reply2: You could break it up into multiple notebooks, or reset ur environemnt before running the next model (remember to save locally if u do that)
post107-comment2-reply2: im having the same issue, what did you end up doing to fix it?
post107-comment3: pay $10000 then retry
post107-comment3-reply1: my macbook isn't working either :(
post108: Does anyone have some tips on how to make the code run faster? For the VGG16 model, colab takes 10 hrs+ per epoch ETA and Jupyter 4 hrs+ per epoch. Am I doing something wrong?
post108-comment1-reply1: <md>VGG16 takes a ridiculous amount of time to run for me as well. Only thing you can really do is make sure that the VGG16 layer is frozen and your dense layer is not too big.</md>
post108-comment2: I purchased Colab Pro and that helped significantly 
post108-comment2-reply1: you need to run locally anyway, they won't take google colab pro from my understanding. I hope I am wrong
post108-comment2-reply2: wait then if it takes forever to run what do we do?
post108-comment2-reply3: You can run it on collab to generate the outputs for your submission. Just make sure it wouldn't run into any errors if you were to run it locally.
post108-comment2-reply4: The Colab Pro or Colab Pro+? Thanks
post108-comment3: how big is your dense layer?
post109: Can we maintain 2 separate notebooks for CNN + MLP & Transfer Learning? I have worked on CNN + MLP & at times kernel dies while executing code due to resource exhaustion, so I want to avoid losing outputs of CNN+MLP while executing Transfer Learning (just in case kernel crashes again) 
post109-comment1-reply4: That should be fine, just make sure to label them clearly so they are not missed
post109-comment2: Can we make one per method for a total of 4 notebooks? Each has the same preprocessing code and just trains a different model? Thank you! 
post110:  I am getting a very good test accuracy but it does not reflect on my f1-score from the cnn+mlp model. Does anyone have any suggestions to why this would be happening?
post110-comment1-reply4: <p>I have the same problem like that. I suspect that it is because the label extraction problem. can I see your code around the classification_report? Here is mine:<br /><br /></p> <p>def compute_metrics(dataset, model):<br /> predictions = model.predict(dataset)<br /> predicted_classes = [1 if prob[0] &gt; 0.5 else 0 for prob in predictions]</p> <p> true_labels = [label.numpy() for img, label in dataset.unbatch()]</p> <p> return classification_report(true_labels, predicted_classes)<br /><br />mine is</p> <div> <div> <div>test_predictions = model_resnet50.predict(tf_dataset_test)</div> <div>test_predictions = np.argmax(test_predictions, axis=1)</div> <div>true_labels = np.concatenate([y for x, y in tf_dataset_test.unbatch().batch(batch_size)], axis=0)</div> <div>print(classification_report(true_labels, test_predictions))</div> </div> </div>
post110-comment1-reply4: <p>I have the same problem like that. I suspect that it is because the label extraction problem. can I see your code around the classification_report? Here is mine:<br /><br /></p> <p>def compute_metrics(dataset, model):<br /> predictions = model.predict(dataset)<br /> predicted_classes = [1 if prob[0] &gt; 0.5 else 0 for prob in predictions]</p> <p> true_labels = [label.numpy() for img, label in dataset.unbatch()]</p> <p> return classification_report(true_labels, predicted_classes)<br /><br />mine is</p> <div> <div>report = classification_report(true_labels, test_predictions, target_names=[&#39;background&#39;, &#39;frost&#39;])</div> <div>print(report)</div> </div>
post110-comment1-reply4: <p>I have the same problem like that. I suspect that it is because the label extraction problem. can I see your code around the classification_report? Here is mine:<br /><br /></p> <p>def compute_metrics(dataset, model):<br /> predictions = model.predict(dataset)<br /> predicted_classes = [1 if prob[0] &gt; 0.5 else 0 for prob in predictions]</p> <p> true_labels = [label.numpy() for img, label in dataset.unbatch()]</p> <p> return classification_report(true_labels, predicted_classes)</p>
post110-comment2: Getting the same results. Test loss and accuracy seem fine. Doesn't translate well to the F1 score
post111: Does anyone understand what could be the cause of this?
post111-comment1-reply4: fitting issue? potential data leakage? Tune your parameters and check for leaks
post111-comment2: i don't understand how it could be data leakage if i did not alter the data preprocessing steps
post111-comment2-reply1: As a follow up, I tried running the model without augmenting the images and got a very decent validation accuracy. Should I just not augment the images then?
post111-comment2-reply2: If that is indeed the case for you, you should then include both in your file and state your final findings
post112: Hi, my output for the epoch looks like this: Epoch 1/20 130/928 [===>..........................] - ETA: 55:16 - loss: 4.6852 - accuracy: 0.7214 but I saw other's looks like this: Epoch 1/20 556/928 [================>.............] - 117s 4s/step - loss: 5.3966 - accuracy: - val_loss: xxx - val_accuracy: xxx Why my output can not show the val_loss and val_accuracy?
post112-comment1-reply2: Try setting your verbosity to 2.
post112-comment2-reply2: You should be able to see it once the epoch is completed.
post113: My training time/epoch is really high and at this point I have tried various recommendations from online forums regarding this issue. From what I understand this is a widespread issue as metal plugins for macOS Sonoma have limited compatibility. Just wanted to know what specific versions of python, base tensorflow and tensorflow metal worked for anyone who has reasonably quick training time/epoch. I have already tried the following codewise: increased the batch size, simplified my model architecture (as much as I can without violating the requirements), and reduced no. of augmentation layers. But if I am missing something then pls do let me know. Thanks for the help!
post113-comment1-reply2: <p>For the final project, you may use cloud resources like Google Colab if your training time is too long and nothing else works. However, this may cost you some money, and make sure your notebook runs locally too as that is what we&#39;ll be evaluating the project on.</p> <p></p> <p><a href="https://colab.google/" target="_blank" rel="noopener noreferrer">https://colab.google/</a></p>
post113-comment1-reply2: <p>You may use cloud resources like Google Colab if your training time is too long and nothing else works. However, this may cost you some money. </p> <p></p> <p><a href="https://colab.google/" target="_blank" rel="noopener noreferrer">https://colab.google/</a></p>
post113-comment2: Yes, that was my backup option but the dataset takes ages to upload on drive or on GitHub. 
post113-comment2-reply1: sounds like a skill issue Well if you're talking about having to reupload each time you restart the runtime on Colab, make sure you're mounting your drive so Colab's VM can get access to your data immediately. If you're just talking about uploading the files to your drive then start now, work on other stuff in parallel while you wait. Read documentation. Good luck!
post113-comment3:  Google Colab? M R Rajati’s phone 
post113-comment4: python=3.9.18 tensorflow-macos=2.15.0 tensorflow-metal=0.1.2 These are not the latest versions but they were able to use GPU in my case (M1, MacOS Sonoma)
post113-comment4-reply1: Hi, I'm also using metal plug in running on M1 but I'm not sure whether my GPU is on. What's your running time per epoch for part c(ii)? Mine is like 10min/epoch.
post113-comment4-reply2: Must be using GPU then as without GPU it takes about 25 min/epoch
post114: solved!
post114-comment1: Maybe the Dense layer needs to be 1 rather then 4?' Also some people have said using sigmoid rather than softmax fixes their issues too
post114-comment2: Try changing num_classes to 2 and change loss to sparse_categorical_crossentropy
post114-comment2-reply1: This helped, thank you!
post115: I had a good accuracy and val_accuracy while in training, but for the report part, they became bad. Is that normal? 
post115-comment1: I assume it can be mainly because of two reasons, (please correct me if I am wrong) 1. We utilized data augmentation during the training, which is why we achieved good accuracy during the training process. 2. Also, we save the model with the lowest validation error,(the model might be an underfit)
post115-comment1-reply1: Got same problem! Anyone can help?
post115-comment2-reply1: <md>If you are inputting the batched data for prediction, there could be a misalignment based on how you are getting the true labels. @1107_f4 might help for this.</md>
post115-comment2-reply1: <md>@1107_f4 might help for this.</md>
post116:  Hello, Can any one help me with the following error, I tried to set the the shapes as suggested in post @1027 but still no luck. ValueError: as_list() is not defined on an unknown TensorShape. So, maybe the issue with the model creation, so can someone check it and give any insight or help # Image input shape input_shape = (299, 299, 3) # Create the CNN def build_cnn(input_shape): model = models.Sequential() # Three-layer CNN model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape)) model.add(layers.BatchNormalization()) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation='relu')) model.add(layers.BatchNormalization()) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(128, (3, 3), activation='relu')) model.add(layers.BatchNormalization()) model.add(layers.MaxPooling2D((2, 2))) # Flatten layer model.add(layers.Flatten()) # Dense layer (MLP) model.add(layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))) model.add(layers.BatchNormalization()) model.add(layers.Dropout(0.3)) # Softmax activation model.add(layers.Dense(2, activation='softmax')) return model # Create the model model = build_cnn(input_shape) # Compile the model with 'sparse_categorical_crossentropy' model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) # Display the model summary model.summary() # Set up Early Stopping callback early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # Train the model history = model.fit(tf_dataset_train_combined, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping]) Thank you 
post116-comment1-reply1: Use activation sigmoid and only 1 neuron in last Dense instead of 2
post116-comment2: I tried this too, but no luck:(
post116-comment2-reply1: Can you share how you used @1027 in your code
post116-comment2-reply2: sure, i tried both the first and second solution, and here is the first def load_and_preprocess(img_loc, label): def _inner_function(img_loc, label): # Convert tensor to native type img_loc_str = img_loc.numpy().decode('utf-8') label_str = label.numpy().decode('utf-8') # Load image using PIL and convert to RGB img = Image.open(img_loc_str).convert('RGB') # Convert PIL image to numpy array img = np.array(img) img = tf.image.resize(img, [299, 299]) # Normalize the image to the [0, 1] range img = img / 255.0 # Convert label to integer (assuming binary classification) label_int = 1 if label_str == 'frost' else 0 return img, label_int # Wrap the Python function X, y = tf.py_function(_inner_function, [img_loc, label], [tf.float32, tf.int64]) # Set the shape of the tensors X.set_shape([299, 299, 3]) y.set_shape([]) # Scalar label return X, y 
post116-comment2-reply3: for the second approach, I used this before start building the model def _fixup_shape(images, labels): images.set_shape([None, 299, 299, 3]) labels.set_shape([None, 2]) return images, labels tf_dataset_train_combined = tf_dataset_train_combined.map(_fixup_shape) tf_dataset_val = tf_dataset_val.map(_fixup_shape) tf_dataset_test = tf_dataset_test.map(_fixup_shape) 
post116-comment2-reply4: In fixup shape use [None, ] for labels instead of None, 2
post116-comment2-reply5: no, i got this InvalidArgumentError: {{function_node __wrapped__MakeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes at component 0: expected [?,299,299,3] but got []. [Op:MakeIterator] name: 
post116-comment2-reply6: Looks like shape of X is not right, try printing the shape just before training 
post116-comment2-reply7: Thanks for your help, I'll check it out again
post116-comment2-reply8: I commented out the data augmentation part and it seems the issue comes from here! because the model start to run Can you look at it? def flip_image(image, label): flip_code = random.choice([0, 1, -1]) augmented_image = cv2.flip(image.numpy(), flip_code) return augmented_image, label def zoom_image(image, label): zoom_range = 0.2 zoom_factor = np.random.uniform(1 - zoom_range, 1 + zoom_range) height, width, channels = image.shape new_height = int(height * zoom_factor) new_width = int(width * zoom_factor) # Resize the image resized_image = cv2.resize(image.numpy(), (new_width, new_height)) # Crop cropped_image = resized_image[:height, :width, :] return cropped_image, label def rotation_image(image, label): rotation_range = 0.2 rotation_angle = np.random.uniform(-rotation_range, rotation_range) height, width, _ = image.shape center = (width // 2, height // 2) # Perform rotation rotation_matrix = cv2.getRotationMatrix2D(center, rotation_angle, 1.0) rotated_image = cv2.warpAffine(image.numpy(), rotation_matrix, (width, height), flags=cv2.INTER_LINEAR) return rotated_image, label def adjust_brightness(image, label): brightness_range = [0.2, 0.1] delta = np.random.uniform(-brightness_range[0], brightness_range[1]) adjusted_image = tf.image.adjust_brightness(image, delta) return adjusted_image, label # Apply augmentation functions to the training dataset tf_dataset_train_flipped = tf_dataset_train.map(lambda x, y: tf.py_function(flip_image, [x, y], [tf.float32, tf.int64]), num_parallel_calls=tf.data.experimental.AUTOTUNE) tf_dataset_train_zoomed = tf_dataset_train.map(lambda x, y: tf.py_function(zoom_image, [x, y], [tf.float32, tf.int64]), num_parallel_calls=tf.data.experimental.AUTOTUNE) tf_dataset_train_rotated = tf_dataset_train.map(lambda x, y: tf.py_function(rotation_image, [x, y], [tf.float32, tf.int64]), num_parallel_calls=tf.data.experimental.AUTOTUNE) tf_dataset_train_brightness = tf_dataset_train.map(lambda x, y: tf.py_function(adjust_brightness, [x, y], [tf.float32, tf.int64]), num_parallel_calls=tf.data.experimental.AUTOTUNE) # Combine the augmented datasets tf_dataset_train_combined = tf.data.Dataset.concatenate(tf_dataset_train, tf_dataset_train_flipped) tf_dataset_train_combined = tf.data.Dataset.concatenate(tf_dataset_train_combined, tf_dataset_train_zoomed) tf_dataset_train_combined = tf.data.Dataset.concatenate(tf_dataset_train_combined, tf_dataset_train_rotated) tf_dataset_train_combined = tf.data.Dataset.concatenate(tf_dataset_train_combined, tf_dataset_train_brightness) # Shuffle tf_dataset_train_combined = tf_dataset_train_combined.shuffle(buffer_size=tf.data.experimental.cardinality(tf_dataset_train_combined)) 
post116-comment2-reply9: Maybe add padding after cropping cause that changes the size
post116-comment2-reply10: Yes! thank you it works fine now 
post116-comment2-reply11: Great!
post116-comment3-reply11: @1027 might help
post117: The accuracy of CNN+MLP is 0.80, but the accuracy of transfer learning is around 0.92, is that normal?
post117-comment1-reply11: <md>Yeah that's really good! Just keep @1108 in mind, they care more that your model is following the requirements from the project description more than the score it gets.</md>
post118: Are we supposed to split our BatchDatset object into X_train and y_train? do we just unzip tf_dataset_train I had just passed in tf_dataset_train and val into the model.fit but can I evaluate with them directly too?
post118-comment1-reply11: <md>Just use the tensors set up in the example like tf_dataset_train. You can use them to evaluate too.</md>
post118-comment2: But dont we need a y_true and y_pred for the classification report?
post118-comment2-reply1: use the labels as y_true
post118-comment2-reply2: Yeah you need y_true and y_pred for classification report but that's not explicitly required in the project spec. There are other ways of getting precision, recall, and f1 score.
post119: Can we use libraries like https://github.com/albumentations-team/albumentations for image data augmentation? They provide a lot of options.
post119-comment1-reply2: Fairly certain we can do whatever we want for this step 
post119-comment2-reply2: You can, but make sure to provide the needed readme and requirement.txt files if you can using something that is not commonly seen
post120: I tried to use ModelCheckpoints to save my model in case the code stopped working or in case of a crash and my model suddenly stopped running, but when I checked the place where the model was supposed to be saved, there was nothing. I am using Google Colab, is there any difference between using the filepath for Google Colab vs on Jupyter Notebook? For reference, this was the code I used checkpoint_filepath = '/content/drive/My Drive/DSCI-552/tmp/checkpoint.model.keras' checkpoint_dir = os.path.dirname(checkpoint_filepath) model_checkpoint_callback = keras.callbacks.ModelCheckpoint( filepath=checkpoint_filepath, monitor='val_accuracy', mode='max', save_best_only=True) 
post120-comment1-reply2: I used on jupyter notebook with the same code as yours and it&#39;s saving to the correct place.
post121: I am trying to unpack the images from the labels in colab using images_train, labels_train = zip(*tf_dataset_train) but this is taking a really long time. Has anyone else run into this issue or is there a faster way to do this? 
post121-comment1-reply2: Maybe try using a GPU runtime instead of the default CPU. I don&#39;t know if it will solve your issue, but overall was faster for me.
post122: Where can I find the train_source_images.txt, test_source_images.txt, val_source_images.txt files for 2.5GB data?
post122-comment1-reply2: It&#39;s there in the project github repo!
post123: How much data do we have in each When doing it in jupyter notebook i got Total number of samples in the train dataset: 14144 Total number of samples in the val dataset: 7904 Total number of samples in the test dataset: 8000 but the numbers dropped significatnly in colab? I redownloaded the zip file in the final project folder, was there a difference today and a couple days ago for the zip data? is that one maybe smaller?
post123-comment1-reply2: Use the data on the website (the one you should be using), @1096
post123-comment2: for some reason it says I can't access the website which is why I used the one from the project folder we were provided?
post124: How should I structure the directory of final project. and also the filename?
post124-comment1-reply2: follow the same as previous HWs (except you can leave the data folder as a placeholder), Name your jupyter file as Lastname_Firstname_Project.ipynb
post125: I was confused about Transfer Learning part. Should we run 3 pre-trained models separately and compare their results with CNN+MLP model or should we combine the 3 pre-trained models to train a final_model and compare the results of that to CNN+MLP.
post125-comment1-reply2: Pretty sure it’s separately! 
post126: I'm already finished the project but when I tried to run all of them. The transfer learning (d) was always stuck at the first epoch for several hours It was running fine if I restart the kernel and just run the TL model, but it failed if i run CNN model prior. Before fitting TL, i already redo the preprocessing, augmentation, and everything as the same as CNN. does anyone have any clue on this? thanks a lot!
post126-comment1-reply2: May be it&#39;s crashing, what is your batching size ?<div>Try reducing the batch size may be that works.</div>
post126-comment2: I have also tried reducing batch_size down to 5 to test this, still not working. but thanks for the guide! any clue on this?
post126-comment3: update: i have resolved this by just using jupyter lab. Running in pycharm causes kernel to die when changing model for me.
post126-comment3-reply1: evil pycharm
post126-comment3-reply2: 🥲 I never had the issue. Sometimes you need to change the IDE settings so that the kernel doesn't die.
post127: Hi, I see some conflicting answers here on piazza. For question c and d, do we need to set the max epochs to 20 and 10 respectively and then see if early stopping stops the model early, or do we run at least 20/10 epochs and then perform early stopping afterwards, eg with the start_from_epoch parameter? 
post127-comment1-reply2: <md>I think the wording in the project description is pretty clear, you have to run "at least" the given number of epochs. So early stopping must happen after the "at least" has been satisfied. This is supported by @1088 and can be easily implemented using `start_from_epoch` Plus I've seen that there tends to be a lot of variability in the accuracy of the first several epochs until ithe model learns enough, so allowing it to early stop during the period of variability will be more about if the validation set just happens to have good accuracy rather than a model that has learned well.</md>
post127-comment1-reply2: <md>I think the wording in the project description is pretty clear, you have to run "at least" the given number of epochs. So early stopping must happen after the "at least" has been satisfied. This is supported by @1088 and can be easily implemented using `start_from_epoch`</md>
post127-comment2-reply2: @1114
post128:  Since the CNN model takes such a long time to run through every epoch, is there a way to save the model once it is finished so that even if you close Python or your computer logs out, the model output is retained and you don't have to re-run it? 
post128-comment1-reply2: You can try using <a href="https://keras.io/api/callbacks/model_checkpoint/" target="_blank" rel="noopener noreferrer">Model Checkpoints</a> in Keras which gives you the option to save the best model and/or to save the model after a certain number of epochs.
post128-comment1-reply2: You can try using <a href="https://keras.io/api/callbacks/model_checkpoint/" target="_blank" rel="noopener noreferrer">Model Checkpoints</a> in Keras which gives you the option to save the best model and/or to save the model after certain epochs.
post128-comment2-reply2: if you want to save the model after all epochs you can try model_name.save(‘path’). If you want to only save the best model you get during training, you can use checkpoints. 
post128-comment3: And if I want to load the saved model from using Model Checkpoints, would I just use the keras.models.load_model(checkpoint_filepath) code? How would I continue from where I left off?
post128-comment3-reply1: Simply googling this gives the result https://www.tensorflow.org/tutorials/keras/save_and_load
post129: Hi! Did anyone keep getting the same validation accuracy of 0.3218 for the EfficientNetB0 Model? I have tried different hyperparameters combo and tried both sigmoid/softmax layers, but the issue still happened. Does anyone have any idea for this? Thanks!!!
post129-comment1-reply1: <md>I think that is the accuracy that you get when the model is predicting the same class for every data point (i.e. all 1s or all 0s). You probably need to tune your network parameters some more.</md>
post129-comment1-reply1: yes, the same thing is happening to me too ! Any help appreciated. 
post129-comment2: Do you guys store best_model.h5 or something similair to store previous best model? I was getting similar phenomenon but everytime I rerun the code I deleted best_model.h5 then did it again this resolved the issue.
post129-comment2-reply1: Hi! Did you mean setting restore_best_weights=False instead of restore_best_weights=True inside the EarlyStopping() function?
post129-comment2-reply2: yes im saving the model from question c. So you are saying i should try delete the model from question c before i run the efficient net? Im not sure how that would impact because its two different models. or do you mean in efficient net, try to not save the best model? 
post129-comment2-reply3: No. I meant, in my case I ran and reran for checking the code of each models (with different names) but it did made difference I deleted previous *.h5 s versus not deleting it.
post129-comment2-reply4: im not saving my model in c and i still run into this issue. Is it an issue with the network parameters?
post129-comment3: Same here! Do you solve the problem now?
post129-comment4: same issue here, how did you resolve it?
post130: Is there an expected runtime to go through 20 epochs for CNN model?
post130-comment1-reply4: <md>There is a lot of variability depending on the model and your computer specs but the range can be large @1024</md>
post131: Is anyone getting a memory error? --------------------------------------------------------------------------- MemoryError Traceback (most recent call last) Cell In[51], line 95 92 resized_images.append(resized_image) 94 # Convert TensorFlow tensor to NumPy array ---> 95 image_np = resized_image.numpy() 97 print(f"Original image shape: {image_np.shape}") 99 # Apply augmentation functions File ~\AppData\Local\anaconda3\Lib\site-packages\tensorflow\python\framework\ops.py:395, in _EagerTensorBase.numpy(self) 393 # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors. 394 maybe_arr = self._numpy() # pylint: disable=protected-access --> 395 return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr MemoryError: Unable to allocate 32.7 MiB for an array with shape (32, 299, 299, 3) and data type float32
post131-comment1-reply4: <p>I got the same memory issues with much bigger number 14gb.</p> <p>I resolved by doing it running CNN &#43; MLP only. Then reset the notebook (probably emptied memories) and ran EffNet only this time. Again reset the notebook, the ran ResNet50 only. Resetted, then again ran VGG.</p> <p>Obivously you should do basic preprocessing before each different runs.</p>
post131-comment1-reply4: <p>I get the same memory issues with much bigger number 14gb.</p> <p>I resolved by doing it running CNN &#43; MLP only. Then reset the notebook (probably emptied memories) and ran EffNet only this time. Again reset the notebook, the ran ResNet50 only. Resetted, then again ran VGG.</p> <p>Obivously you should do basic preprocessing before each different runs.</p>
post131-comment2: Did you get these errors on 1.c.i? I'm still trying to do the empirical regulation portion.
post132: when training my model, with this line:cnn = model.fit(tf_dataset_train, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping]) I get this error, before the first epoch is completed: I've double checked the shape of the data, and not sure what to do here
post132-comment1: update, am now getting this error:
post132-comment1-reply1: you can't expect everyone else helping you to debug line by line
post132-comment2-reply1: <p><a href="/class/lll6cacyxjfg3/post/1027_f5" target="_blank" rel="noopener noreferrer">https://piazza.com/class/lll6cacyxjfg3/post/1027_f5</a></p> <p></p> <p>check out this one for the as_list() error</p>
post133: Hello, Frankly, I am a bit lost on how to perform the image augmentation for part c (i). I understand that I am supposed to use the tf_dataset_train, but it seems that in all the resources that I search up, they are performing these augmentations on single images. Every time I try to create a loop to it for each image in the dataset, I get an error. Could anyone help me here? 
post133-comment1: What i did was add the image augmenting layers to my model, that way, whenever a new image is passed through the training step, it augments it. It also augments differently (since its random) each epoch, which simulates even more training data! 
post133-comment1-reply1: How were you able to create a loop to pass the images through? The tf_dataset_train is a batch dataset so the traditional methods I know don't seem to be working.
post133-comment1-reply2: I don’t think you need a loop. Since this is part of the model, as you feed in images (one at a time) they’re augmented first and then the model learns (on the augmented images) 
post133-comment1-reply3: But how do you get a discrete image from the batch dataset to feed to the model? Can we just put the tf_dataset_train into the model? I thought we need to feed the images and labels separately into the model?
post133-comment2-reply3: You can either apply data augmentation to the given TensorFlow <a href="https://www.tensorflow.org/tutorials/images/data_augmentation#option_2_apply_the_preprocessing_layers_to_your_dataset" target="_blank" rel="noopener noreferrer">dataset object</a> before training or, include a data augmentation stage in your <a href="https://www.tensorflow.org/tutorials/images/data_augmentation#option_1_make_the_preprocessing_layers_part_of_your_model" target="_blank" rel="noopener noreferrer">model pipeline.</a>
post133-comment3: If we add augmentation layer(s) in the model before CNN, it means we'll augment every training image, right? Is that the right thing to do? Thanks!
post134:  I'm using Colab and I tried to use zipfile, but unfortunately the data didn't unzipped successfully. Its created two folder one as data and other as _MacOS and both of them have data. Could anyone share your way to unzip the data? what changes did you make in the preprocessing code in particular to read unzipped file. 
post134-comment1: I'm also having this issue Also Does anyone know how to read in the txt files? it keeps saying file not found even though its there? absolute pathing is not working either 
post134-comment1-reply1: Did you mount your drive? That worked well for me.
post134-comment1-reply2: Yea I figured it out needed to remount since I added the txt files after I mounted already lol
post134-comment2-reply2: I just used one of the external apps for unzipping in google drive. It took about 15 minutes but worked well. Just double click the zip file and pick the app.
post134-comment3: can you tell me what app you used?
post134-comment3-reply1: If its zip extractor mine just keeps crashing?
post134-comment3-reply2: Colab takes long time reading files from the mounted google drive. I used terminal within colab (basic command lines, like, wget to download the file from URL, and then unzip). It worked fine and it was fast.
post135: I all got the same results as below. Is this a problem for the training? But all three plots are different. 
post135-comment1-reply2: <md>@1101</md>
post135-comment2: I don't think @1101 is the same issue for this problem. 
post135-comment2-reply1: They also have the same f1 score, precision, and recall. Their model is just always giving 1 while your model is always giving 0.
post135-comment2-reply2: I mean all 4 models got the same number of 0.34, 1.00, and 0.51. Does this mean that there is a problem with a the model? Should I retrain for the model?
post135-comment2-reply3: Yes
post135-comment3: Can someone please tell how to fix this? I am also getting 0 for one class
post136: CNN model stopped training after 10 epochs and the validation error increased leading to very low accuracy in test dataset. Is this okay? Or, there is something I can do about it? Is this output acceptable for 1c? The accuracy is 49% for testing.
post136-comment1-reply3: <md>I've seen it increase in accuracy after 20 even though there are dips before 20. You should be setting it so that it runs at least 20 as the instructions say @1040 @1114 @1116 @1088</md>
post136-comment2: But the code stopped by itself, I did not stop it. I guess, this is because of early stopping. Can you help me avoid the stop before 20 epochs
post136-comment2-reply1: Maybe try assigning a number to `start_from_epoch` in `EarlyStopping`
post136-comment3-reply1: @1114 you cant force an early stop
post136-comment3-reply1: @1114
post137: When performing early stopping, should we use `start_from_epoch` to ensure that the model doesn't stop prematurely before running the minimum 20 epochs for part c ii and 10 for d iv?
post137-comment1-reply1: <p>I think it is enough to set the epoch to 200 and set some patience for no improvement. If it stops after 15 epochs then just leave it as is.</p> <p></p> <p>Forcing the model to keep learning will likely overfit the data.</p>
post137-comment1-reply1: I think it is enough to set the epoch to 200 and set some patience for no improvement. If it stops after 15 epochs then just leave it as is.
post137-comment2: I set my start from epoch to 20 for c and 10 for d. It's the easiest way I could find to guarantee the minimum number of epochs that must be run. I don't think setting the patient value is useful for guaranteeing the minimum.
post138: Is there supposed to be a single dense layer or a hidden dense layer followed by an output dense layer for c ii? The language of Final Project document refers to, "**a** dense layer", but also refers to it as an "MLP" layer.
post138-comment1-reply1: <md>Yeah my understanding is that we need 1 dense hidden layer with relu and 1 dense output layer with sigmoid/softmax</md>
post139: If mine stopped after 9 epochs, is this okay? ![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fkjw2zepev894fm%2Ffb14eb03d70348a5d03a12e8412935807f2658c31ff30219222e023d71611eec%2Fimage.png)
post139-comment1-reply1: <md>~~Needs to be at least 20 @1040 @1116 @1088~~ Instructor said stopping before required value is ok in the below comment</md>
post139-comment1-reply1: <md>Needs to be at least 20 @1040 @1116 @1088</md>
post139-comment1-reply1: <md>Needs to be at least 20 @1040 @1116</md>
post139-comment2: but according to TA's answer here, it can be any but need to set maximum more than 20 "correct it could early stop and any time, just set the max to whatever the requirement is" @1021
post139-comment2-reply1: There is no maximum set in project description, only minimum. It needs to early stop after the minimum is reached. Look at @1088 which is endorsed by instructor. With my model for 1c I noticed that the validation accuracy had quite a bit of variability but became more consistent around ~15-20 epochs so I think that's probably why it's a minimum of 20.
post139-comment2-reply2: I endorsed @1088 because it says "run at least 10 but it should stop with early stopping", as in if it stops early its ok
post139-comment3-reply2: If it early stops before your goal that is ok, as long as you set the goal as the required epochs
post139-comment3-reply2: If ir early stops before your goal that is ok, as long as you set the goal as the required epochs
post139-comment4: Is it fine if it runs for 20 epochs and doesn't early stop? 
post139-comment4-reply1: I don't want to increase the number of epochs because it takes too long to run and crashes my kernel, but the accuracy is OK so was wondering if early stopping is super necessary (I still have the code there, the patience just isn't being reached). 
post140: Hi, I am a little confused about data augmentation in our project. In class, Prof. Rajati taught us how data augmentation is a way to introduce regularization. But, I also understand that data augmentation is used to increase the data size. But we already have plenty of data. So should we append to our existing training data with the modified training data or should we just apply transformations to our existing images and not append them to our existing training data(leaving the number of training data the same.)
post140-comment1-reply1: The amount of training data should not be changed I think. Augmentation is used to create more flexible data and prevent overfitting the noise and generalize better.
post140-comment2: So it is more randomly choosing images to augment in our training data?
post140-comment2-reply1: @1093
post140-comment3: hi, can an instructor please help clarify this? 
post140-comment4-reply1: @1043, more the better (its also for your practice to deal with images)
post140-comment4-reply1: @1043, more the better
post140-comment4-reply1: @1043
post141: Tensorflow detects my GPU when i check using the terminal, however it is not available on jupyter. Any idea how to solve this issue? 
post141-comment1-reply1: Activate your virtual environment and run Jupiter notebook in this environment.
post141-comment2: I have set the kernel on jupyter same as this conda environment. That is supposed to work right?
post141-comment2-reply1: It should work as long as you launch jupyter notebook as (py39) C:\Users\LIMOS>jupyter notebook The `(py39)` indicates a virtual environment. I would also suggest you to use Visual Studio Code with Jupyter Notebook extension, where you can choose what kernel or server you want.
post142: Can anyone share the time it takes to run an epoch, I'm running it on a cpu and it's almost 20 minutes for an epoch, so that works out to an estimated 30 hours or so for me to run through all the models each time. Considering that I might also make some changes, this would take much longer than I thought. I was thinking about running it on Colab, but it doesn't support decompression of data very well, the decompressed data is missing, and also I found that running an epoch on Colab takes 20min. Does anyone have a better method?
post142-comment1-reply1: I purchase Colab Pro and change runtime type to gpu v100
post142-comment2: Thanks for sharing, how much time does an epoch take?
post142-comment2-reply1: around 150～200s 
post142-comment2-reply2: i also got colab pro yet it still takes 10 min? Would it be a coding issue?
post142-comment2-reply3: prob related to your parameter settings
post143: I just found that the data.zip at dropbox has been deleted. Could TAs upload again? Because I need to use this data.zip to download from Google Colab, and the link addresses of the official data website seems could not work well as Google Colab does. Thanks! Just ignore this post. I just found that the official data website link could work.
post143-comment1-reply3: A new version of the dataset downloaded yesterday must be there.
post144: 
post144-comment1-reply3: <md>From office hours, the actual score is not very important. You just need to follow the steps in the instructions and your model has to show some level of learning (for example, it can't just pick randomly or the same category for every sample).</md>
post144-comment2-reply3: That is correct and as previously said on @1074
post145: I'm not really sure i'm doing right or wrong but just wished to share what I have so far. I agumentated train dataset only without filpping (since I hardly believe those satellite telescope flims flipped images). I used the same agumentated train dataset across all the problems. <CNN + MLP : Running time 5hrs> Precision: 0.3277 Recall: 0.5 F1 Score: 0.3959 <EfficientNetB0 : Running time 40mins, Steps /5> <ResNet50 : Running time 1hrs, Steps /5> <VGG16 : Running time 1.5hrs, Steps /5> Some of models have high Recalls which I'm figuring out what to fix.
post145-comment1-reply3: what&#39;s your test accuracy?
post145-comment2: I have results kind of similar to yours.
post145-comment3: My CNN+MLP is higher than yours, rest three models are similar. BTW how many epochs did you run for VGG?
post145-comment3-reply1:  I am getting these stats for the CNN+MLP model. Do you think these are good results or should I change anything in the model to improve it? Any feedback is appreciated!
post145-comment3-reply2: These results are amazing! Unfortunately I can't get anywhere near here :(. Is your validation set accuracy oscillating b/w very high and very low? How many layers do you use in your MLP and do you use BatchNormalization / Dropout after every layer? Thanks so much! Any tips for improving accuracy would be appreciated :D! 
post145-comment3-reply3: Beaker has very good accuracy. Can you share what's your metrics used?
post145-comment3-reply4: Hi, I've used the same number of layers as mentioned in the description (3 CNN + 1 Dense in hidden). BatchNorm after every layer but dropout just at the ending. Also, you can consider choosing the learning rate to be a lower value, as I observed that it played a crucial role while training the model in my case. 
post145-comment4: Getting similar results here.
post145-comment5: How did you get the full classification report?
post145-comment5-reply1: need to flatten it first then write as follow or similar form. ``` # Extracting True Labels from the Test Dataset true_labels = [] for images, labels in tf_dataset_test.unbatch(): true_labels.append(labels.numpy()) true_labels = np.array(true_labels) predicted_labels_flat = predicted_labels.flatten() true_labels_flat = true_labels.flatten() report = classification_report(true_labels_flat, predicted_labels_flat) print(report) ```
post145-comment6: Does someone have the same result for all models?0.34 1 0.51
post145-comment6-reply1: Mostly they came with slightly different figures
post145-comment7-reply1: Would you like to help building a Kaggle competition on this for the fun of it?
post145-comment8: I came to better performance after rerunning it for CNN + MLP. 401/401 [==============================] - 125s 309ms/step Precision: 0.5004768850987433 Recall: 0.5003044438023132 F1 Score: 0.3812767891565958 Which is interestingly better than some pre-trained models.
post145-comment9: Hi everyone, I got a better performance this time. However, to run on local GPU, I had to reduce the batch_size to accommodate the data in my GPU memory. 
post145-comment9-reply1: What does your architecture look like? Do you just have the 3CNN and 1 Dense layer? Any tips for these accuracies? Mine are hovering around 55% 
post145-comment9-reply2: Yes that's right, 3 CNN and 1 Dense in the hidden layer, and Dense(2, softmax) in output layer. Also using pooling layers after each CNN layer. Kept my learning rate very low around 0.00001. Also, increased my l2 param to 0.1.
post145-comment9-reply3: Thanks for sharing, How do you adjust the learning rate?
post145-comment9-reply4: You can set the learning rate when you're defining the Adam() optimizer. The name of that parameter is learning_rate. Then you can specify this optimizer during the compilation step. Hope this helps!
post145-comment9-reply5: It improved model vastly, 93%!! 
post145-comment9-reply6: Good to hear that mate, happy to help! :)
post145-comment9-reply7: @Seungil, congrats! What changes did you make? I have my learning rate at 0.00001 but still hovering around 60s :(. Do you have l2 regularization after every layer? 
post145-comment9-reply8: @Seungil, what was your min validation loss after training?
post145-comment9-reply9: It would be great if you could share your loss curves here if possible. Thanks!
post145-comment10: So far I tried some different way and here are what I have. For CNN MLP, used dropout only to the last layer. They are all test result. <CNN+MLP> <Eff...> <Res...> <VGG> 
post145-comment10-reply1: I think your model might be getting underfit because of l2 at every layer. But do correct me if I'm wrong!
post145-comment10-reply2: Are you recommand use the l2 only the last layer?
post145-comment10-reply3: How long did your pretrained models take per epoch? My efficient net is ETA ~40 min/epoch, while my CNN + MLP was 5 min per epoch. Any tips for reducing time/epoch for efficient net? I tried to increase the batch size to 64 (originally 8) but didnt do much. Thanks !
post145-comment10-reply4: Efficient net is huge, so the training time is mostly because of inference (forward propagation), not update (backward propagation). That's why training CNN+MLP is easier, while it has more parameters to tune than just one FF layer after efficient net. -------------------------------------------------------------- Mohammad Reza Rajati, PhD Senior Lecturer, Thomas Lord Department of Computer Science University of Southern California 
post145-comment10-reply5: Reducing training steps might fitting model faster. I used /5
post146: I'm having a hard time understanding the tensor objects and how to use them. I trained my model for c2 on tf_dataset_train which worked, but now i am unsure of what i am supposed to use for model.predict(). let me know if someone can help!
post146-comment1-reply5: <p>Since you have the batch_dataset for testing (probably named <strong>tf_dataset_test</strong> in your code), you can loop through the batches of X and y in this dataset, and predict on each X and store the y_true as well as results of y_pred (you can extend these results in an empty list). After the loop is finished, you will have obtained the y_true and y_pred for all the batches in <strong>tf_dataset_test</strong>. </p> <p></p> <p>Hope this helps!</p>
post147: Does anyone got the error 'Incompatible shapes: [32] vs. [32,2]' when fitting the model? Thank you!
post147-comment1-reply5: <md>Similar to @1027, but you need to specify the shape of y as 2 and modify appropriately if you are using softmax.</md>
post148: Every time I run VGG16, train loss increases as the epoch increases. Training accuracy fluctuates. Is this fine? If not, do you think I should handle the model? (This is epoch from 1 to 3. I'm training now, but I want to check if it's ok.) 
post148-comment1: Your accuracy is pretty good btw.
post148-comment1-reply1: I think so too. haha. But I'm worried about the increasing val_loss and loss. It's getting increasing every time epoch increases. But the training accuracy is about 86~88
post148-comment1-reply2:  it's my VGG
post148-comment1-reply3: Should I redesign a model if I get high loss and val_loss? 
post148-comment1-reply4: What's your val_accuracies?
post148-comment1-reply5: it's on the left side below the epoch :)
post148-comment1-reply6: Oh. they are pretty good. I think mine seem wrong. for the loss I have no idea. But test accuracy would tell you your model is fine enough.
post148-comment1-reply7: Do you mind if I ask about your test accuracy for all transfer learning models? just to compare
post148-comment1-reply8: @1107
post149: When I try to use ImageDataGenerator on the model it will report this typeerror: TypeError: float() argument must be a string or a real number, not '_BatchDataset'. Could someone help me to solve this problem? Or could you give me some tips on how to use tf_dataset_train on the ImageDataGenerator? Thanks! 
post150: is anyone got the same problem that in my transfer learning models, got this warning: /usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result))
post150-comment1-reply8: <md>It seems like your model is always predicting true (1) so you are having a lot of false positives and the scores are 0 for the false class. You'll need to reevaluate your model to make it less underfit.</md>
post150-comment2: How did you get this classification report? I do not see the documentation for this. Thanks!
post150-comment2-reply1: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html
post151: Hello,After training the CNN model, we have to report the metrics. I am not understanding the role of validation set. Can someone please explain on how to go about after training CNN? 
post151-comment1-reply1: <p>Validation set is used to tune your hyperparams like penalty, batch size, learning rate or even structure of nn. Metrics from validation set can be tracked to check if the model is overfitting or underfitting.</p> <p></p> <p>Test set is the final evaluation of your model and should not be leaked to the model during training or hyperparams tuning.</p>
post151-comment2: In addition to the above mentioned answer, you can also include your validation dataset in the validation_data parameter during model fitting. This will give you the validation loss and accuracy along with the training loss and accuracy. 
post152: Could an instructor please confirm if it is acceptable to access the data by uploading a zip file to google drive and then using: zipfile.ZipFile(file_path, 'r') as zip_ref: zip_ref.extractall(dest_path) I know this would work but would it make it so that others cannot run the code unless they have the data zip in their google drive? Should we be using another approach instead? I saw someone say to upload the data on the github repo and then use wget...Which way is recommended? 
post152-comment1-reply1: We don&#39;t have to upload dataset to github according to an instructor.
post153: Hello, Unfortunately, my kernel dies during model training and I'm new to Colab. Is there any advice related to working with Colab? Thanks
post153-comment1-reply1: <p>Colab is also slow for accessing the actual dataset.</p> <p>I think best way is use your GPU</p> <p>Also try disconnect and connect(or reset) for each problems.</p> <p>I was able to finish by my local laptop this way.</p>
post153-comment2-reply1: Collab pro also much faster (but paid)
post154: Can we use the data.zip file that is present in the dropbox instead of downloading from https://dataverse.jpl.nasa.gov/dataset.xhtml?persistentId=doi:10.48577/jpl.QJ9PYA ?
post154-comment1-reply1: The data.zip file is specially for students who are having trouble accessing it due to region lock (@1017_f5), download from the website if you can
post154-comment2: I downloaded it yesterday and again uploaded it. I don't think if it changes slightly, it's a big problem or changes the results.
post155: The assignment says to just augment the data--how much of the data are we augmenting? Just a small percentage, or all of it?
post155-comment1-reply1: It&#39;s up to you how much you want to do. Data augmentation is just a strategy to prevent over fitting. But if you do too much there&#39;s a chance of the augmented data looking too different and the model will underfit.
post155-comment2: do we have to do it at all?
post155-comment2-reply1: It does not say it's required so I don't think so. But it's much easier to overfit without data augmentation.
post155-comment3: Do we add the augmented images to the training dataset or do we just augment the original images? so the number of actual images stays the same or should be increased?
post155-comment3-reply1: The point of data augmentation is to generate additional data when the original data set is small to avoid overfitting. So the number of training images should increase after augmentation. If you keep the number of images the same, the model will just overfit to that set of images.
post155-comment3-reply2: I would highly recommend adding image augmentation layers directly in your model. It's really easy to implement and the model will see newly augmented images on every epoch.
post156: I've seen people running the project on Conda/CoLab, is there a reason for this, or can I run the project locally on my mac?
post156-comment1-reply2: On these platforms you can finish epochs faster. You should turn to them only when it takes too long to run locally.
post156-comment2: Yeah it depends on your computer. If you have enough processing power locally then there's no need for collab. You could also use collab to run in parallel though if you are trying to try different combinations for the best model.
post157:  A media company has volunteer positions for interns in a project about enhancement of content using artificial intelligence and machine learning. The candidate must be a senior college student or graduate student in Computer Science, Electrical Engineering, Statistics, or related fields. The intern will help with collecting and storing data from the web and benchmark databases, building data pipelines, management of cloud, checking the validity of software and models, and literature review in AI. The intern will apply algorithms developed by the team on various datasets and problems. The intern must have verifiable knowledge of python, data management systems, machine learning, and software systems through academic course projects or previous internships and hands-on work. Knowledge of working with AWS, deep learning, PyTorch, and Tensorflow is highly desired. This is an unpaid internship and for school credits only. It does not support post-completion OPT candidates. If you are interested, please contact me with your CV via a private Piazza post and also send it via email to myavarimanesh@presaige.com 
post158: Hi, My efficient net model was set to 10 epochs and ran all 10 without early stopping - is that ok or should I run it again with more epochs (eg 20) and see if it converges early? 
post158-comment1-reply2: The instructions say to run at least 10 but it should stop with early stopping, so you probably want to let it run more epochs if it is not early stopping from the validation set.
post159: No matter how I tune my hyper parameters, the pre-trained models always overfit, is anyone else in the same boat? Is this the expected behaviour for this dataset?
post159-comment1: I am experiencing the same thing. I have run probably close to 40 permutations of models, can't seem to find the right hyperparameters or layer combinations that give consistent results without underfitting/overfitting. Does anyone have any tips?
post159-comment1-reply1: I seem to be stuck in a loop where it's overfit, so I decrease the network size to around 1000 tunable parameters, but then it underfits. So I increase network size and it over fits again, no matter the batch normalization/l2 regularization/dropout permutations I make within the layers. Is anyone able to give some tips or give a rough idea of their network size?
post159-comment1-reply2: I'm having the sane problem. I wonder if its because I augmented all of the images rather than just a fraction.
post159-comment1-reply3: I augment all of my images, my augmentation is part of the model since I'm using the keras augmentation layers. I solved my overfitting problem by decreasing the pool size of my max pooling layer and not having dropout and batch normalization in every layer. Also sometimes the model fluctuates quite a bit but stabilizes in accuracy after 15-20 epochs. Hopefully that helps!
post159-comment1-reply4: Thanks for the response. Are you able to make the convolutional model better than transfer learning?
post159-comment1-reply5: I do not have any results yet for transfer learning but I'm getting ~90% test accuracy for the convolution model.
post159-comment1-reply6: Just to circle back, my transfer learning models ended up performing better than my convolutional model. Each of the transfer learning models have around ~95% test accuracy, and the learning curves for the transfer learning models appear much better.
post159-comment1-reply7: How did you end up getting 95% accuracy on test for transfer learning? So far my efficientnetb0 is stuck at around 60% accuracy on test.
post159-comment1-reply8: I tried to keep the mlp layer decently small and was pretty aggressive against overfitting with data augmentation and normalization since the transfer models are so complex. That seems to have worked.
post159-comment1-reply9: When you mean being aggressive with data augmentation, did you augment the entire dataset? Or only a small percentage of it?
post159-comment1-reply10: I included data augmentation as layers in my model. So yeah entire training set is augmented and there are new augmentations on every training epoch so that it doesn't overfit to the same images. I found it was much easier that way but if you do augmentation outside the model then you need to make sure you are increasing the size of your training data to prevent overfitting.
post159-comment1-reply11: When you say aggressive with data augmentation, I assume you're using tf.keras.layers.*augmentation*, are you setting really high random values or are you using a lot of different augmentations? Thank you!
post159-comment1-reply12: I'm not using every type of augmentation layer but I'm using quite a few. Just think of what our dataset looks like and what types of augmentation make sense for the data. The goal is to change it enough so it doesn't overfit but not so much that it no longer looks like the validation/test data.
post159-comment2-reply12: <p>There is definitely a possibility of overfitting on this dataset, as the tiles are small and grey scale and frost is &#34;vague.&#34;<br /><br />We know that this is not an easy dataset for transfer learning. You do your best and try to learn what the idea is.</p> <p></p> <p>If you have learned one thing from me, it must be that there is no free lunch and this whole thing may not work.<br /><br />JPL researchers retrained the whole Inception model to get acceptable results.</p>
post160: Hi! I am currently saving the model with the lowest validation error (highest val_accuracy). However, when generating a train classification report, I notice that the train accuracy is lower than the test accuracy by more than 20%. Is this normal?
post160-comment1-reply12: It can happen depending on the test set, sometimes it just generalizes well randomly. But that&#39;s probably a symptom of underfitting and your results might be very different from run to run depending on the initialized parameter values or on the number of epochs you run.
post161: Hi,During the iteration of my 20 epochs, I am seeing an average of 59-64%. I wanted to know if this is acceptable or if we should play with our hyperparameters.
post161-comment1-reply12: I am having the same for the transfer learning models, but for cnn it was quite high. It might be ok? I tried adding a learning rate schedule and tweaked a few diff parameters in my model but hasn&#39;t seemed to help much.
post161-comment2: I'm not a prof or TA but I'm pretty sure a 3 layer neural net classifying thousands of martian images at 65 percent is pretty darn awesome! As for getting the same for transfer learning, I'm expecting that. I've seen transfer learning boost neural nets to near perfection on classifying images of say a dog, gorrila, street sign, etc... but I don't think most of these pretrained models see images of earth let alone another planet
post162: Is it okay to augment validation data as well? I have the augmentation steps in my keras model and use model.fit() with the validation_data argument. The documentation states that the validation data will be augmented as well, and I was wondering if that was fine or not! 
post162-comment1-reply12: Augmentation should only be done on training data to prevent overfitting. I do not see anywhere in the document where it says to augment validation data.
post162-comment2: How do you separate augmentation from training and validation sets? Right now I have the augmenting layers in my model, which causes the validation data to also be augmented. I tried augmenting the training data with ImageDataGenerator but we can't use that after batching...
post162-comment2-reply1: Hi, I think once you have train, validation, and test data sets (before batching) you can add a condition if isTraining = True --> then augment the images and no otherwise.
post162-comment2-reply2: If you have augmentation layers in the model, they should only apply to training and will automatically not apply during inference. ``` During inference time, the output will be identical to input.
post162-comment2-reply3: I think this only applies to testing data and not validation data right?To Anon Mouse, if you do that, you only augment the image once, and not every single epoch?
post162-comment2-reply4: The validation set is inference time since it is evaluating the predictions. The validation set isn't used for training.
post163: Hi, my jupyter notebook always showed die kernel when I ran the datapreprocessing notebook. I tried to establish a virtual environment but it didn't work. What other solutions can I use? Should I use google Colab?
post163-comment1-reply4: I was facing the same issue and Colab did solve this but it was very slow. I had earlier installed TensorFlow using conda and later I created another environment and installed Tensorflow using pip which actually solved this issue for me in my Jupyter Notebook.
post163-comment2-reply4: Kernal death is usually computer spec issue, yeah use collab
post164: What are your Validation and Test accuracies? If they are above 80% any recommendations for some of the hyperparams? Also are the validation accuracies fluctuating? It makes sense to me that they fluctuate because we augment the images, but just wanted to double check! 
post164-comment1-reply4: <p>If you have the time to let things run: </p> <p></p> <p>I would try to reduce image augmentation if you have a lot already. Increase batch size (&gt;64) and buffer size (&gt;128). Include learning rates and regularizations of 0.000somethings. apply batch normalization in each layer (if you do make sure it&#39;s before the activation function and be ok with some overfit)</p> <p></p> <p></p>
post164-comment1-reply4: <p>If you have the time to let things run: </p> <p></p> <p>I would try to reduce image augmentation if you have a lot already. Increase batch size (&gt;64) and buffer size (&gt;128). Include learning rates and regularizations of 0.000somethings. apply batch normalization in each layer (if you do make sure it&#39;s before the activation function) but it over fits</p> <p></p> <p></p>
post164-comment1-reply4: <p>If you have the time to let things run: </p> <p></p> <p>I would try to reduce image augmentation if you have a lot already. Increase batch size (&gt;64) and buffer size (&gt;128). Include learning rates and regularizations of 0.000somethings. Don&#39;t apply batch normalization in each layer (if you do make sure it&#39;s before the activation function) because it over fits</p> <p></p> <p></p>
post164-comment1-reply4: <p>If you have the time to let things run: </p> <p></p> <p>I would try to reduce image augmentation if you have a lot already. Increase batch size (&gt;64) and buffer size (&gt;128). Include learning rates and regularizations of 0.000somethings. Don&#39;t apply batch normalization in each layer before the activation function because it over fits</p> <p></p> <p></p>
post164-comment1-reply4: <p>If you have the time to let things run: </p> <p></p> <p>I would try to reduce image augmentation if you have a lot already. Increase batch size (&gt;64) and buffer size (&gt;128). Include learning rates and regularizations of 0.000somethings. Apply batch normalization in each layer before the activation function. </p>
post165: Do we need to make a augmented train data for one time? I want to know that both cases are fine which are augment every time before gave it to the model, and use one augmented train data to all models 
post165-comment1-reply4: It depends on how you are applying data augmentation since it is simply a technique to prevent over fitting. If you only augment the data once but do not increase the training size enough, it will just overfit on the augmented data. You might want to try including data augmentation in the training steps so that data is augmented on each epoch, which always gives different inputs to the model and hopefully prevents overfitting.
post165-comment2-reply4: If I understood you correctly, yes you should try to use the same data across the models, just for consistency
post166: Hi! Does anyone have ideas about the following issue? My validation accuracy for EfficientNetB0/ResNet50/VGG16 was stuck at some fixed number such as 0.3218 / 0.6782. Is this normal? Thanks a lot for any hints or suggestions!!! 
post166-comment1: I have the same problem!
post166-comment2-reply4: Same problem here. 
post166-comment3: I had the same problem for efficient net, I switched my dense layer neurons to 2, make sure to use softmax, and used sparse categorical cross entropy loss 
post166-comment3-reply1: If using (2, softmax), did you one hot encode the labels in tf_dataset_train?
post166-comment3-reply2: I didn't, not sure if you need to do that
post166-comment3-reply3: I am getting "ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1))" without encoding.
post166-comment3-reply4: It worked after encoding but getting F1 score 0.41
post166-comment3-reply5: hmm yeah our code might be different, im getting an f score of .72
post166-comment3-reply6: Did you resize the image? I am using (75,75)
post166-comment3-reply7: I am also facing this problem where my validation accuracy is the same. Does anyone have a solution to this? I am already using 2, softmax in my dense layer as mentioned above, and categorical cross entropy. Thanks ! 
post166-comment3-reply8: Try sparse_categorical_crossentropy
post166-comment3-reply9: yes, tried that - still facing same issue. 
post166-comment3-reply10: Did anyone figure out how to resolve this?
post166-comment3-reply11: Anyone get it?
post166-comment4: Facing same issue! Has anyone figured out the solution?
post166-comment4-reply1: Same! any help would be appreciated 
post167:  Hello, I want to ask should we perform the augmentation on tf_dataset_train data? Thanks
post167-comment1-reply1: Yes
post168: Hi everyone, I am rescheduling my office hours to next week due to some prior commitments. I apologize for any inconvenience this may cause.
post169: What runtimes are you getting? I'm averaging around 4 min/epoch on M1 and am wondering if collab is much faster! 
post169-comment1-reply1: I got that much on M1 as well! Not sure if colab would be faster but it wasn&#39;t too bad.
post169-comment2: Thank you! Do you mind sharing your accuracies if you're done?
post169-comment2-reply1: For c: Test Metrics: Loss: 0.5358 Accuracy: 0.9403 Precision: 0.9584 Recall: 0.9503 F1 Score: 0.9543
post169-comment2-reply2: that is really high f1 score, mine is 0.4. Do you think I did something wrong?
post169-comment2-reply3: My train score is having a hard time getting to 90, let alone my test score. Would you mind sharing any tips @Ankita? When you do model.summary(), how many tunable parameters do you have?
post169-comment3-reply3: 4min is a good speed
post170: Does anyone have any ideas on this? I would appreciate it.
post170-comment1-reply3: how are you calculating this?
post170-comment2: .
post170-comment2-reply1: try model.evaluate instead! worked for me
post170-comment2-reply2:  The problem is from y_pred, maybe it is because the last layer of my cnn is layers.Dense(1, activation='sigmoid')?
post170-comment2-reply3: yeah i did 2, and softmax
post170-comment2-reply4: I got the same result (also using `Dense(1, activation='sigmoid')`)
post171: Hello,I used the data preparation code given in the repo. The tf_dataset_train shape is <unknown>: <_BatchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int64, name=None))> and because of which I am coming across following error which performing data augmentation: Cannot iterate over a shape with unknown rank. Can someone help me out here?
post171-comment1: I'm facing the same problem :( 
post171-comment1-reply1: @1027 helped me! 
post171-comment2-reply1: The sample code does not define the shape and the model is not able to infer it. You need to specify the shape of the input data manually in the first layer or define the shape in the data.
post171-comment2-reply1: The sample code does not define the shape and the model is not able to infer it. You need to specify the shape of the input data manually in the first layer.
post172: My model for Q1 gives only a 54% accuracy for test dataset. Is there a expected range for the accuracy? I have performed random augmentation to random images, should I apply augmentation to all images?
post172-comment1-reply1: For c, I got around 94% accuracy for the test data set, you may want to recheck your model!
post172-comment2: I'm sitting between 75 - 85 percent on the first 10 epochs rn
post172-comment2-reply1: that's a good place to be ^ 
post172-comment3-reply1: No &#34;Expected&#34; as of right now, but it should perform well
post172-comment3-reply1: No &#34;Expected&#34; as of right now
post173: 
post173-comment1-reply1: ETA December 12th
post173-comment2: Hi, sorry just wanted to check up whether the HW8 marks have been released yet. Please do let us know. Thanks!
post173-comment2-reply1: I see mine now 
post173-comment2-reply2: yeah it should be
post173-comment2-reply3: Where exactly are you able to see it? I mean, on which platform?
post173-comment2-reply4: Sorry, I misunderstood the scenario for the final grade, my bad! You can ignore my previous question!
post174: Hi, I've done preprocessing as the template file, then did the augmentation. Then i was fitting the training dataset to the model and i got this error at the last element as in following screenshot InvalidArgumentError: Graph execution error: Detected at node map/while/TensorArrayV2Read/TensorListGetItem defined at (most recent call last): <stack traces unavailable> Detected at node map/while/TensorArrayV2Read/TensorListGetItem defined at (most recent call last): <stack traces unavailable> 2 root error(s) found. (0) INVALID_ARGUMENT: Trying to access element 15 in a list with 15 elements. [[{{node map/while/TensorArrayV2Read/TensorListGetItem}}]] [[IteratorGetNext]] [[IteratorGetNext/_9]] (1) INVALID_ARGUMENT: Trying to access element 15 in a list with 15 elements. [[{{node map/while/TensorArrayV2Read/TensorListGetItem}}]] [[IteratorGetNext]] For investigating this error, I have tried to access the last element in the train dataset and also got the same error InvalidArgumentError: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Trying to access element 15 in a list with 15 elements. I've also tried using "drop_remainder=True" during the batching, but it didn't resolve the issue
post174-comment1-reply4: <p>just resolved this.</p> <p>The cause was that i set the shape of the data incorrectly</p> <p></p> <p>The first argument shouldn&#39;t be 32, as the last batch would have only 15 elements</p> <pre> images.set_shape([images.shape[0], IMG_HEIGHT, IMG_WIDTH, COLOR_CHANNEL]) labels.set_shape([images.shape[0]])</pre>
post174-comment2: What do you mean that the first augment shouldnt be 32? I've been stuck on the following error but haven't been able to resolve it... InvalidArgumentError: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 0. First element had shape [32,299,299,3] and element 15 had shape [15,299,299,3]. [Op:IteratorGetNext] name: 
post174-comment2-reply1: actually the only last batch should have 15 elements not 32 so we need to be careful when setting shape
post175: Do I need to use all of the data augmentation techniques such as crop, randomly zoom, rotate, flip, contrast, and translate images? Can I use some of them, not all? If so, is this a reason for the degradation of the project?
post175-comment1-reply1: please refer posts @1043 and @1055 :)
post176: Tried to run at local for Transfer learning problems. Should I run in Colab?
post176-comment1-reply1: Yeah colab will help w other issues too like conda 
post176-comment2-reply1: ram issue, download more!!! (jk dont)
post177: This might be a stupid question, but are we using the existing JSON labels for the images at all, or just the images themselves? 
post177-comment1-reply1: <p>From what i understood from the example notebook, we can just use the folder names of those images as a label, no need to go through those json files</p> <p>frost = 1</p> <p>others = 0</p>
post177-comment1-reply1: <p>From what i understood from the example notebook, we can just use the folder names of those images as a label.&#39;, no need to go through those json files</p> <p>frost = 1</p> <p>others = 0</p>
post178:  I unzipped the data and passed the path of the data folder
post178-comment1-reply1: I ran into that error too. Check your file structure. I had a folder called data and then another folder called data inside it. Once I fixed that mistake the code ran without error.
post179: Hi, Was anyone able to install tensorflow and run it on GPU (on their machine)? If yes, please let me know what steps you followed. So far, I have installed the CUDA toolkit and cuDNN but the installed tensorflow is not able to identify the GPU on my device. Any help is highly appreciated! Thanks in advance.
post179-comment1-reply1: Maybe you can refer: <a href="https://www.tensorflow.org/guide/gpu">Use a GPU | TensorFlow Core</a>
post179-comment2: Actually, the thing is, the TensorFlow that I installed is not able to identify the GPU in my device. Do you know anything about how this can be addressed? 
post179-comment2-reply1: Can you provide more information about your device? Mac？
post179-comment2-reply2: It is Windows 11 
post179-comment2-reply3: I think cuda cannot adapt to some drivers. If your driver is nvidia you can use cudnn. If you are using amd or other driver the solution will be much more difficult. 
post179-comment3: Same issue here, I spent the entire day trying to make my TensorFlow identify the GPU. Still not working yet.
post179-comment3-reply1: Why dont you want to use Colab instead ? Machine dependency is eliminated right ?
post179-comment4: my intel macpro CPU cannot be used for acceleration, 0 cpu is recognized. 
post179-comment5-reply1: GPU is always a pain to install properly, best advice is just to look through articles online, everyone has a different environment
post179-comment6: Try using python=3.9, cudatoolkit=11.2, cudnn=8.1.0, tensorflow<2.11. This combination works 
post180: Can I use loss='sparse_categorical_crossentropy'?
post180-comment1-reply1: I&#39;m also not sure which cross-entropy function to use. Can I use &#39;binary_crossentropy&#39; since the data has 2 classes? 
post180-comment2-reply1: Test each of them out and see which performs better
post181: I'm using ImageDataGenerator for my data augmentation. My code looks like this. datagen = ImageDataGenerator( rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, zoom_range=0.2, shear_range=0.2, horizontal_flip=True, vertical_flip=True, #contrast_factor=0.2 ) #CNN Model datagen_variable = datagen.flow(tf_dataset_train, batch_size = 32) history = model.fit(datagen_variable, epochs=20, validation_data=tf_dataset_val, callbacks=[early_stopping]) #Plotting and Testing I get this error on line datagen_variable = datagen.flow(tf_dataset_train, batch_size = 32). TypeError: float() argument must be a string or a real number, not '_BatchDataset' Does anyone have a solution for this?
post181-comment1-reply1: You can&#39;t apply data generator over batched data you will have to apply before batching it like during load and process
post181-comment2: Using ImageDataGen works fine when I use it at the data processing step, but causes the kernel to die when training my model. 
post181-comment3: Did you figure out how to use ImageDataGenerator or are you using something else? 
post181-comment3-reply1: Load and preprocess takes in one image at a time so I'm not sure how to do it, also not sure how to apply it only on train_data and not validation_data 
post181-comment4: Im confused, why don't you just use the augmented images from Q1C "To perform empirical regularization, crop, randomly zoom, rotate, flip, contrast, and translate images in your training set for image augmentation. You can use various tools to do this, including OpenCV."? Or are you not using opencv and using imagedatagenerator instead?
post182: Can someone suggest the best way to access the data from google colab? I'm having trouble using !wget and its taking a really long time to ulpoad the extracted dataset to google drive. Please let me know, thanks :)
post182-comment1-reply1: Hi, did you get any solution to this?
post182-comment2: Yes!, if you upload a zip file to google drive and then use zipfile.ZipFile(file_path, 'r') as zip_ref: zip_ref.extractall(dest_path) (somethind along those lines^) after mounting google drive it works. took me 1.5 hours to extract in google drive
post182-comment2-reply1: Okay, thanks a lot 
post182-comment3: I tried to use zipfile, but unfortunately the data didn't unzipped successfully. Its created two folder one as data and other as _MacOS and both of them have data. Could you please share your way to unzip the data? what changed did you make in the preprocessing code in particular to read unzipped file. Thanks.
post182-comment4: i cant read in any of the train_source_images.txt after uploading them to drive?
post183: 
post183-comment1-reply1: ETA December 10th
post184: Hi, I'm confused about the requirements highlighted below. Which layer(s) do we need to apply batch normalization, dropout and L2 regularization? Only on the dense layer MLP?The layer that needs softmax is the output dense layer after the MLP, right? Thx. 
post184-comment1-reply1: I think you should include the Batch Normalization
post184-comment2: I'm finding that having batch normalization in each layer is good but no free lunch a little overfitting but hey. If you do include it make sure it's before your activation function. I found some things that say some regularization in each layer is smart and it's helpful to decrease the regularization parameter as you get into deeper layers
post184-comment2-reply1: Thank you! Do you mean we should do batch normalization before relu?
post184-comment2-reply2: yes as opposed to after 
post185: Hi, Not sure if I am missing something as a non-CS/ DS major, but is the notebook provided supposed to be able to read in all the data for us so that we can use datasets stored in the variables- tf_dataset_train, tf_dataset_val, tf_dataset_test? I edited the data_head_dir to what I believe is the right location for where I am keeping all the files, but print(tf_dataset_test) gives me - <_BatchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int64, name=None))> I think I'm having trouble understanding what is being asked and so any help on what I may have missed would be greatly appreciated. Thank you
post185-comment1-reply2: In order to print an element in the BatchDataset, try extracting one element using next(iter(tf_dataset_train)), this will give you a tuple (most likely) with the first tuple element as the image path tensor and second as the label tensor
post186: Has anyone successfully installed tensorflow-metal for Mac? Why do I keep getting Error: Could not find a version that satisfies the requirement tensorflow-metal (from versions: none) ERROR: No matching distribution found for tensorflow-metal I have installed tensorflow and even started running the code but keep having trouble installing tensorflow-metal and therefore can't accelerate the running.
post186-comment1-reply2: <p>Try to use command &#34;SYSTEM_VERSION_COMPAT=0 pip install tensorflow-macos tensorflow-metal&#34;</p> <p>Seems to work for me</p>
post186-comment2: I successfully use the code to install ......but seems it not support for Intel GPUs.... Currently not supported Multi-GPU supportAcceleration for Intel GPUsV1 TensorFlow networks 
post186-comment3: my Mac is from 2016, tensorflow metal has not reduced any times for me and introduced many more problems than solutions
post186-comment4: This is how I install everything for tensorflow on MAC:https://github.com/deganza/Install-TensorFlow-on-Mac-M1-GPU/blob/main/Install-TensorFlow-on-Mac-M1-GPU.ipynb
post187: Hi! My training accuracy improved in each Epoch, but my validation accuracy was jumping and decreasing. Is this normal? Can anyone give some hints or suggestions? Thank you!!! 
post187-comment1:  Same here!
post187-comment1-reply1: Same for me; validation accuracy fluctuates a lot. Did anyone try to figure this out?
post187-comment1-reply2: Me too 
post187-comment1-reply3: Did you figure out any solution to this?
post187-comment2: Does anyone have any idea why this happened? Thanks a lot!
post187-comment3: Same here !
post187-comment4: Running into the same issue.
post187-comment5-reply3: <p>To me it&#39;s likely that we could be adding too much noise by requiring data augmentation or that our learning rate parameter is too high. Smaller batch sizes introduce unnecessary noise too. Regularization parameters could be too large. If I had infinite time I would cut back the number of data augmentation parameters (This is my first guess in fluctuations) have a learning rate parameter of like 0.000something, a batch size of at least 16 probably more like 32 probably more like 64, and bring down regularization parameters to 0.000something. large fluctuations are a sign of overfitting so decreasing model complexity is the way to go</p> <p></p> <p>Edit by Trey:</p> <p>To me the regularization param should be increased to avoid overfitting. Overall modifications you may try are:</p> <p>1. Simpler augmentation or smaller fraction of data to augment.</p> <p>2. Bigger batch size.</p> <p>3. Lower learning rate.</p> <p>4. Greater penalty.</p> <p>5. Increase droprate (we&#39;re instructed to use 0.3 here so ignore it).</p> <p>6. Smaller patience value.</p> <p></p> <p>Edit Again by Trey:</p> <p>For reference: I used lr=0.0003 and weight_decay=0.001, 30% of my training set was augmented, and received smaller fluctuation.</p>
post187-comment5-reply3: <p>To me it&#39;s likely that we could be adding too much noise by requiring data augmentation or that our learning rate parameter is too high. Smaller batch sizes introduce unnecessary noise too. Regularization parameters could be too large. If I had infinite time I would cut back the number of data augmentation parameters (This is my first guess in fluctuations) have a learning rate parameter of like 0.000something, a batch size of at least 16 probably more like 32 probably more like 64, and bring down regularization parameters to 0.000something. large fluctuations are a sign of overfitting so decreasing model complexity is the way to go</p> <p></p> <p>Edit by Trey:</p> <p>To me the regularization param should be increased to avoid overfitting. Overall modifications you may try are:</p> <p>1. Simpler augmentation or smaller fraction of data to augment.</p> <p>2. Bigger batch size.</p> <p>3. Lower learning rate.</p> <p>4. Greater penalty.</p> <p>5. Increase droprate (we&#39;re instructed to use 0.3 here so ignore it).</p> <p>6. Smaller patience value.</p>
post187-comment5-reply3: <p>To me it&#39;s likely that we could be adding too much noise by requiring data augmentation or that our learning rate parameter is too high. Smaller batch sizes introduce unnecessary noise too. Regularization parameters could be too large. If I had infinite time I would cut back the number of data augmentation parameters (This is my first guess in fluctuations) have a learning rate parameter of like 0.000something, a batch size of at least 16 probably more like 32 probably more like 64, and bring down regularization parameters to 0.000something. large fluctuations are a sign of overfitting so decreasing model complexity is the way to go</p> <p></p> <p>Edit by Trey:</p> <p>To me the regularization param should be increased to avoid overfitting. Overall steps are:</p> <p>1. Simpler augmentation or smaller fraction of data to augment.</p> <p>2. Bigger batch size.</p> <p>3. Lower learning rate.</p> <p>4. Greater penalty.</p> <p>5. Use batch normalization layer.</p> <p>6. Increase droprate (we&#39;re instructed to use 0.3 here so ignore it).</p> <p>7. Smaller patience value.</p>
post187-comment5-reply3: <p>To me it&#39;s likely that we could be adding too much noise by requiring data augmentation or that our learning rate parameter is too high. Smaller batch sizes introduce unnecessary noise too. Regularization parameters could be too large. If I had infinite time I would cut back the number of data augmentation parameters (This is my first guess in fluctuations) have a learning rate parameter of like 0.000something, a batch size of at least 16 probably more like 32 probably more like 64, and bring down regularization parameters to 0.000something. large fluctuations are a sign of overfitting so decreasing model complexity is the way to go</p> <p></p> <p>Edit by Trey:</p> <p>To me the regularization param should be increased to avoid overfitting. Overall steps are:</p> <p>1. Simpler augmentation or smaller fraction of data to augment.</p> <p>2. Bigger batch size.</p> <p>3. Lower learning rate.</p> <p>4. Greater penalty.</p>
post187-comment5-reply3: To me it&#39;s likely that we could be adding too much noise by requiring data augmentation or that our learning rate parameter is too high. Smaller batch sizes introduce unnecessary noise too. Regularization parameters could be too large. If I had infinite time I would cut back the number of data augmentation parameters (This is my first guess in fluctuations) have a learning rate parameter of like 0.000something, a batch size of at least 16 probably more like 32 probably more like 64, and bring down regularization parameters to 0.000something. large fluctuations are a sign of overfitting so decreasing model complexity is the way to go
post187-comment5-reply3: To me it&#39;s likely that we could be adding too much noise by requiring data augmentation or that our learning rate parameter is too high. Smaller batch sizes introduce unnecessary noise too. Regularization parameters could be too large. If I had infinite time I would cut back the number of data augmentation parameters (This is my first guess in fluctuations) have a learning rate parameter of like 0.000something, a batch size of at least 16 probably more like 32 probably more like 64, and bring down regularization parameters to 0.000something. 
post187-comment6: .
post188: 
post188-comment1-reply3: yes. see the post @1043.
post189: Hi, I had a issue when I try to train the model in question 1 c ii. The error message indicating that the TensorSpec shape of the training dataset after image augmentation does not have the batch size (like "299,299,3") whereas my validation data is "None, 299,299,3). This issue is blocking me to train the model. Is anyone facing the same issue? How I can solve this issue? Thanks
post189-comment1-reply3: @1027 Check out this article.
post189-comment2-reply3: its a dimension issue, try looking at your data
post190: Should we use batchNormalization after every layer or just before the output layer(softmax) ? 
post190-comment1: Did you figure this out?
post190-comment2-reply3: If you&#39;re going to use it use it before your activation function. I think it overfits in each layer
post190-comment2-reply3: If you&#39;re going to use it use it before your activation function. I think it helps in each layer
post191: Do we need to use image augmentation steps to increase the actual number of images in the dataset, or do these steps modify existing images in the dataset in various ways?
post191-comment1-reply3: @1043
post192: For CNN model, I set kernel_regularizer=regularizers.l2(0.001) in the layers.Conv2D, then the loss is getting bigger and bigger for each epoch, and also same accuracy for every epoch, just like @1036, does anyone have any idea how to fix it, is it because of the regulaizers? and is this accuracy around 60% acceptable?Update:I tried to change the value for the l2.regularizer but still get the accuracy to be 0.5878 since the first epoch. I only managed to reduce the loss by changing my loss function to binary_crossentropy 
post192-comment1: solved by using sigmoid instead of softmax
post192-comment1-reply1: i am using sigmoid, and my other values look correct but i am getting the same val accuracy (.3218) for each epoch like you - did you fix that as well or is that the same for you still?
post192-comment1-reply2: i fixed that also
post192-comment1-reply3: how did you fix that?
post192-comment1-reply4: this is how mine looks right now 
post192-comment1-reply5: Maybe something wrong with your model, for example , if u set dropout for every layer then the issue will happen. 
post192-comment1-reply6: Did you figure out how to fix this? 
post192-comment2: I solved it by changing Dense to 2 and use loss='sparse_categorical_crossentropy'
post192-comment2-reply1: this worked for me too!
post192-comment3: I'm getting the exact same accuracy (training and validation) on every run, has anyone fixed this with high accuracy?
post192-comment3-reply1: I tried multiple combinations, finally learning rate:0.00001, binary_crossentropy, sigmoid solved it.
post193: How much time is taking to use the pre-trained models?
post193-comment1-reply1: EfficientNetB0 is taking 3.5 minutes / epoch for me
post194: Is it acceptable to report metrics solely based on the test datasets? Or should we report each metric for train, valid, and test datasets?
post194-comment1-reply1: Do it for all datasets
post194-comment2: I was wondering if we will be graded on our train and validation scores. It seems like reporting the scores for train and validation is not mentioned in the project instructions.
post195: The given strings in *_source_images.txt are just prefixes to actual folder names, do we select all the parent folders that have the prefix, or any one, this will increase the size of the data by a lot, for example: 1st prefix from train_source_images.txt (ESP_018002_1820) has following 6 matches ESP_018002_1820_0_5120_20480_25600 ESP_018002_1820_35840_40960_20480_25600 ESP_018002_1820_40960_46080_25600_29510 ESP_018002_1820_51200_56320_5120_10240 ESP_018002_1820_5120_10240_20480_25600 ESP_018002_1820_5120_10240_5120_10240
post195-comment1-reply1: <p>Hi,<br />Did you figure this out?</p> <p>I have same question too, should we group folders with same id together before training?</p>
post195-comment2: Yes, the preprocessing code provide suggest we need to select data from all folders with the said prefix 
post196: Does anyone know of an efficient way of uploading the data files to Colab? I tried using the wget method but that only downloaded the zip file, to access the actual data I need to download them from Drive onto my computer and then reupload them... has anyone figured out how to address this issue?
post196-comment1-reply1: You can use wget command on colab to download the zip file &amp; then use python&#39;s zipfile to unzip.<div><br /><div>Both these steps can be performed on colab itself &amp; you will have the data on drive.</div><div>No need to manually upload from local system to drive as it takes quite a lot of time.</div></div><div><br /></div><div>Hope this helps!</div>
post196-comment2: Thanks! Did you use the extractall() method to unzip?
post196-comment2-reply1: Yes!
post196-comment3: I tried to use zipfile, but unfortunately the data didn't unzipped successfully. Its created two folder one as data and other as _MacOS and both of them have data. Could you please share your way to unzip the data? what changed did you make in the preprocessing code in particular to read unzipped file. Thanks.
post196-comment3-reply1: did u find a solution to this?
post197: Hello, I am trying to run the written code for splitting data into train, test, and validation sets but I am getting an error at the line - import tensorflow as tf SymbolAlreadyExposedError: Symbol Zeros is already exposed as (). I tried uninstalling and reinstalling TF as well as updating Conda as advised from a post I saw online but it did not work. Please advise if anyone else has faced this error.
post197-comment1: I was having issues with tensorflow in VS Code, but it worked for me in Google Collab
post197-comment2-reply1: Upon googling the first result indicates that this has been an unclosed issue: <a href="https://github.com/tensorflow/tensorflow/issues/39606">https://github.com/tensorflow/tensorflow/issues/39606</a><div><br /></div><div>I would suggest using native python and pip to try again, or alternatively use google colab</div>
post198: Before training CNN + MLP，we need to do image augmentation for all the images in tf_dataset_train, do we need to apply all of the methods (empirical regularization, crop, randomly zoom, rotate, flip, contrast) to every single image, or we only need to randomly choose one of the method to some random images?
post198-comment1-reply1: You can do it however you would like, but remember this step essentially generates additional training data, so I would recommend doing different methods on each picture for more data
post198-comment2: When applying augmentation exclusively to the tf_dataset_train, a potential concern arises regarding the image shape management. Given that the original image size is 299x299x3, if cropping is employed during augmentation, the resultant image size may be smaller than 299x299. This discrepancy raises the question of whether it could lead to errors in the subsequent CNN MLP processing.how do we manage the image size if we perform the crop?
post198-comment2-reply1: you could scale up if you want to maintain a certain size
post198-comment2-reply2: I did what you suggested, but I am getting this error InvalidArgumentError: Graph execution error: Detected at node random_crop/Assert/Assert defined at (most recent call last): <stack traces unavailable> Detected at node random_crop/Assert/Assert defined at (most recent call last): <stack traces unavailable> 2 root error(s) found. (0) INVALID_ARGUMENT: assertion failed: [Need value.shape >= size, got ] [15 299 299 3] [32 239 239 3] [[{{node random_crop/Assert/Assert}}]] [[IteratorGetNext]] [[IteratorGetNext/_11]] (1) INVALID_ARGUMENT: assertion failed: [Need value.shape >= size, got ] [15 299 299 3] [32 239 239 3] [[{{node random_crop/Assert/Assert}}]] [[IteratorGetNext]] 0 successful operations. 0 derived errors ignored. [Op:__inference_train_function_11132] 
post198-comment2-reply3: How did you resolve this error? 
post198-comment2-reply4: i couldn't resolve it yet. I just removed crop since we don't have to have a crop function. But, I hope to have some advice from TA so I can resolve the error.
post198-comment2-reply5: how many of the training images do we have to augment? Do we create an augmented version of all of the training images, or just some? 
post198-comment3: Are we performing these transformations in addition to the non-edited images? As in, we essentially double the training dataset size by performing transformations on each image in addition to the originals?
post198-comment3-reply1: i only choose 10 percent, double size lead to 40 min each epoch.
post198-comment3-reply2: me too, my epochs are taking like 25 minutes each, and doubling can make them reach 1 hour
post198-comment4: I want to know where you put the augmentation code, should it be put in "load_and_preprocess" function and keep the same size of the training set? 
post198-comment4-reply1: whereever you see fit
post199: I am running into the problem where my kernel keeps dying when I train the model locally. Has anyone run into this issue? Does moving everything to Colab fix the issue?
post199-comment1-reply1: Colab pro would help!
post199-comment2-reply1: Might be a spec issue
post199-comment3: how did you fix this?
post200: Does anyone know how we can implement data augmentation in a simple way? I have a bit of a problem with cropping, zooming, and translating.
post200-comment1: Also, should we multiple augmentation layers on the same image? Or should we augment all training images separately? For example, should we create a few rotated and zoomed, transposed and contrasted, flipped and cropped images? Or should we add a rotated image, zoomed image, transposed image, contrasted image, flipped image, and cropped image, in addition to the original? Should our training set be seven times larger with data augmentation?
post200-comment1-reply1: I think you should perform different methods on the same image 
post200-comment2-reply1: There are some data augmentation methods in keras. Here are some sample notebooks: https://keras.io/examples/
post200-comment3-reply1: OpenCV should be able to do this easily, googling should give you some good results to get started: <div><br /></div><div><a href="https://www.kaggle.com/code/ahmedabdelfattah20/image-augmentation-using-opencv">https://www.kaggle.com/code/ahmedabdelfattah20/image-augmentation-using-opencv</a></div><div><br /></div><div><a href="https://towardsdatascience.com/complete-image-augmentation-in-opencv-31a6b02694f5">https://towardsdatascience.com/complete-image-augmentation-in-opencv-31a6b02694f5</a></div>
post200-comment4: Is it acceptable to use the keras layers for data augmentation? I am getting decent results with them.
post200-comment4-reply1: yeah thats fine
post201: for the cnn, it require to train for at least 20 epochs and perform early stopping using the validation set.. Does it means we need to set the patience parameter to be 20?
post201-comment1-reply1: I think we should set patience=20 in CNN, as the requirement in &#34;Final Project.pdf&#34; (Train for at least 20 epochs and perform early stopping using the validation set.)
post201-comment2: What I did is epochs=20 (to make sure 20 epochs), but set patience parameter to be 3, I am not sure if this is right. I t would be great if anyone can clearify
post201-comment2-reply1: I did that as well, mainly due to runtimes. I am wondering, what is the point of adding early stopping if we are supposed to go through 20 epochs??
post201-comment2-reply2: I think we just need to set epoch = 20, and let it run. If it early stops at some value smaller than 20, then it’s fine.
post201-comment3: can some instructor clarify this? I set patience parameter to 7 but the training stops after 8 epochs. 
post201-comment3-reply1: @1114
post202: For our final project, we are asked to use the softmax funciton. But it seems like we have a binary classification problem. Can we use sigmoid function instead of softmax? 
post202-comment1-reply1: Same doubt
post202-comment2: +1
post202-comment3: +1
post202-comment4-reply1: You are allowed to use sigmoid function instead of softmax.
post203: Question, Running local but submitting 2.6GB possible for Github? Would it be okay just upload except all the datafiles?
post203-comment1-reply1: no data needed
post203-comment2: So, with that been said, I just need to submit my python file to Github and Don't need to submit the data, right?
post204: Is this fine to get the same accuracy every epoch? I set the the number of epoch over 20, so it could be an early decision, but I am worried about the same accuracy per every epoch. Thank you in advance. 
post204-comment1-reply1: Seems like your accuracy is repeating the printout of the first one. Cuz if you check losses, they are all different. it should be converge to better accuracy.
post204-comment2-reply1: Remember early stopping as the instruction mentions
post204-comment3: hi, were you able to figure this out? I have a similar output but all my other values are diff and the val accuracy is the same each time and it is only happening for efficientnet 
post204-comment3-reply1: Hi! Did you figure this out?
post204-comment3-reply2: @ankita, I also have the same val accuracy in each epoch for efficient net and not sure how to fix it. were you able to figure it out? thanks ! 
post204-comment3-reply3: hey, no i haven't. i have since adjusted my model and theyre not all exactly the same but its still around .3 and not showing much variation.
post204-comment3-reply4: I have the same issue--for both the CNN+MLP and the 3 transfer models I get the same training accuracy for each epoch (~0.65068), but validation losses are different. When I evaluate the 4 models, their validation accuracies are all 0.2309544 and test accuracies are 0.72428. I haven't had much time to tune the parameters due to long processing times, but I assumed I was getting exactly the same results across all models because they were only assigning one class. Is anyone else experiencing the same thing with multiple models? 
post204-comment4: Yeah, having the same issue. If some one was able to rectify this then pls let us know. Thanks!
post204-comment5: I am having the same issue. I get the same accuracy score in each epoch, I have added early stopping
post205: The problem says to have a "three-layer CNN". Can we use a pooling layer in the CNN? Would the pooling layer count as one of the 3 layers or do we always need to have 3 convolution layers?
post205-comment1-reply4: I think a conv layer &#43; pooling layer counts as one layer in CNNs.
post206: Do we have office hours this week, and will the location alternate to last week's?
post206-comment1-reply4: Please refer to the OH post
post206-comment2: Just to follow up, the OH post does not have whether it is in person or online for this week. Can an instructor please clarify the schedule?
post207: in the question, it said to use a use the softmax function which more suitable for multi-class. but since our project is about binary classifiction can we use the sigmoid instead?
post207-comment1-reply4: Even binary, Softmax is working as well. So I&#39;m just stick the original problem states.
post208: I have noticed that the dataset given by Google drive and by the link from pdf is different (one is 1.6GB and one is 2.6GB), and the txt given in the repo is using the 2.6GB dataset. I wonder which dataset we should pick? Can we just using the 1.6GB dataset and split train, test, validation dataset by ourselves?
post208-comment1-reply4: Please use the dataset from the link if you’re able to download it.
post208-comment2: I am unable to download the dataset from the link. If I am to use the dataset from dropbox can you please help me with the splits?
post209: I'm playing around with the data and I'm trying to print it out using `print(tf_data_test.as_numpy_iterator())` but I keep getting this error `AttributeError: 'list' object has no attribute 'as_numpy_iterator'`. Online it says it's because < v2.1 but I'm using 2.15 on Mac via Anaconda. Am I doing this correctly / is this working for anyone else? Thank you! 
post209-comment1-reply4: NEVERMIND I was using tf_data_test instead of tf_dataset_test, oof! 
post210: Hi, what does 32 and 3 mean in the image dimensions? Thanks! 
post210-comment1-reply4: 32 is most likely batch size and 3 is number of channels in the input image/depth 
post210-comment1-reply4: 32 is batch size and 3 is number of channels in the input image/depth 
post211: Does someone solve this error for c-ii?
post211-comment1-reply4: <md>I was able to fix it by modifying preprocess lines. ``` def load_and_preprocess(img_loc, label): def _inner_function(img_loc, label): # Convert tensor to native type img_loc_str = img_loc.numpy().decode('utf-8') # Load image using PIL and convert to RGB img = Image.open(img_loc_str).convert('RGB') # Convert PIL image to numpy array img = np.array(img) img = tf.image.resize(img, [299, 299]) # Normalize the image to the [0, 1] range img = img / 255.0 # Convert label to integer (assuming binary classification) label = 1 if label.numpy().decode('utf-8') == 'frost' else 0 return img, label # Wrap the Python function X, y = tf.py_function(_inner_function, [img_loc, label], [tf.float32, tf.int64]) # Set the shape of the tensors X.set_shape([299, 299, 3]) y.set_shape([]) # Scalar label return X, y ```</md>
post211-comment1-reply4: I encounter the same problem, have not solve yet
post211-comment2: Can anyone help with this problem? Thx
post211-comment3: I'm facing same issue. If anybody resolves, please update
post211-comment4: I'm facing same issue. 
post211-comment5:  Same here!!
post211-comment6: try to set the dense to 1 since we need to classify using binary classification.
post211-comment6-reply1: Thankyou but still the aslist() error is not getting resolved
post211-comment6-reply2: Me too. It persist still
post211-comment6-reply3: same here. Issues not resolved
post211-comment6-reply4: To resolve the issue, I explicitly set the shape of the tensors. ``` python def _fixup_shape(images, labels): images.set_shape([None, 299, 299, 3]) labels.set_shape([None, 2]) return images, labels tf_dataset_train = tf_dataset_train.map(_fixup_shape) tf_dataset_val = tf_dataset_val.map(_fixup_shape) tf_dataset_test = tf_dataset_test.map(_fixup_shape) ``` Reference: https://github.com/tensorflow/tensorflow/issues/32912#issuecomment-550363802
post211-comment7: That works! Thank you so much!
post211-comment7-reply1: After doing the _fix_shape I am getting this error: InvalidArgumentError: Graph execution error: Incompatible shapes at component 1: expected [?,2] but got [32]. [[{{node IteratorGetNext}}]] [Op:__inference_train_function_12433]
post211-comment7-reply2: me too, did you solve it?
post211-comment7-reply3: Maybe you are using softmax while providing shape of y as (batch_size, 1), either use sigmoid activation or use one hot encoded Y i.e. make Y (batch_size, 2)
post212: For those of you who are trying to train the model on your M1 or M2 macbooks, here is a Tensorflow plugin that can drastically reduce training time. https://developer.apple.com/metal/tensorflow-plugin/ 
post212-comment1: Thanks for sharing! Do you know how to run the Jupyter notebook in the venv-metal environment? I tried to add the kernel in the venv-metal folder, but it doesn't seem to work.
post212-comment1-reply1: Not sure. All I needed to run was pip install tensorflow-metal and that was enough to improve the training speed.
post212-comment2: Thanks so much for sharing! Are you using your local device, Colab, or conda?
post213: In line 77, should it be (...tf_data_test) instead? 
post213-comment1-reply1: You are correct this was a typo, it should be test, good catch!
post214: I am trying to train my model but each EPOCH is taking around 15 mins due to the large number of pictures. Considering I have ~80 EPOCHS, it will take me 20 hours to train my models using my CPU. I have also tried using my GPU but there is a compatibility issue and I won't be able to use it. What would be my options to fix this? Thank you.
post214-comment1-reply1: Maybe try reducing the number of steps per epoch and see if that helps. 
post214-comment2-reply1: This sounds pretty high but still within the reasonable area depends on your pc specs, but even 20 hrs does not sound too bad to me since you can just let it run. Also remember to perform early stopping as the instruction says
post214-comment3:  Use a large batch size Use Google Colab M R Rajati’s phone 
post215: I wonder if the percentage of train, test and validation data set is required? Will it be ok to set them as 0.8, 0.1, 0.1?
post215-comment1-reply1: The txt files for the three should be provided already when you create the repo
post215-comment2: Just to confirm we should be choosing select subframes as our train, validation, and test set?
post215-comment2-reply1: The txt files provide the folders of the subframes for each category. Every image in that folder is used as a sample for that category. You should not be doing any choosing manually.
post215-comment2-reply2: all the files in my train.txt file look like this: ESP_018002_1820ESP_018263_2230ESP_018751_2230ESP_018828_2445ESP_018951_1205 but i don't see any folder structures matching that nomenclature. so are we wildcard matching to bring back all the folders that start with the 15 characters file names found in the .txt files? i'm assuming that's how we're handling this - but not sure.
post215-comment2-reply3: Yeah its giving the prefix and matches all folders.
post216: Hi, Has anyone uploaded the dataset to Google Drive? I am trying to use Colab, but the dataset couldn't upload because there were too many files. (it shows 12 hours left) 
post216-comment1: Mine is also taking that long to upload to Google Drive :/
post216-comment1-reply1: I found this helpful, still in the process of trying it tho: https://miteshparmar1.medium.com/using-wget-command-in-google-colab-to-retrieve-datasets-from-source-f4ffeb22cd10
post216-comment1-reply2: Can you provide me the Download URL, as I am unable to get the correct Download URL for the above method?
post216-comment1-reply3: +1 
post216-comment2-reply3: Try to upload it to a GitHub repo and then !wget method to download in Google Co lab.
post217: the question ask to train for at least 20 epochs and perform early stopping using the validation set. So does it means we need to set the epochs to 20 and it is possible to early stopping like 5 or 10 epchos. or we set the patience to be 20 and make sure we at least have 20 epchos.
post217-comment1-reply3: correct it could early stop and any time, just set the max to whatever the requirment is
post217-comment2: I am a bit confused by the instructor's response. I do not see a "max" specified, it only says to "train for at least 20 epochs and perform early stopping using the validation set". Are you saying that 20 is the max in this case?
post217-comment3: Hi, please clarify the above query
post217-comment4: Is patience=5 ok?
post218: Are you guys using Google Colab or local computer to do the project? 
post218-comment1-reply3: It depends on the performance of your Colab and your local computer. Choose whichever works best for you. Upgrading Colab could be a good option if you find it more powerful.
post218-comment2: Is it just me or when fitting the cnn model on jupyter, the kernel dies every time :( 
post218-comment2-reply1: It works fine for me, and my kernel doesn't die. If your kernel is dying, you can check the memory and CPU usage or try reducing the size of your dataset and see if it runs smoothly.
post218-comment2-reply2: the same here. I wonder how can we test in the local Jupyter notebook if kernel dies when running code.
post219: Hi, I'm new to TensorFlow and am trying to install it on my Mac. I found conflicting suggestions about whether to install it with Conda online. Does anyone have experience in this? Thank you!
post219-comment1-reply2: <p>You can install it within conda environment itself. I also am running anaconda on Mac</p> <p>I used the below cmd to install in the conda environment</p> <p><code>conda install -c conda-forge tensorflow</code></p>
post219-comment1-reply2: <p>You can install it within conda environment itself. </p> <p>I used the below cmd to install in the conda environment</p> <p><code>conda install -c conda-forge tensorflow</code></p>
post219-comment2: man i hate conda
post219-comment2-reply1: :) it worked for me though ! Shd i proceed or shd i unistall and do some other way of installation ?
post219-comment2-reply2: I was facing some issues with Mac ARM chip, I had to use : python -m pip install tensorflow-macos
post219-comment3: hi did you have the problem of dying kernels when running TF inside conda?
post219-comment4: if you are using a M1 mac here is the steps for installing tensorflow that works for me https://github.com/deganza/Install-TensorFlow-on-Mac-M1-GPU/blob/main/Install-TensorFlow-on-Mac-M1-GPU.ipynb
post220: Hi, Can we use PyTorch for the final project? Thanks
post220-comment1-reply2: Keras for Pytorch is recommended, but I think its similar to the homeworks where we can use anything.
post220-comment2: In the project description, they said Keras and Python are recommended, so yes
post221: Can we have the project tomorrow so that we can work on it during the weekends?
post221-comment1: +1
post221-comment2-reply2: Don&#39;t worry you will have plenty of time to work on it, its similar amount of work as a HW. In the meantime, chill relax and have a little fun :D
post222: When will be the final project posted?
post222-comment1-reply2: Soon^tm, maybe monday
post223: Hi, If you see a studentID card with name Danyang Zhang in THH201(probably in the center of the room), could you please contact me by dzhang69@usc.edu? Thanks very much for your help! 
post224: PXL_20231201_180552949.MP.jpg I will submit it to the SAL lost and found in half an hour. You can collect from there.
post224-comment1: Dropped it off at leavey's lost and found
post225: Just a good luck note. :)
post225-comment1: good luck
post226: Hello class, Please find the MidTerm 2 Zoom link for DEN. Piyush Deep is inviting you to a scheduled Zoom meeting. Topic: MidTerm 2 DEN Zoom Link DSCI 552Time: Dec 1, 2023 07:45 AM Pacific Time (US and Canada) Join Zoom Meetinghttps://usc.zoom.us/j/92312385750?pwd=RWRXSStyNnpRS1JjZzZWQTN3SGY4QT09 Meeting ID: 923 1238 5750Passcode: 391583 --- One tap mobile+12532158782,,92312385750#,,,,*391583# US (Tacoma)+13017158592,,92312385750#,,,,*391583# US (Washington DC) --- Dial by your location• +1 253 215 8782 US (Tacoma)• +1 301 715 8592 US (Washington DC)• +1 312 626 6799 US (Chicago)• +1 346 248 7799 US (Houston)• +1 646 876 9923 US (New York)• +1 669 900 6833 US (San Jose)• +1 613 209 3054 Canada• +1 647 374 4685 Canada• +1 647 558 0588 Canada• +1 778 907 2071 Canada• +1 204 272 7920 Canada• +1 438 809 7799 Canada• +1 587 328 1099 Canada Meeting ID: 923 1238 5750Passcode: 391583 Find your local number: https://usc.zoom.us/u/adSVscg4bE --- Join by SIP• 92312385750@zoomcrc.com --- Join by H.323• 162.255.37.11 (US West)• 162.255.36.11 (US East)• 115.114.131.7 (India Mumbai)• 115.114.115.7 (India Hyderabad)• 213.19.144.110 (Amsterdam Netherlands)• 213.244.140.110 (Germany)• 103.122.166.55 (Australia Sydney)• 103.122.167.55 (Australia Melbourne)• 149.137.40.110 (Singapore)• 64.211.144.160 (Brazil)• 149.137.68.253 (Mexico)• 69.174.57.160 (Canada Toronto)• 65.39.152.160 (Canada Vancouver)• 207.226.132.110 (Japan Tokyo)• 149.137.24.110 (Japan Osaka) Meeting ID: 923 1238 5750Passcode: 391583 Regards, Piyush
post226-comment1: I am receiving a message "Host has joined. We've let them know you're here" despite signing in with my USC credentials to Zoom. Could I be admitted in?
post226-comment2:  Please note that the exam for DEN students will start a few minutes late. M R Rajati’s phone 
post227: Can anyone pls share their cheat sheet? Thanks!
post227-comment1-reply2: This could be useful: https://github.com/aaronwangy/Data-Science-Cheatsheet
post227-comment2-reply2: bruh
post228:  Does anyone know how to do (d) of this question?
post228-comment1-reply2: I remember someone answered this question already but dont remember what number it was 
post229: Can anyone solve this problem please? ?
post229-comment1-reply2: <md>I think the solution in the following URL could be helpful. (page 13 Question 4 - Support Vector Machine). https://www.cs.cmu.edu/~epxing/Class/10701/exams/09s-701-final.pdf</md>
post229-comment2: (a): I believe it is 1, as that would be on the decision boundary y=x for the positive classes (Positive classes are also on the decision boundary worst case) (b): If the worst hyperplane is y=x, you can move it parallely till it is equidistant from y=x, the margins will then be defined by both the positive points and (0.5, 1), so those 3 points are your support vectors (c): I'm not sure what this question is asking, if they are completely separable and a margin has been created, unless a new training point violates the margin the slope will not change when adding new training points. (d): Self training labels the one we are most sure about, so furthest from the decision boundary, so I'm inclined to say (2, 0.5) but I haven't plotted it so there could be a closer point. Active learning labels the one we are most UNSURE about, so that'd be 3,3 (lies on the decision boundary). 
post230: It is showing 28 Tuesday on Github, is it the final deadline or was it 27?
post230-comment1-reply2: @508
post231: Sample MT 2 has the observations drawn, but Midterm 2 does not. 
post231-comment1-reply2: <p>You could maybe argue that one of the questions asks for the &#34;states&#34; and says &#34;show transition probabilities,&#34; while the other question asks for the &#34;model&#34; expecting every detail. Not clear though.</p> <p></p> <p>I would recommend drawing everything, including the observable states as well, as it won&#39;t cause any harm.</p>
post232: I've gone to nearly every single office hours but each time it is an empty zoom. Am I using the wrong link? I'm using the office hours pinned message that contains links for different times of the day. I have some questions that are difficult to type out so I would appreciate a chance to speak with a TA. Thanks
post232-comment1-reply2: Did you check to see if they are in-person or online? Not every office hour is held on zoom.
post233: For Q2 (3), When I calculate P(DEEF, O), I think the answer provided in dropbox has some problems. P(DEEF,O) should equal to 1*(0.8)*(0.8)*(0.6)*(0.8)*(0.4)*(0.2)*(0.2) = 4.916*10-4
post233-comment1-reply2: I also found that the calculations here were a bit off but the conceptual part is correct!
post234: I have a question from the Sample midterm. Is the mapping of positive and negatives arbitrary? I understand that they have to follow a consistent pattern, but confused as to whether or not there is exactly one correct mapping or a few different options for mapping. 
post234-comment1: Pretty sure they’re arbitrary, but a different mapping would give you different weights 
post234-comment2-reply2: Up to you as long as it is consistent 
post235: Are we responsible for these on the final? I think the professor mentioned we aren't responsible for BW in class, but confirming for Viterbi. 
post235-comment1-reply2: I don&#39;t believe so
post236: can anyone explain to me how we draw 0.8 .4 .2 in (a) and why F reverse back in the diagram? And how we compute (b)? Thank you 
post236-comment1-reply2: <p>I don&#39;t understand what you mean by F reverses back in the diagram...you draw an arrow connecting D, E, and F all back to themselves because that is the probability of state transition to themselves (if you look at Matrix A, D to D transition probability is 0.2, F to F is 0.6, and E to E is 0.8). Also note that each row in the matrices corresponds to D, E, and F (from top to bottom). Then, the rest of the state diagram transitions are just the values in the A matrix from D to E, E to F, and F to D (0.8, 0.2, and 0.4, respectively). Also, if you look at the A matrix there is 0.0 probability for certain transitions (like D to F, E to D, and F to E), which is why the arrows are not connecting those specific transitions.</p> <p></p> <p>B) pi = [1 0 0] shows us that the initial state is D. You want sequences where P(X, 0) &gt; 0 AND the initial state is D. So we know (D, X2, X3, X4). To find X2, we look at the A matrix and see that A(D, F) = 0.0 (so this is not a possible state transition bc its not &gt; 0.), A(D, E) = 0.8, ad A(D,D) = 0.2. SO, we know X2 can be D or E. Now to find X3, we look to see if A(D, E), A(D,F), A(D,D), A(E,F), or A(E,E), A(E,D) are &gt; 0. It looks like only A(E,E), A(E,F), and A(D,D), and A(D,E) are &gt; 0.0. Then, we do the same for X4, and see that A(E,E) and A(E,F) are &gt; 0.0, so the final sequences that are possible ( &gt; 0.0) are DEEE, DEEF, DDEE, DDEF).</p>
post236-comment2: Thanks for explanation. But I still wonder how we do the calculation in (b)?
post236-comment2-reply1: I think there’s a similar explanation in @949
post236-comment2-reply2:  Use this formula to calculate P(X,O) for each sequence X and O is given. 
post237: Can someone explain why these two are equal? 
post237-comment1-reply2: <a href="https://rpubs.com/ppaquay/65568">https://rpubs.com/ppaquay/65568</a><div><p></p><img src="https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flll6cacyxjfg3%2Flcp8r7usiibt0%2Fwbfrwvgdptwh%2Fpublic_2023_11_30.jpeg" /><br /></div>
post238: After the first round of clustering between X1 and X2, I got the below euclidean distance. The distance between the X1X2 cluster and X3 is 2 while the distance between X3 and X4 is also 2. I wonder if somebody got the same numbers as me? In this case, how should the dendrogram look like? IMG_1604.jpg
post238-comment1-reply2: <p>Same! I guess X3 and X4 need to be swapped in the fig2</p> <p></p>
post238-comment2: same
post238-comment3: Got the same numbers.
post239: Hi, can anyone help with this problem? I think a) is not linearly separable, but don't know how to solve b). Thank you! 
post239-comment1-reply2: Maybe you can see @935 as a reference
post239-comment2: @935 is clear but I suspect this question is wrong...
post239-comment2-reply1: (b) if f(x1,x2)=ax1^2+bx2^2+cxy+d, I do not think we can draw some quadratic boundaries that can separate those two classes. So I assume the problem itself wrong.
post240: Can someone explain why the back propagation sensitivity is different from the other back propagation sensitivities? 
post240-comment1-reply1: the last layer is where you multiply the difference between expected output and actual output the rest of the layers is where you calculate the partial derivative of the output with respect to the input
post241: Can someone explain how to calculate the probability of each of states? Thanks! 
post241-comment1-reply1: sum up the states where LA is first sum up the probability states where SD is first sum up the states where LA is 2nd sum up the probabliity of states where SD is 2nd sum up the probability of states where La is third sum up the probability of states where SD is third choose thwe highest probability at each step 
post241-comment2: With same method. why I get the different values as above table?
post241-comment2-reply1: Take sum up the states where LA is first sum up the probability states as an example:sum(P(LA))/SUM(P(LA)+P(SD)) 
post242: In the formula for sensitivity do we have to use the updated value for W(m+1) or the old value? 
post242-comment1-reply1: I think we should use “old” values of W(m&#43;1). Here calculating sensitivity is part of process in back propagation, which is the process of calculating gradient! We update weights with gradient after finishing the process of calculating gradient. Below is a more detailed breakdown:<div><br /></div><div><div> 1. Forward Propagation: Input data is fed into the network, and it passes through the layers. Each neuron applies a linear transformation (weight and bias) and then an activation function to the input data. The final output is obtained at the output layer.</div><div> 2. Loss Calculation: The output from the forward pass is used to calculate the loss, which measures the difference between the predicted output and the actual target values.</div><div> 3. Backpropagation: This is the process of calculating the gradient of the loss function with respect to each weight in the network. It starts from the output layer and moves backwards through the network, applying the chain rule to compute gradients step by step.</div><div> 4. Weight Update: After calculating these gradients, the weights are updated, typically using a gradient descent optimization algorithm. The update is usually done for all weights after computing the gradients for the entire training batch (batch gradient descent) or for each data point (stochastic gradient descent) or mini-batch.</div></div><div><br /></div><div>Hope these can be helpful.</div>
post243: How is the final tree constructed? at x> 2.5 there is a tie between red blue and green. Why is green not picked then? 
post243-comment1-reply1: The tree is wrongly made here. For x &lt; 2.5 swap the left and right nodes so it will be b on the left and g on the right.
post243-comment2: I believe the correct answer is [g, b, g] with the same splits. However, only the professor can confirm if it's right or not since he wrote the solution. 
post243-comment3: for x < 2.5, the classification should be "B". It is because, after we divided from the split 1.5, then only B remains on the left side of 2.5. Therefore, it should be classified as B. 
post243-comment3-reply1: why do we stop at 2.5 and not split at 3.5 as well?
post243-comment3-reply2: Because the question instructed to `Build a decision tree with three leaves`
post244: Could anyone explain how to calculate gini index here? Thanks! 
post244-comment1-reply2: <img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl75o477w67a5wy%2Ff0f221ed4c9223b56f7dfda8bba064461b3e9fc1837844dba104148f07279226%2Fimage.png" alt="image.pngNaN" />
post244-comment2: Thank u! What about second split?
post244-comment2-reply1: @888 comment section
post244-comment3: im still pretty confused on this, could someone explain it further? how does this relate to the given gini index equation in the question?
post244-comment3-reply1: For the first split, you calculate the Gini index for TH = 1.5, 2.5, and 3.5, which are $$\frac {10}{16}$$, $$\frac {10}{9}$$, and $$\frac {14}{16}$$, respectively. Since the Gini index is the smallest when TH = 1.5, that becomes the threshold for the first split. Next, repeat the same calculation for TH = 2.5 and 3.5 to determine the threshold for the second split.
post244-comment3-reply2: im still confused on this
post244-comment3-reply3: Its probability for each class for each split plugged into the formula.
post244-comment4: why is the second leaf g and not b? considering the two g's are left of 1.5 so they are in their own region, and that leaves only b in the region 1.5<x<2.5 why is it not b? is it majority class within each region or am i misunderstanding?
post244-comment4-reply1: I think that was a typo
post245: Hello, I was working on Question 4 of the April 2022 exam and got confused by the solutions, why is point x1 said to be part of cluster 1 when doing the reclustering? Root 2 is bigger than root 5/4 so shouldn't it be classified to the closest cluster? In addition, for the distance between X4 and Cluster 2, I am not sure how the professor got root 5/4 as the distance so if anyone could clarify this I would appreciate that 
post245-comment1-reply1: <p>@957</p> <p></p>
post245-comment2: x1 should be assigned to cluster2, and x4 to cluster1 then we have x2, x4 to cluster1, x1, x3 to cluster 2 which gives new centroids: Cluster 1 centroid (1, 3/2) Cluster 2 centroid (1, 0)
post246: I am not seeing how there is a unique solution for mu1, mu2, and sigma. It seems there are multiple solutions as long as mu1=sigma and mu2 = 2*mu1. For example, mu1 = 3, mu2 = 6 and sigma = 3 also works. Can someone please explain?
post246-comment1-reply1: yes when I took the exam I fixed one value for mu and calculated the rest, the question asks for any LDA classifier that works and I believe there are many 
post246-comment2: I think as long as u1 + u2 = 3, it should work?
post247:  Can someone explain this solution for Q5b on the midterm from 10/20/23? How was each mean calculated for each bootstrap sample? How could the expected value (y) of x=0 possibly be y=5.83 (unless it should be 0.5833)?? Our sample range for y is [-2, 2]Edit: I think I figured out the solution. In case someone else get's confused, here's what I came up with. Since K=2, we are taking the average Y of the two closest X points to x=0. For example, with {1,2,2} (a bootstrap sample of index 1 and two index 2) the solution is as follows: Bootstrap sample: (0,1), (-2, 2), (-2, 2) test point: x=0 Closest k=2 neighboring points are (0,1), (-2, 2). Average y's: (1+2)/2 = 3/2 Repeat for all 6 bootstrap samples so that predicted y's for each sample are 1,1,1,3/2,-1/2,-1/2 Sum and take an average: 3.5/6 = 0.5833 Predicted y at x=0 is 0.5833
post247-comment1-reply1: <p>I think I figured out the solution. In case someone else get&#39;s confused, here&#39;s what I came up with.</p> <p></p> <p>Since K=2, we are taking the average Y of the two closest X points to x=0. For example, with {1,2,2} (a bootstrap sample of index 1 and two index 2) the solution is as follows:</p> <p></p> <p>Bootstrap sample: (0,1), (-2, 2), (-2, 2)</p> <p>test point: x=0</p> <p>Closest k=2 neighboring points are (0,1), (-2, 2). </p> <p>Average y&#39;s: (1&#43;2)/2 = 3/2</p> <p>Repeat for all 6 bootstrap samples so that predicted y&#39;s for each sample are 1,1,1,3/2,-1/2,-1/2</p> <p>Sum and take an average: 3.5/6 = 0.5833</p> <p></p> <p>Predicted y at x=0 is 0.5833</p> <p></p> <p></p>
post247-comment2:  I think the calculation for D4 is incorrect. sqrt( (1-0)^2 + (1-2)^2 ) = sqrt(2). So, that means the distances from D3 and D4 are the same (although both are positive class). I did another calculation based on term frequency (# word / # words in doc instead of simply # words) and got D4 as the closest. This does not change the overall result (positive classification), but based on this weighted method D3 is the farthest from our test document. Is this term frequency method valid?
post248: 
post248-comment1-reply1: <p>a) no </p> <p>b) phi(x) = (x, x^2) </p> <p>c) when plotting the new points I see that x2 = 0.5 would be in the middle of them so w = 0 b = 0.5 , to go back to the original space &#43;-sqrt(0.5) approx -0.71 and 0.71 which would classify 1/3 as &#43; </p>
post248-comment2: can anyone solve d? 
post248-comment2-reply1: all i know is f(1) is probably Relu to mimic a nonlinear transformation (feature expansion) and f2 is probably sigmoid (for binary classification) how does one come up with weights and biases on the spot? 
post248-comment2-reply2: and W1 is two neurons so we can go from 1 dimensional to two dimensional
post248-comment2-reply3: would you be able to post your whole solution for this question please?
post248-comment3: Came up with a fun one it relies upon encoding Y + class as -1 and Y - class as 1 which seems backwards but led me to a simple solution: 
post248-comment3-reply1: can you share that solution?
post248-comment3-reply2: 
post248-comment3-reply3: I cannot see anything.
post248-comment3-reply4: ahh heic gimme a sec 
post248-comment3-reply5: wrong upload
post248-comment3-reply6: oh woops b = -1 forgot to change it! 
post248-comment3-reply7: 
post248-comment3-reply8: Which midterm is this from? I can't find this problem in the two sample exams. 
post248-comment3-reply9: I have no idea I've just been answering peoples questions I'm not sure where everyone is getting these from but they are very interesting 
post248-comment4: Please could you explain how you determined the boundary in 1D?
post248-comment4-reply1: right so phix was a feature expansion. we went from x to x1, x2 space using the (x1, x2) = (x, x^2). In the expanded feature space it was clear that x2 = 0.5 (a horizontal line) would be the decision boundary of the MMC. so then since x2 was achieved by squaring x we take the square root of x2 to determine that +- root 0.5 would be the decision boundaries for the original feature space 
post248-comment5: The two layer feedforward neural network for this maximum margin classifier has the following weights and biases: W(1) = [1, 0] b(1) = 0 W(2) = [-1/2, -1/2] b(2) = 1/2 f(1) = [u1(x), u2(x)] f(2) = [wTφ(x) + b]
post248-comment5-reply1: how did you get these values?
post249:  Hello, Can someone clarify the answer for point (a) in question #3 Thanks
post249-comment1-reply1: If you calculate the distance of each point to the initial centroids, you will find that the top 12 points are closest to the top initial centroid and the bottom 16 points are closest to the bottom initial centroid. So those become our clusters. With new clusters we will find the new centroids, since everything is nice and evenly spaced the top centroid will be in the middle of the middle row of points. The bottom centroid will be in the middle of the 1 and 2. If we recalculate each points distance from the new centroids we see the clusters do not change. this is all from eyeballing
post249-comment2: In question A, determine the distances from each data point to two centroids, and assign the data point to the cluster associated with the shortest distance.
post250: When looking visually at an MLP architecture, how do I distinguish between the number of neurons, layers, and classes? Thanks!
post250-comment1-reply1: <p>Layers can be counted after activation functions (think vertical lines like counting columns left to right).</p> <p></p> <p>With neurons if the weights are not matrices then you can count them like the number of rows, but a lot of the times they are squashed into matrices so then you have to count the number of columns in the weight matrix or the number of rows in the transpose weight matrix or the number of rows in the bias matrix or the number of rows in n (the S value)</p> <p></p> <p>The number of neurons in the output layer is the number of classes. If the output layer is just like one activation function then it&#39;s good think about what&#39;s going into that activation function once again the S value of the previous layer</p>
post251: When looking at an MLP architecture, how do I distinguish between the number of neurons, layers, and classes? Thanks!
post251-comment1-reply1: a neuron consists of the weighted sum operation, a bias term, and an activation function, a layer is like columns of a matrix, a layer may have multiple neurons but a neuron only has one layer, as for classes I think you can look at it like the # of columns of W, the rows of b, or the rows of n, practically, we saw a scenario where 2 classes were represented with s = 1 and where 4 classes were represented with s = 2. It depends on how you encode the classes
post251-comment2: but for the most part it seems like when we have 4 classes we have 2 neurons and when we have 2 classes we have 1 neuron 
post251-comment2-reply1: and while this may be true for the weight matrices we saw, i'm just finding out that the number of neurons in the output layer corresponds to the number of classes 
post252: Why the input isn't fully connected with next layer? 
post252-comment1-reply1: The point is to classify the cases where x1 and x2 are both 1 as 1 and where x3 and x4 are both 1 as 1. Therefore it would make sense to have two layers one for the case where x1 and x2 are 1 (1 AND 1) -&gt; 1 and the other for the case where x3 and x4 are both 1 (1 AND 1) -&gt; 1. We have two layers to deal with seperate cases but also to merge them together in an OR since class 1 is the case when (x1 AND x2 is 1) OR (x3 AND x4 is 1)
post252-comment1-reply1: The point is to classify the cases where x1 and x2 are both 1 as 1 and where x3 and x4 are both 1 as 1. Therefore it would make sense to have two layers one for the case where x1 and x2 are 1 (1 AND 1) -&gt; 1 and the other for the case where x3 and x4 are both 1 (1 AND 1) -&gt; 1. We have two layers to deal with seperate cases but also to merge them together in an or since class 1 is when x1 AND x2 is 1 OR x3 AND x4 is 1 
post252-comment2: there are scenarios where you want the entire vector to go into both layers for example the XOR
post252-comment2-reply1: where is -1, 2 and all coming from ? 
post252-comment2-reply2: i mean how are we deciding on weights = 2 and bias = -1
post252-comment2-reply3: backpropogation
post252-comment2-reply4: ill post an example 
post252-comment2-reply5: you would forward propagate through the AND layer, then forward propagate through the OR layer, and similarly, you would backward propagate through the OR layer, and then backward propagate through the AND layer. new_parameter=old_parameter+learning_rate×gradient Here, the gradient is the partial derivative of the loss with respect to the parameter being updated.
post252-comment2-reply6: https://www.youtube.com/watch?v=tUoUdOdTkRw
post252-comment2-reply7: thanks
post252-comment2-reply8: Can you please explain how to solve this question from scratch? I'm completely lost after creating the x vectors :(
post253: Can someone explain how the equation x1-x2=0 came?
post253-comment1-reply8: not sure if this is the right way but: if you plot the points you&#39;ll see that x1 and x2 are on one parallel hyperplane and x3 and x4 are on the other. So we want the equation of the line in between. The midpoint between x1 and x3 is (0.5, 0.5) and the midpoint between x2 and x4 (-0.5, -0.5). from here we can determine the slope of the line is 1. if we plug into point slope form x1 - 0.5 = 1 ( x2 - 0.5) -&gt; x1 = x2 -&gt; x1 - x2 = 0 
post253-comment2: Not sure how x1, x2 and x3, x4 each land up on parallel hyperplanes, in the diagram provided. Can you explain?
post253-comment2-reply1: nvm got it
post253-comment3: Actually I think the linear decision boundary should be x2-x1 =0.Because x1=[0,1] with label y=1. If use the decision boundary f(x) = x1-x2, f(x1) = 0-1 =-1, the label y should be-1. Thus, I think x2-x1=0 might be better.
post253-comment3-reply1: I agree, this is what I got, did anyone verify this / is this also a correct solution? 
post254: I understood that after the first iteration we have the following 3 clusters: C1 = {x1, x2}, C2 = {x3} and C3 = {x4}. Now for the next iteration I am assuming to find the distance between C1 and C2 for example we must compute min(d(x1, x3), d(x2, x3)) = min(2, sqrt(5)) = 2 but the answer mentions sqrt(5). Could someone explain how? And similarly should the distance between C1 and C3 be sqrt(5) and not 2.
post254-comment1-reply1: Yeah the only ways to get the values he got is if we performed complete linkage which takes the max distance between clusters. that means the distance between cluster 1 and cluster 2 is performed by the distance between (2, 0) (0, 1). Between cluster 1 and cluster 3 the distance between (0, 0) and (2,2). Between cluster 2 and 3 is the same (2,2) and (2,0) since there is only one point in each cluster.
post255: When running spectral clustering, I'm getting a LinAlg Error that says SVD did not converge. Does anyone know how to resolve this?
post256: Edit: Realized that this is the same question as @920 but can we please get a confirmation on which answer is correct? I understand how we get splits at 1.5 and 2.5 but why is the tree like this: x<1.5 /\ / \ g x<2.5 /\ / \ g b My understand is that at split 1.5, we have majority of green on the left(2 green/all green) and for the 2nd split(2.5), we have 1 blue on the left and 1 red, green, blue on the right. So shouldn't the tree look like this: x<1.5 /\ / \ g x<2.5 /\ / \ b g (due to tie being broken in favor of smaller number)
post256-comment1-reply1: <p>we do the first split at 1.5 then we split at 2.5 less than 2.5 has majority green, greater than 2.5 is a tie between r g b but since we already used green it can&#39;t be that and then we have to break a tie between red and blue and blue has less value so we choose the smaller one</p> <p></p> <p>I think what you are doing is you&#39;re excluding the data from the first split but we are supposed to keep it </p>
post256-comment2: I got same result as you(gbg).
post256-comment2-reply1: same here
post256-comment2-reply2: I could be wrong I'll get back to you guys 
post257:  I am a little confused on the solution for part a of this question. I presumed the single linkage would fuse first, but the solution says that there is not enough information, because the closest two points (single linkage) and furthest two points (complete) could be equal. But I can't think of a scenario where that would be true for five separate points? Unless the points in each cluster are on top of each other. Is the solution wrong or can someone please visualize this scenario. Thanks!!
post257-comment1-reply2: <p>Consider this case - 8, 9 are vertices of two symmetrical cones with the same height - and 1, 3, 5 lies on the base circle of the two cones<br /><br />Do you see that d(1, 8) = d(1, 9) = d(3, 8) = d(3, 9) = d(5, 8) = d(5, 9)? That is, heights of single linkage and complete linkage are equal for these 2 clusters.</p> <p></p> <p>P.S., I know my drawing is ugly. It is just for visualization, so please do not use rulers to measure the actual distances</p> <p></p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjqgzl00wahz3nb%2F3672bc25835df0dd76204be497f54551f250af78749a0c2707e20f896a428a7e%2F1111701218651_.pic_hd.jpg" alt="1111701218651_.pic_hd.jpgNaN" /></p>
post257-comment1-reply2: <p>Consider this case - 8, 9 are vertices of two symmetrical cones with the same height - and 1, 3, 5 lies on the base circle of the two cones<br /><br />Do you see that d(1, 8) = d(1, 9) = d(3, 8) = d(3, 9) = d(5, 8) = d(5, 9)? That is, heights of single linkage and complete linkage are equal for these 2 clusters.</p> <p></p> <p>P.S., I know my drawing is ugly. It is just for visualization (please do not use rulers to measure the actual distances)</p> <p></p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjqgzl00wahz3nb%2F3672bc25835df0dd76204be497f54551f250af78749a0c2707e20f896a428a7e%2F1111701218651_.pic_hd.jpg" alt="1111701218651_.pic_hd.jpgNaN" /></p>
post257-comment1-reply2: <p>Consider this case - 8, 9 are vertices of two symmetrical cones with the same height - and 1, 3, 5 lies on the base circle of the two cones<br /><br />Do you see that d(1, 8) = d(1, 9) = d(3, 8) = d(3, 9) = d(5, 8) = d(5, 9)? That is, heights of single linkage and complete linkage are equal for these 2 clusters</p> <p></p> <p>P.S., I know my drawing is ugly. It is just for visualization (i.e., please do not use rulers to measure the actual distance between points)</p> <p></p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjqgzl00wahz3nb%2F3672bc25835df0dd76204be497f54551f250af78749a0c2707e20f896a428a7e%2F1111701218651_.pic_hd.jpg" alt="1111701218651_.pic_hd.jpgNaN" /></p>
post257-comment1-reply2: <p>Consider this case - 8, 9 are vertices of two symmetrical cones with the same height - and 1, 3, 5 lies on the base circle of the two cones<br /><br />Do you see that d(1, 8) = d(1, 9) = d(3, 8) = d(3, 9) = d(5, 8) = d(5, 9)? That is, heights of single linkage and complete linkage are equal for these 2 clusters</p> <p></p> <p></p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjqgzl00wahz3nb%2F3672bc25835df0dd76204be497f54551f250af78749a0c2707e20f896a428a7e%2F1111701218651_.pic_hd.jpg" alt="1111701218651_.pic_hd.jpgNaN" /></p>
post257-comment1-reply2: <p>Consider this case - 8, 9 are vertices of two symmetrical cones with the same height - and 1, 3, 5 lies on the base circle of the two cones<br /><br />Do you see that d(1, 8) = d(1, 9) = d(3, 8) = d(3, 9) = d(5, 8) = d(5, 9)? That is, heights of single linkage and complete linkage are equal for these 2 clusters</p> <p></p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjqgzl00wahz3nb%2F255d1e5243588934e507ca2ed06e43895211b02ae37ba55228ff56bfca0df4cf%2FWechatIMG110.jpg" alt="WechatIMG110.jpgNaN" /></p>
post257-comment2: I see. My issue was I was only thinking in 2-dimensions. Thank you!
post258: This post (@883) asks the same question but I'm confused which one is the correct answer: 1) P(DEEF, O) = (1*0.8) * (0.2*0.8) * (0.6*0.8) * (0.2*0.2) or 2) P(DEEF, O) =(1*0.8) * (0.8*0.6) * (0.8*0.4) * (0.2*0.2) If the correct answer is 2), I understand how it is done but if the it is 1), can someone please explain how we come up with it? Thank you.
post258-comment1-reply2: The answer is 2. I asked prof in class and he said it was a typo
post258-comment2:  Some students mentioned there might be a miscalculation in the answer. The solution strategy is straightforward, so I think one must double check the soundness of the calculations. M R Rajati’s phone 
post259: Hi all! I have been struggling with understanding MLP architecture drawings with AND and OR operations, and reviewing the class slides and practice problems hasn't helped much. Does anyone have any recommendations for additional resources that might help? Thanks!
post259-comment1-reply2: <a href="https://www.youtube.com/watch?v=tUoUdOdTkRw" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=tUoUdOdTkRw</a> I feel like most people including me are confused as to how the weights and bias are calculated. Here&#39;s a video on backpropagation which is how a neural network updates its weights and bias. 
post260: Is this cluster wrong? Is it supposed to calculate the distance between each point and the centroid and which is closer to which cluster? Apparently, √2 >√(4/5), so it should be clustered into cluster2? 
post260-comment1-reply2: Yep @887
post261: I'm a little confused how to do this, does this pseudocode look fine? For t in range (101): group all datapoints in cluster 0, 1 based on distance calculate fpr and tpr plot fpr and tpr This would lead 100 values, but I'm not sure where "majority polling" comes into play here? For KMeans I use distance and a softmax, based on the footnotes I assume I can't repeat that here, even if I do calculate the center distance manually? 
post262: For the probabilities, should I use the distances the the nearest cluster? Which means I would have (dataset_size*0.8 values)? Or should I use the distance to both the clusters, which would mean (dataset_size*0.8*2 values)? Thank you!
post262-comment1-reply2: You will need the distance of each datapoint from both cluster centers to calculate probabilities using softmax. You will get &#34;(dataset_size*0.8*2 values)&#34;, and after you apply softmax on each distance pair, you will get probabilities, which will sum up to 1 for each datapoint.
post262-comment2: So the softmax should be over ALL distances for both clusters? Thank you! 
post262-comment2-reply1: All distances but row-wise. So if you have a nX2 matrix of distances from 2 cluster centers, apply softmax for each row and you will get probabilities matrix of nX2, where each row will sum up to 1.
post262-comment2-reply2: Awesome, thank you! Then for the ROC curve, function, pass in the probability of the cluster it was assigned to right? or do I use these values for the tpr fpr? 
post262-comment2-reply3: I do it manually by finding tpr-fpr values and plotting them but both the ways are fine.
post263: From the lecture, ![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fkjw2zepev894fm%2Fbdbac523f79388ecf6a40f7309461e1c69076d0f55f814022acc47ba54d4c101%2Fimage.png) and a problem from the practice exam, ![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fkjw2zepev894fm%2Fa0001a8f19622a47a523ce5671ea19c00ae70e7043a6447688a30d09fabe7417%2Fimage.png) Given ``` π = [0.6 0.4] ``` Would I be correct in saying that it's ``` LA SD LA 0.9 0.1 SD 0.2 0.8 ``` and ``` L M H LA 0.05 0.15 0.8 SD 0.1 0.6 0.3 ``` Then to show the work, how do I use these matrices to lineup what's shown in the solutions?
post263-comment1-reply3: <p>doing all would be a lot but here&#39;s an overview</p> <p></p> <p>P(LA, LA, LA, L, H, M) = P(LA) = 0.7 * P(L | LA) * 0.05 * P(LA | LA) = 0.9 * P(H | LA) = 0.8 * P(LA | LA) = 0.9 * P(M | LA) = 0.15 </p> <p></p> <p>you do this for all possibilities and calculate the probabilities. The dynamic programming solution is SD, SD, SD because it is the highest probability (max Likelihood path) </p> <p></p> <p>The expectation maximization solution is different. We maximize the expected number of correct states which means at each step we search for all paths that have LA in 1st position and sum them up, same for SD, we find that having LA in the first step is more probable.</p> <p></p> <p>Next, we search for all paths that have LA in 2nd position and sum them up, same for SD, we find that having LA in the second step is more probable. </p> <p></p> <p>Finally, we search for all paths that have LA in 3rd position and sum them up, same for SD, we find that having SD in the third step is more probable. </p> <p></p> <p>This gives us La, La , SD for expectation maximization</p>
post263-comment2: Oh, I get that. How do you go about doing the expectation maximization?
post263-comment2-reply1: I mean, according to the table here: ![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fkjw2zepev894fm%2Fdb0374ca4970bef2d7db00359575467cc7e7502105cbf3278d01124ae99a6371%2Fimage.png)
post263-comment2-reply2: For the first position, sum all of the probabilities that have LA in the first spot, and all that have SD in the first spot. Divide each of those by the sum of the two, to get the output in the table provided. Repeat for the second and third spots.
post264: 'It is left to the student to verify that a single layer perceptron with two neurons can classify the vectors given that the four classes are modeled using vectors. Then the diagram is given' To me, that looks like a single layer perceptron with 1 neuron. Am I missing something? Does this have to do with our weight vector being 2 by 2 instead of 2 by 1 as it normally is? Is an activation function its own neuron? From all definitions I've seen: a neuron consists of the weighted sum operation, a bias term, and an activation function. cheers
post264-comment1-reply2: Now that i look at it is because W = [ w1 w2] and it actually could be broken into two neurons if one wanted to
post264-comment2: Hey so is this solution arrived at by taking an initial assumption of W to be 2x2 matrix of 0 and then correctly it with each instance? Are we expected to show all the steps leading up to the answer for this question or is there some trick I am missing to get to the answer?
post265: It seems that afternoon lecture was cancelled today because of the guest lecture earlier, along with the after-class office hours. Is there any way we'll be able to talk to professor Rajati before Wednesday? Also, are we still having morning and afternoon lectures?
post265-comment1-reply2: you can always email professor to try to get an time setup (provided that its not CP application related)
post266: On the syllabus it says Monday Nov 28, but it’s the 27th. Just clarifying if the HW is due tonight or tomorrow night
post266-comment1-reply2: @508
post266-comment2:  It's 28th, right? 
post266-comment2-reply1: It's 27th, Monday
post266-comment2-reply2: Can a TA confirm or deny which? Currently conflicting messages..
post266-comment2-reply3: @508
post267: If I am turning in my HW late and need to use some of the late days, do I have to notify the TAs or note it down anywhere? Or if the HW is turned in late will it automatically be checked?
post267-comment1-reply3: Just write how many of your late days you want to use at the top of the notebook
post268: Is the recording for 11/20 afternoon section going to upload to DEN?
post268-comment1-reply3: I think that class was on reinforcement learning. There’s a lecture for it under week 13.
post268-comment2: Thanks.
post269: Hi, I am just wondering how is the sequence of the observation O affecting the selection of the sequence of states? For example if the observation is (0,1,0,2) and the states is (A B C) and we have X1 X2 X3 X4. does that mean for the sequence of states I need to choose at least two A or B or C in order to satisfy the requirement? (I know P(X,O) not equal to 0 is another determination, just want to ask about above question) Thanks!
post269-comment1-reply3: I would assume you map A to 0 B to 1 and C to 2?
post270: I don't understand the solution given, wouldn't the point nearest to the decision boundary be the most informative / most uncertain for both active learning and self training? 
post270-comment1-reply3: <p></p> <h3>Active Learning:</h3> <p>In active learning, the emphasis is on selecting data points that are most informative or uncertain to the model. By choosing points closest to the decision boundary, you target instances where the model is most uncertain about the correct label. This strategy aims to reduce model uncertainty and improve its performance by focusing on the most challenging instances.</p> <h3>Self-Training:</h3> <p>In self-training, the iterative process involves expanding the labeled dataset by selecting points that the current model is confident about. By choosing points furthest from the decision boundary, you are likely selecting instances that the model is more confident in classifying correctly. This strategy aims to gradually expand the model&#39;s training set with instances that it confidently classifies, potentially improving its ability to generalize</p>
post270-comment1-reply3: <p>Ok here I am answering myself again: </p> <p></p> <h3>Active Learning:</h3> <p>In active learning, the emphasis is on selecting data points that are most informative or uncertain to the model. By choosing points closest to the decision boundary, you target instances where the model is most uncertain about the correct label. This strategy aims to reduce model uncertainty and improve its performance by focusing on the most challenging instances.</p> <h3>Self-Training:</h3> <p>In self-training, the iterative process involves expanding the labeled dataset by selecting points that the current model is confident about. By choosing points furthest from the decision boundary, you are likely selecting instances that the model is more confident in classifying correctly. This strategy aims to gradually expand the model&#39;s training set with instances that it confidently classifies, potentially improving its ability to generalize</p>
post270-comment2-reply3: <p>Let me expand a little bit</p> <p></p> <p>Why we say points that are close to the decision boundary most informative? Because it may change our decision boundary - imaging a SVM decision boundary - you can see that points close to the boundary, if their labels are fixed - change the boundary (and thus our model)</p> <p></p> <p><a href="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fjqgzl00wahz3nb%2Fbecdaa9f16f94aa4dbfd69c34d2760f6ee291d54634624686bd97fae3b291e02%2FWechatIMG106.jpg" target="_blank" rel="noopener noreferrer">Most_Informative_Point.jpg</a></p>
post271: Hello, I am trying to understand the solution for question 5. Where are the weights and biases(2 and -1 for the first layer and 2 and 1 for the second layer) derived from and how are they derived? In addition, I am a bit confused on why the first layer exists. Is it just because the question asked for a multi-layer model? It seems that you could train the classifier on the whole 4 element vector and the rule would be if either the first two or last two elements are 1, the result would be a 1. Is this not possible within a single layer, which is why we have to break it into the first two elements and the last two elements in the first layer? Any clarification would help. 
post271-comment1-reply3: <p>Hi, <br />According to my understanding, we got the weight and bias via cross validation or in simple terms via hit and trial method. We can put the values of vector X1 to X4 and simply follow the diagram to get final output as 1 or 0. Based on 0 or 1, we can assign the vectors to their class. I am not sure how we can solve this using only one layer. </p> <p></p> <p>Hope it helps.</p>
post271-comment2: I really think he's trying to teach us what the weights and biases for an AND layer looks like and what an OR layer looks like when using the bipolar step function for activation. The step function favors the positive class, this set up where the AND layer has a positive bias term and the OR layer has a negative bias term and positive weights of the same magnitude work. 
post271-comment2-reply1: Could you elaborate on positive/negative bias terms. Isn't the bias term '-1' for the AND operator and '1' for the OR operator? Also, what is the calculation behind the first neuron, is it x1 * 2 + x2 * 2 - 1? if so what would be x1, is it the first element of the x1 vector or the entire vector? sorry I am a little lost on this one, thank you :>
post272:  I am trying to solve this problem, but I never get the given W vector and B vector. I am initialize the both as 0, and calculating through perceptron. Am I doing something worng? Hope anyone posts the answer with the right step to take. Thank you!
post272-comment1: I also got different weights that produce 0 training error, while I think that is fine, I am curious to see how the weights in the answer were calculated. 
post272-comment2-reply1: <p>import numpy as np</p> <p># Given data<br />X = np.array([[1, 1], [1, 2], [2, -1], [2, 0], [-1, 2], [-2, 1], [-1, -1], [-2, -2]])<br />targets = np.array([[-1, -1], [-1, -1], [-1, 1], [-1, 1], [1, -1], [1, -1], [1, 1], [1, 1]])</p> <p># Given weights and biases<br />W = np.array([[0, 0], [0, 0]], dtype=np.float64)<br />b = np.array([0, 0], dtype=np.float64)</p> <p># Learning rate<br />learning_rate = 0.5</p> <p># Activation function (Step function)<br />def activation(x):<br /> return np.where(x &gt;= 0, 1, -1)</p> <p># Training the perceptron for n iterations<br />for iteration in range(1000):<br /> for i in range(len(X)):<br /> # Forward pass<br /> net = np.dot(X[i], W.T) &#43; b<br /> y = activation(net)</p> <p> # Update weights and biases<br /> delta_w = learning_rate * np.outer(targets[i] - y, X[i])<br /> delta_b = learning_rate * (targets[i] - y)</p> <p> W = W &#43; delta_w.astype(np.float64)<br /> b = b &#43; delta_b.astype(np.float64)</p> <p> # Print weights and biases after each iteration<br /> print(f&#34;Iteration {iteration &#43; 1} - W: {W}, b: {b}&#34;)</p> <p># Print the final weights and biases<br />print(&#34;\nFinal Weights (W):&#34;)<br />print(W)<br />print(&#34;\nFinal Biases (b):&#34;)<br />print(b)</p> <p></p> <pre> Iteration 1 - W: [[-2. 1.][ 1. -2.]], b: [0. 0.] Iteration 2 - W: [[-3. -1.][ 1. -2.]], b: [-1. 0.] It converges </pre>
post272-comment2-reply1: <div> <p>Feel free to run this code. It does 1000 iterations of the perceptron algorithm. I have not been able to find the correct pairing of W alpha and b that gives the matrix and b that the professor arrives upon but you might be able to </p> <p>import numpy as np</p> <p># Given data<br />X = np.array([[1, 1], [1, 2], [2, -1], [2, 0], [-1, 2], [2, -1], [-1, -1], [-2, -2]])<br />targets = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1], [-1, 1], [1, -1], [-1, -1], [1, 1]])</p> <p># Given weights and biases<br />W = np.array([[0, 0], [0, 0]], dtype=np.float64)<br />b = np.array([0, 0], dtype=np.float64)</p> <p># Learning rate<br />learning_rate = 0.5</p> <p># Activation function (Step function)<br />def activation(x):<br /> return np.where(x &gt;= 0, 1, -1)</p> <p># Training the perceptron for n iterations<br />for iteration in range(1000):<br /> for i in range(len(X)):<br /> # Forward pass<br /> net = np.dot(X[i], W.T) &#43; b<br /> y = activation(net)</p> <p> # Update weights and biases<br /> delta_w = learning_rate * np.outer(targets[i] - y, X[i])<br /> delta_b = learning_rate * (targets[i] - y)</p> <p> W = W &#43; delta_w.astype(np.float64)<br /> b = b &#43; delta_b.astype(np.float64)</p> <p> # Print weights and biases after each iteration<br /> print(f&#34;Iteration {iteration &#43; 1} - W: {W}, b: {b}&#34;)</p> <p># Print the final weights and biases<br />print(&#34;\nFinal Weights (W):&#34;)<br />print(W)<br />print(&#34;\nFinal Biases (b):&#34;)<br />print(b) </p> <p></p> <pre> Final Weights (W): [[ 2. -3.] [-2. 1.]] Final Biases (b): [-2. 3.] </pre> </div> <p> So carrying out the standard perceptron algorithm with W = [ [0 0] [0 0] ], learning rate = 0.5, b = [0 0] does not converge to values the professor provided even after 1000 iterations, I don&#39;t know how the professor got there but it&#39;s not with the standard values</p>
post272-comment2-reply1: <div> <p>import numpy as np</p> <p># Given data<br />X = np.array([[1, 1], [1, 2], [2, -1], [2, 0], [-1, 2], [2, -1], [-1, -1], [-2, -2]])<br />targets = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1], [-1, 1], [1, -1], [-1, -1], [1, 1]])</p> <p># Given weights and biases<br />W = np.array([[0, 0], [0, 0]], dtype=np.float64)<br />b = np.array([0, 0], dtype=np.float64)</p> <p># Learning rate<br />learning_rate = 0.5</p> <p># Activation function (Step function)<br />def activation(x):<br /> return np.where(x &gt;= 0, 1, -1)</p> <p># Training the perceptron for n iterations<br />for iteration in range(1000):<br /> for i in range(len(X)):<br /> # Forward pass<br /> net = np.dot(X[i], W.T) &#43; b<br /> y = activation(net)</p> <p> # Update weights and biases<br /> delta_w = learning_rate * np.outer(targets[i] - y, X[i])<br /> delta_b = learning_rate * (targets[i] - y)</p> <p> W = W &#43; delta_w.astype(np.float64)<br /> b = b &#43; delta_b.astype(np.float64)</p> <p> # Print weights and biases after each iteration<br /> print(f&#34;Iteration {iteration &#43; 1} - W: {W}, b: {b}&#34;)</p> <p># Print the final weights and biases<br />print(&#34;\nFinal Weights (W):&#34;)<br />print(W)<br />print(&#34;\nFinal Biases (b):&#34;)<br />print(b) </p> <p></p> <pre> Final Weights (W): [[ 2. -3.] [-2. 1.]] Final Biases (b): [-2. 3.] </pre> </div> <p> So carrying out the standard perceptron algorithm with W = [ [0 0] [0 0] ], learning rate = 0.5, b = [0 0] does not converge to values the professor provided even after 1000 iterations, I don&#39;t know how the professor got there but it&#39;s not with the standard values</p>
post272-comment2-reply1: <div> <p>import numpy as np</p> <p># Given data<br />X = np.array([[1, 1], [1, 2], [2, -1], [2, 0], [-1, 2], [2, -1], [-1, -1], [-2, -2]])<br />targets = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1], [-1, 1], [1, -1], [-1, -1], [1, 1]])</p> <p># Given weights and biases<br />W = np.array([[0, 0], [0, 0]], dtype=np.float64)<br />b = np.array([0, 0], dtype=np.float64)</p> <p># Learning rate<br />learning_rate = 0.5</p> <p># Activation function (Step function)<br />def activation(x):<br /> return np.where(x &gt;= 0, 1, -1)</p> <p># Training the perceptron for n iterations<br />for iteration in range(1000):<br /> for i in range(len(X)):<br /> # Forward pass<br /> net = np.dot(X[i], W.T) &#43; b<br /> y = activation(net)</p> <p> # Update weights and biases<br /> delta_w = learning_rate * np.outer(targets[i] - y, X[i])<br /> delta_b = learning_rate * (targets[i] - y)</p> <p> W = W &#43; delta_w.astype(np.float64)<br /> b = b &#43; delta_b.astype(np.float64)</p> <p> # Print weights and biases after each iteration<br /> print(f&#34;Iteration {iteration &#43; 1} - W: {W}, b: {b}&#34;)</p> <p># Print the final weights and biases<br />print(&#34;\nFinal Weights (W):&#34;)<br />print(W)<br />print(&#34;\nFinal Biases (b):&#34;)<br />print(b) </p> <p></p> <pre> Final Weights (W): [[ 2. -3.] [-2. 1.]] Final Biases (b): [-2. 3.] </pre> </div> <p> So carrying out the standard perceptron algorithm with W = [ [0 0] [0 0] ], learning rate = 0.5, b = [0 0] does not converge to values the professor provided even after 1000 iterations</p>
post272-comment2-reply1: <div> <p>import numpy as np</p> <p># Given data<br />X = np.array([[1, 1], [1, 2], [2, -1], [2, 0], [-1, 2], [2, -1], [-1, -1], [-2, -2]])<br />targets = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1], [-1, 1], [1, -1], [-1, -1], [1, 1]])</p> <p># Given weights and biases<br />W = np.array([[0, 0], [0, 0]], dtype=np.float64)<br />b = np.array([0, 0], dtype=np.float64)</p> <p># Learning rate<br />learning_rate = 0.5</p> <p># Activation function (Step function)<br />def activation(x):<br /> return np.where(x &gt;= 0, 1, -1)</p> <p># Training the perceptron for n iterations<br />for iteration in range(1000):<br /> for i in range(len(X)):<br /> # Forward pass<br /> net = np.dot(X[i], W.T) &#43; b<br /> y = activation(net)</p> <p> # Update weights and biases<br /> delta_w = learning_rate * np.outer(targets[i] - y, X[i])<br /> delta_b = learning_rate * (targets[i] - y)</p> <p> W = W &#43; delta_w.astype(np.float64)<br /> b = b &#43; delta_b.astype(np.float64)</p> <p> # Print weights and biases after each iteration<br /> print(f&#34;Iteration {iteration &#43; 1} - W: {W}, b: {b}&#34;)</p> <p># Print the final weights and biases<br />print(&#34;\nFinal Weights (W):&#34;)<br />print(W)<br />print(&#34;\nFinal Biases (b):&#34;)<br />print(b) </p> <p></p> <pre> Final Weights (W): [[ 2. -3.] [-2. 1.]] Final Biases (b): [-2. 3.] </pre> </div> <p>So carrying out the standard perceptron algorithim with W = [ [0 0] [0 0] ], learning rate = 0.5, b = [0 0] does not converge to values provided</p>
post272-comment3: @Shiva, your inputs and targets are incorrect. Your third last input should be [-2, 1], and your targets are wrong (ex the second one should be [-1, -1] etc.) 
post272-comment3-reply1: woops updated
post272-comment3-reply2: Thank you! Since this is what is in the solution manual, do you mind outputting the weights after each datapoint? might be easier to error check with manual solutions 😅
post272-comment3-reply3:  Weight: [[-1. -1.] [-1. -1.]] bias: [-1. -1.] Weight: [[-1. -1.] [-1. -1.]] bias: [-1. -1.] Weight: [[-1. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-1. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-2. 1.] [ 1. -2.]] bias: [0. 0.] Weight: [[-2. 1.] [ 1. -2.]] bias: [0. 0.] Weight: [[-2. 1.] [ 1. -2.]] bias: [0. 0.] Weight: [[-2. 1.] [ 1. -2.]] bias: [0. 0.] Iteration 1 - W: [[-2. 1.] [ 1. -2.]], b: [0. 0.] Weight: [[-2. 1.] [ 1. -2.]] bias: [0. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Iteration 2 - W: [[-3. -1.] [ 1. -2.]], b: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Weight: [[-3. -1.] [ 1. -2.]] bias: [-1. 0.] Iteration 3 - W: [[-3. -1.] [ 1. -2.]], b: [-1. 0.] Final Weights (W): [[-3. -1.] [ 1. -2.]] Final Biases (b): [-1. 0.]
post272-comment3-reply4: When I run your code, I didn't have the answer.
post272-comment4: can someone clarify this note and how t1,t2,t3,t4 are identified? thx 
post272-comment4-reply1: you want to think of the step function, it's going to output 1 for values greater than or equal to 0 and -1 for negative values. So if our activation function outputs 2 values and we need something that classifies four distinct values we're going to have to use pairings to create 4 labels. It does not really matter which order you choose it in, but if you want to end up with the same weight matrix in the answer then you will assign cluster 1 -> [-1 , -1] cluster 2 -> [-1 ,1] cluster 3 -> [1, -1] cluster 4 -> [1, 1] 
post272-comment4-reply2: Correct, adding on to the student above, the way I look at it is By plotting the points and assigning cluster values based on whether the region is above the decision boundary (1) or below the decision boundary (-1) for each region. 
post272-comment5: Could someone clarify how we determine the dimensions of the weight and bias matrices?
post273: Hello,Can someone please provide an elaborate solution for part b and c in the below question? 
post273-comment1-reply2: <p>b)</p> <ol><li>a&#43;b&#43;c&#43;d≥0</li><li><br /><br /><br />a&#43;4b&#43;2c&#43;d≥0</li><li><br /><br />4a&#43;4b&#43;4c&#43;d≥0</li><li>2.25a&#43;2.25b&#43;2.25c&#43;d&lt;0</li></ol> <p>trial and error bring me to a fun solution a =1 b =1 c =1 d =0 cool there are probably many more and smarter ways to solve this </p> <p></p> <p>next we substitute these values for X3 = X1^2 &#43; X2^2 &#43; X1X2 </p> <p></p> <p>B0 = X1^2 &#43; X2^2 &#43; X1X2 </p> <p></p> <p>C) </p> <p>in the original feature space X2 = (-X1^2 &#43; X3 ) / X1 </p> <p></p> <p>so X2 = (-X1 ^2 &#43; B0) / X1 </p> <p></p> <p>I would look at the case B0 = 0 which gives us a decision boundary of x2 = - x1 which means the data above the line with slope of -1 would be class 1 and below would be class 2. For B0 not equal to zero, im not sure how to do that by hand</p> <p></p> <p>(THIS IS WRONG awesome solution below)</p> <p></p>
post273-comment1-reply2: <p>b)</p> <ol><li>a&#43;b&#43;c&#43;d≥0</li><li><br /><br /><br />a&#43;4b&#43;2c&#43;d≥0</li><li><br /><br />4a&#43;4b&#43;4c&#43;d≥0</li><li>2.25a&#43;2.25b&#43;2.25c&#43;d&lt;0</li></ol> <p>trial and error bring me to a fun solution a =1 b =1 c =1 d =0 cool there are probably many more and smarter ways to solve this </p> <p></p> <p>next we substitute these values for X3 = X1^2 &#43; X2^2 &#43; X1X2 </p> <p></p> <p>B0 = X1^2 &#43; X2^2 &#43; X1X2 </p> <p></p> <p>C) </p> <p>in the original feature space X2 = (-X1^2 &#43; X3 ) / X1 </p> <p></p> <p>so X2 = (-X1 ^2 &#43; B0) / X1 </p> <p></p> <p>I would look at the case B0 = 0 which gives us a decision boundary of x2 = - x1 which means the data above the line with slope of -1 would be class 1 and below would be class 2. For B0 not equal to zero, im not sure how to do that by hand</p> <p></p> <p>(this is wrong look below)</p> <p></p>
post273-comment1-reply2: <p>b)</p> <ol><li>a&#43;b&#43;c&#43;d≥0</li><li><br /><br /><br />a&#43;4b&#43;2c&#43;d≥0</li><li><br /><br />4a&#43;4b&#43;4c&#43;d≥0</li><li>2.25a&#43;2.25b&#43;2.25c&#43;d&lt;0</li></ol> <p>trial and error bring me to a fun solution a =1 b =1 c =1 d =0 cool there are probably many more and smarter ways to solve this </p> <p></p> <p>next we substitute these values for X3 = X1^2 &#43; X2^2 &#43; X1X2 </p> <p></p> <p>B0 = X1^2 &#43; X2^2 &#43; X1X2 </p> <p></p> <p>C) </p> <p>in the original feature space X2 = (-X1^2 &#43; X3 ) / X1 </p> <p></p> <p>so X2 = (-X1 ^2 &#43; B0) / X1 </p> <p></p> <p>I would look at the case B0 = 0 which gives us a decision boundary of x2 = - x1 which means the data above the line with slope of -1 would be class 1 and below would be class 2. For B0 not equal to zero, im not sure how to do that by hand</p> <p></p> <p></p>
post273-comment1-reply2: <p>b) ax2&#43;cxy&#43;dx&#43;ey&#43;f</p> <p></p> <p>I believe to find a b c d e f you have to do some trial and error on a few different inequalities but to find the max marginal classifier we can do this: </p> <p></p> <p>B * x3 &#43; B0 = 1 for class 1 points </p> <p>B * x3 &#43; B0 = -1 for class 2 points </p> <p></p> <p>where B = weight and B0 = bias </p> <p></p> <p>let&#39;s try the point [1.5, 1.5] </p> <p></p> <p>B * (2.25a &#43; 1.5c &#43; 1.5d &#43; 1.5e &#43;f) &#43; B0 = -1 </p> <p></p> <p>f = - 2.25a -1.5c - 1.5d - 1.5e - 1.5 d - 1.5e</p> <p></p> <p>B* 0 &#43; B0 = -1 </p> <p></p> <p>B0 = -1 </p> <p></p> <p>x3 = -1 is our classifier</p> <p></p> <p>c) -1 = ax2&#43;cxy&#43;dx&#43;ey&#43;f </p> <p></p> <p>once again I don&#39;t have the values so I can&#39;t solve it but </p> <p></p> <p>ax2&#43;cxy&#43;dx&#43;ey&#43;f &#43; 1 &gt; 0 is the region for class 1 </p> <p>ax2&#43;cxy&#43;dx&#43;ey&#43;f &#43; 1 &lt; 0 is the region for class 2 </p> <p></p> <p></p>
post273-comment2: Have you found a way other than trial and error to get the coefficients in B?
post273-comment2-reply1: I have not. 
post273-comment3: Where is this problem from?
post273-comment3-reply1: Can you share where is this problem from?
post273-comment4: How does it become a=b=c=1 and d=0? I think one of either a,b or c has to be -2 to satisfy the #4 equation?
post273-comment4-reply1: I solved different way. If you think the 2D space, we would have an equation like (y-1.5)^2 + (x-1.5)^2 = 0.25^2 Then it would become Maximum Margin Classifier. Solving the equations and replace y with x2 (using original notation) results, f(x1,x2) = x1^2+x2^2-3x-3y+4.375.
post273-comment4-reply2: you are right my bad it was late 
post273-comment4-reply3: Hi Seungil, thanks for the nice solution. f(x1,x2) = x1^2+x2^2-3x-3y+4.375 is the answer for (c) right? What's β0 in (b)? Thanks.
post273-comment4-reply4: Refer below!
post273-comment5: I am unable to understand how are we getting the equations, can someone please write a detailed answer for this question?Like how do I proceed with the given question? I have ZERO clue about this question.
post273-comment5-reply1:  Hopefully this will help you understand.
post273-comment5-reply2: thank you so much.I wanted to know, like, do we always look out for a circle? And use the circle equation as our answer?And where have you considered the condition X3=beta0? If you haven't considered it, how to consider it? What is the final answer?I know a lot of questions but, I am really struggling with this one.
post273-comment5-reply3: It could be cone shape or wiggly shape depends on the problem from my understanding but obviously it will be impossible to fit the model with some strange looking decision boundary during handwrittin exam. I also first thought that it would be negative cone shaped but it is not going to be maximum margin classfier. beta0, in this case, is a threshold which is 0.125 (d/2)^2
post273-comment5-reply4: beta0, would be constant in the equation and based on the equation you have beta0 would be 4.375
post273-comment5-reply5: I was bit confused that part, what does 4.375 actually mean there? If generally meaning threshold X3=b0, Wouldn't mean the maximum distance?
post274: What does the instruction 'Use an RBF kernel with gamma=1 or identify a gamma value that achieves the same cluster balance as seen in the original dataset' imply? I discovered that using gamma=1 results in poor performance. Are we permitted to continue using gamma=1 despite this? If not, what's the recommended approach for finding an optimal gamma value? Should we test a range of gamma values to find one that ensures one cluster precisely matches the count of the positive class in the true dataset, and the other matches the count of the negative class? Since this process seems computationally intensive, is there a suggested range for efficiently determining the appropriate gamma value?
post275: Should we be re-choosing 472 data points randomly as the test set for each of the 50 iterations?
post275-comment1-reply5: It is not mentioned but it&#39;s better to shuffle the data points to see how it works.
post276: I'm not sure what we are averaging... are we averaging first training increment test errors (test error 1 of 90 for rep 1 out of 50 + test error 1 of 90 for rep 2 out of 50 + test error 1 of 90 for rep 3 out of 50 + ...etc) / 50? Pasting the question below. Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot average test error versus number of training instances for both active and passive learners on the same gure and report your conclusions. Here, you are actually obtaining a learning curve by Monte-Carlo simulation.
post276-comment1-reply5: <p>I assume that we are averaging the test error on different training set size over 50 runs.</p> <p></p> <p>You will have 90 test errors for passive learning and 90 test errors for active learning, and you can see how the test error changes when more data were added to the training set. That&#39;s the learning curve. </p>
post277: Can someone please provide a solution for this problem?
post277-comment1-reply5: <img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl7447dhe8xh5ld%2Fc0bd9452026bc110ab3ffda1f3cb5430acb57ed12a94239f6eee28c9587367a7%2FScreenshot_2023-11-25_at_5.47.16_PM.png" alt="Screenshot_2023-11-25_at_5.47.16_PM.pngNaN" />
post277-comment2: @935 
post277-comment2-reply1: gave it a shot 
post278: Hii all, I am confused about how to implement spectral clustering. Should we use the fit-predict method on train set or the entire data set? Also, why should we use majority polling for cluster labels instead of directly using the results from fit-predict method. Thanks! 
post278-comment1-reply1: I think we should only use training set for the fit-predict method. For the reason why majority polling is needed, I think that is to simulate monte carlo simulation
post279:  Can anyone give the solution for 4. d)
post279-comment1-reply1: <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl786o538aat5ei%2F6645c99b3bd9be4d1b60d679de2bf9e3bb3cf917594d50ab29fbd85b2be2a003%2Fimage.png" alt="image.png" /></p> <p>This is my best guess, but I am not sure. Which practice midterm did this problem come from?</p>
post279-comment1-reply1: <img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Fl786o538aat5ei%2F6645c99b3bd9be4d1b60d679de2bf9e3bb3cf917594d50ab29fbd85b2be2a003%2Fimage.png" alt="image.png" />This is my best guess, but I am not sure. Which practice midterm did this problem come from?
post279-comment2: Sample Midterm 1-DSCI 552
post279-comment2-reply1: Oh ok thanks!
post280: The slides say the second approach is "Start from the weights in the first run, train with both training data and validation data until the training loss < validation loss at the early stopping point". If we're using the same weights, wont the training loss ALWAYS be less than the validation loss since we're using more data with the same weights? So the loss will always improve?
post280-comment1-reply1: <p>I think the PPT says &#34;reusing validation set&#34; correct? I.E., you use not only training set but also the validation set to update the new weights - so the loss on your validation set could be larger</p> <p></p> <p>P.S., for early stopping, this is just one approach - we can also use other approaches (e.g., stops when the current validation loss becomes larger than the previous one)</p>
post281: Hello, I was going through the lectures to prepare for the exam and I saw this calculation for the Perceptron Rule and I got confused about this multiplication, when I calculated it, I got .6 but I don't know where the negative comes from? Any help would be appreciated! 
post281-comment1-reply1: it is 1*1 &#43; (-0.8*2) = 1 &#43;(-1.6) = 1 - 1.6 = -0.6
post281-comment1-reply1: it is 1*1 &#43; (-08*2) = 1 &#43;(-1.6) = 1 - 1.6 = -0.6
post282: After comparing the labels provided by k-means with the true labels of the training data, we get the predicted class of each sample. However, sklearn's roc_curve requires the y_score, usually probability estimates of the positive class, as a parameter. How are we supposed to get these probabilities from the predicted class labels?
post282-comment1-reply1: <p>Based on the footnotes </p> <p></p> <p>K-means algorithm does not provide probabilities, so one can use the distances from cluster center and<br />pass them through a softmax to calculate probabilities. Alternatively, one can calculate the ROC curve by<br />varying the threshold for majority polling. Usually, a majority is achieved when t = 50% of the data are in<br />a class. one can vary t and obtain an ROC curve</p>
post283: When can we expect HW6 grades?
post283-comment1-reply1: I think some students have already received their grades but grades have typically been released two weeks after the due date, so potentially Monday. Although the grad student strike that is planned might complicate things.
post284: I have got the decision tree as attached in the SS. The solution however has a different tree structure. Could someone please confirm regarding what the right solution is? 
post284-comment1-reply1: I think your solution is correct. Iam getting the same tree!!
post284-comment2: Great! Thanks for confirming 
post285:  Hi Class, The first midterm of DSCI 552 will be held on Friday, December 1, 8:00 AM PDT. The location of Midterm 1 is: On Campus Students in 12-1:50 Section: THH 101DEN Students: OnlineOn Campus Students in 3:30-5:20 Section: THH 201All Students with OSAS extra time accommodations who don't want to take their exam in the test center: VPD 106 Please bring your laptops and cellphones with a Scanner App (Such as CamScanner) installed on the phone to the exam room. You will upload a scanned version of your exam on D2L after the exam time is up. Also, please bring your USC picture ID to the exam room and be ready to show it to the proctors. If you have OSAS accommodations and have not posted your letter on Piazza yet, please make a private post to ALL INSTRUCTORS and use OSAS tag ASAP. If you have done so before, you do not need to do anything. For DEN students who are not in Los Angeles area, the exam will be posted in the Assignment Section of D2L/DEN. You will be proctored on DEN and you should make every effort to be on DEN 15 minutes before your exam starts. You should have your USC ID or any official picture ID ready and show it to your proctor prior to the start of the exam. Please do not use headphones (earplugs to avoid noise are OK) during the exam and be ready to unmute if your proctor asks. DEN students who take the exam online will write their answers on sheets of paper and scan them and upload a PDF on DEN/D2L, or will just upload the PDF of the answers they wrote using their tablet. They will be given 15 minutes after the exam to upload the exam on the assignment section of DEN/D2L. Using electronic devices for anything other than exam calculations or writing the solutions of the exam or connecting to DEN/D2L is prohibited. On-Campus Students and DEN students who take the exam on campus will write their solutions on paper and after they are done, they scan the solutions and upload them on D2L. Please stop writing at the end of the exam, otherwise you run the risk of not being able to submit your solutions. (DO NOT take that risk). Also, please refrain from congregating after the exam in the exam room. If you have done scanning, you must leave the room very quietly. Do not speak to anyone except the course staff before leaving the room. Speaking to others in the exam room while some students are still taking the exam or uploading them is a serious offense and may warrant disciplinary action. The exam is closed-book and notes. You can bring TWO letter-sized cheat sheets (back and front) to the exam. It can be handwritten or typed, but it cannot be electronic. You should use a hard copy. Calculators ARE ALLOWED (and probably needed) in the exam. A simple scientific calculator (~$10-$20) suffices. Graphical calculators are not allowed. The exam will be on the material presented in Lessons 1-11, with heavy emphasis on what was not covered in midterm 1. Appendices that WERE TAUGHT IN CLASS are fair game for the exam. I have posted two sample exams for your practice. Please note that you should not make any assumption about the theme and the level of the questions in your midterm based on the sample exams. You can expect around 6 questions in the midterm, and you may expect any type of question. The exam will last for around 100 minutes. Please arrive early, because the time of the exam cannot be extended and we must leave the room 10 minutes before the next lecture starts. Please make sure that you get enough sleep the night before the exam. Also, you need to practice as many sample problems (from your textbook and the posted sample exam) as possible. Reading the solutions is NOT ENOUGH. You have to actually sit down and work out your own solutions. A lot of people have problems with time management in exams. That can be remedied with working on a lot of sample problems. Good Luck, M. R. Rajati 
post286: If I want to skip HW8, should I still upload the GitHub repository to notify TA?
post286-comment1-reply1: <p>I think if you don&#39;t submit anything you&#39;ll just get a 0 which they will drop once they calculate grades</p> <p></p> <p>given this would be your lowest grade</p>
post286-comment1-reply1: I think if you don&#39;t submit anything you&#39;ll just get a 0 which they will drop once they calculate grades 
post287: In question 1) b) iv) for spectral clustering, how do we plot ROC curve? Since, Spectral clustering does not return any cluster centers, how to plot ROC curve?
post287-comment1-reply1: <p>it is mentioned in the note #3:</p> <blockquote> <p>3. Because Spectral Clustering will not give you cluster centers, instead of considering 30 closest data points<br />to the center, consider labeling based on either 30 randomly selected data points or the entire points in each cluster. Also, for ROC curves, you can vary the threshold of majority polling to obtain an ROC.</p> </blockquote>
post288: Which lessons will possibly be covered in the final? Lessons 6 - 10?
post288-comment1-reply1: Hidden Markov models will be the last lesson.
post288-comment1-reply1: Hidden Markov models will be last lesson.
post288-comment2: So lessons 6-11?
post288-comment2-reply1: refer @891 
post289: In the svm lecture, the professor mentioned cross validating for all kernel hyperparameters: (gamma,c) ,(d,c). I'm confused about the term d. What does the hyperparameter 'd' indicate?
post289-comment1-reply1: <p>It adds non-linearity to the model. Kernel functions are substitutes for adding more nonlinear params ($X_{1} ^ 2, X_{2} ^ 2, X_{1}X_{2}$ etc). Using d=2 is a way to add quadratic nonlinearity, using d=3 is a way to add cubic non-linearity etc. Hope this makes sense! </p> <p></p>
post289-comment2: Makes sense. So this d is specific to a certain class of Kernels, right? Can this hyperparameter be tuned for a Radial Kernel? We have a certain definition of K(x_i,x_i'). Or can the exponential term be raised to a further d hyperparameter?
post289-comment2-reply1: The radial kernel has a different function that does not use d, it uses gamma, they’re completely separate iiuc. 
post289-comment2-reply2: Cool! Thanks 
post290: Can someone solve @892? Thank you.
post291: I got this hierarchy by complete linkage. I am wondering what the output of the two clusters. 
post292:  Hi, Can someone clarify what is the way to get the possible state transitions? Also, how the four transitions marked as not equal to zero? 
post292-comment1-reply2: <p>For your first question, you are already given pi=[1 0 0] and that is a hint to reach these state transitions.</p> <p></p> <p>For the second question, follow the HMM algo to see , also prof already provided a hint that only 4 will be valid</p>
post292-comment2: First part: Row 1 column 1 D -> D 0.2, Row 1 Column 2 D -> E 0.8, Row 2 Column 2 E -> E 0.8, Row 2 Column 1 F -> D 2nd part example: DDEE - P(D) = 1 * P(alpha | D) = 0.8 * P(D | D) = 0.2 * P(beta | D) = 0.2 * P(E | D) = 0.8 * P(gamma | E) = 0.8 * P(E | E) = 0.8 * P(alpha | E) = 0 we have arrived at our first 0 probability , we are looking for 4 hidden states that occur simultaneously with 4 observations alpha beta gamma alpha that have nonzero probabilities, this is an example of a zero probability state that we would exclude from the answer 
post292-comment3: Why is DDEE not an correct answer?
post292-comment3-reply1: look above! P(E | E) = 0 so the whole probability goes to 0 (this is a mistake) 
post292-comment3-reply2: how the P(E|E) = 0, not 0.8? 
post292-comment3-reply3: nice catch, P(alpha | E) the final probability is 0
post292-comment3-reply4: just updated the above solution 
post292-comment3-reply5: Now I got this, thanks!
post293: I tried the methods found on StackOverflow to mute sklearn's convergence warning, but they didn't work. Has anyone found a way to do this? Thanks!
post293-comment1-reply5: <p>If you haven&#39;t tried this,</p> <p></p> <p>try:</p> <p>import warnings</p> <p>from sklearn.exceptions import ConvergenceWarning</p> <p></p> <p>warnings.filterwarnings(&#34;ignore&#34;, category=ConvergenceWarning)</p> <p>or</p> <p>warnings.filterwarnings(&#34;ignore&#34;) # ignore all</p>
post293-comment2: Hi, thanks for the answer. I used the code at the beginning of the notebook, but still got the warning in the later cells. I also tried putting the code at the beginning of the cell that gives the warning, but the warnings are still there...
post293-comment2-reply1: I too, am not able to suppress the warnings, not sure 
post293-comment2-reply2: Hello! I tried this and it worked! import warnings warnings.filterwarnings("ignore") import os os.environ['PYTHONWARNINGS'] = 'ignore' 
post293-comment2-reply3: Thanks for sharing. I tried this one, and it worked. Not sure how good this approach is, but did the work. import os import sys sys.stderr = open(os.devnull, "w") # silence stderr
post294: I assume we just need to fit the SVM using the updated labeled data and don't need to select parameters using CV again, right? 
post294-comment1: Agreed. Clarification here would be helpful
post294-comment2-reply3: What?
post294-comment3: thats what I assumed 
post295: I get why we calculate the inner product between all possible points, but why do we have alpha? Sure setting alpha to 0 means we don't care about points that aren't support vectors, but how do we set / why do we need alpha for the remaining points (the support vectors)? Can't we just get f(x) by taking the dot product of all support vectors without applying the alphas? 
post295-comment1: I've been thinking about this more, and could it be that instead of Betas that we would calculate if we actually added nonlinearity, alphas are a way to weigh some support vector points more heavily than others? Which would be weighing some "features" more heavily than others (if we look at what we used to do with betas)? I still find this a little hard to understand, because a high beta means the feature is super important, but why would any one point be more important than the other? So we weigh some points that have high noise and can be classified incorrectly lower than other points?
post296: Can somebody explain how we have calculated the weight and bias matrix in question? 
post296-comment1-reply3: We start with initial weight and bias of 0. When we get to a step in the perceptron algorithm that cycles through features x1,x2, x3, x4, where e does not equal 0 we update thew weight and the bias by applying the step Wnew = Wold &#43; alpha * error * current feature.
post296-comment2: I still don't get why for the professor's first iteration, f(0)=1. How did he decide that f(0)=1? 
post296-comment2-reply1: f(negative #) = -1, f(0 or positive #) = 1 
post296-comment2-reply2: Thank you so much for helping out. But from the slide, this is what the definition is like. From the definition, f(>0) should be classified as 1 and f(<=0) should be classified as -1. Is there something wrong with this definition? Or is there anything that I'm missing? 
post296-comment2-reply3: I think it's just because the data is given is classified as 1 and -1 
post296-comment3: I realized I answered the Midterm 2 Q1 instead of the sample midterm 2. When trying to answer your question, I carried out multiple handwritten iterations and then coded this: import numpy as np # Given dataX = np.array([[1, 1], [1, 2], [2, -1], [2, 0], [-1, 2], [2, -1], [-1, -1], [-2, -2]])targets = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1], [-1, 1], [1, -1], [-1, -1], [1, 1]]) # Given weights and biasesW = np.array([[0, 0], [0, 0]], dtype=np.float64)b = np.array([0, 0], dtype=np.float64) # Learning ratelearning_rate = 0.5 # Activation function (Step function)def activation(x): return np.where(x >= 0, 1, -1) # Training the perceptron for n iterationsfor iteration in range(1000): for i in range(len(X)): # Forward pass net = np.dot(X[i], W.T) + b y = activation(net) # Update weights and biases delta_w = learning_rate * np.outer(targets[i] - y, X[i]) delta_b = learning_rate * (targets[i] - y) W = W + delta_w.astype(np.float64) b = b + delta_b.astype(np.float64) # Print weights and biases after each iteration print(f"Iteration {iteration + 1} - W: {W}, b: {b}") # Print the final weights and biasesprint("\nFinal Weights (W):")print(W)print("\nFinal Biases (b):")print(b) Final Weights (W): [[ 2. -3.] [-2. 1.]] Final Biases (b): [-2. 3.] So carrying out the standard perceptron algorithim with W = [ [0 0] [0 0] ] and b = [0 0] does not converge to values provided
post296-comment3-reply1: but the hardest part for me was understanding that a 2 x 1 vector achieved from subtracting f(wTx + b) from our model vector t is our e term and it is multiplied by a 1 x 2 vector xT to create a change in w term that is 2x2 . Still, I was unable to converge to the values provided even after 100 coded iterations: (
post297: Do we need to show some output in b ii A or just the final output of ROC, CM, and averages in b ii B?
post297-comment1-reply1: b ii) A seems more like an instruction on how to train the SVM. Should not generate a result I believe. The metrics are generated in part B) which you&#39;d use to draw conclusions
post298: Can someone help me with my code below? It didn't even finish 1 iteration in 12 hrs. From my understanding this code looks what the problem states. (except 2 iteration for checking if it is working) Thank you.
post298-comment1-reply1: <p>i&#39;m not sure if your X_train is a dataframe or not</p> <p>but if it is, you might need .iloc for every slicing on X_train here since it might not match up with the df index</p>
post298-comment1-reply1: edit
post298-comment1-reply1: <p>i think you might be wrong here </p> <p></p> <pre> pool_indices.update(new_indices) remaining_indices.difference_update(new_indices)</pre> <p>these two lines should not be executed in only the first iteration, where len(pool_indice) &lt; 10)</p> <p>because your pool_indice will never be updated after that and its length will stay 10 forever (then you will stuck in the outer while loop)</p>
post298-comment1-reply1: <p>i think you might be wrong here </p> <p></p> <pre> pool_indices.update(new_indices) remaining_indices.difference_update(new_indices)</pre> <p>these two lines should not be executed in only first iteration, where len(pool_indice) &lt; 10)</p> <p>because your pool_indice will never be updated after that and its length stays as 10</p>
post299: Do we use normalized data for unsupervised learning or simply use the dataset given?
post299-comment1-reply1: <p>It depends on the algorithm.</p> <p>For SVM, It is recommended to normalize (or standardize) data because it is sensitive to the feature scales</p>
post299-comment2: Yeah. I understand. But unsupervised is kmeans right. So, I wanted to know for that algorithm should we use normalized dataset
post299-comment2-reply1: Since the algorithm is based on distance computation, features with higher magnitudes can dominate the distance measure. so yes
post300:  Can someone clarify the following point, why in here we only update the bias term? 
post300-comment1-reply1: Yes so at that point the weight has become w = [ -2 0 ] from the previous step with data point x4 = [2 2]. On this step with x1 = [0 0]^t, f([-2 0] * [0 0]^t - 1) = f(-1). We know that f( # less than 0) will be classified as -1, so f(-1) = -1. e = 1 - f(-1) = 1 --1 = 1 &#43; 1 = 2. Since our error is not 0, we will have to update out weight and our bias. When we try to update our weight, we do [-2 0] &#43; 0.5*2*[ 0 0]^t. Since we are multiplying alpha and e by a zero vector we know that there is no change in the weight. There is a change in the bias however. 
post301: I just want to clarify, we attain 90 errors for each SVM, and then do we average those 90 for each of the 50 iterations? I'm pretty sure we wouldn't be asked to output 4500 errors, but I just want to clarify I'm understanding correctly because the question doesn't mention averaging the errors.
post301-comment1-reply1: Actually I just read in part c, would that be where we average the errors?
post301-comment2: I think there should only be 90 test errors total? 1 for each SVM?
post302: Does this mean to use randomly selected train and test data in every one of the 30 iterations of Monte Carlo?
post302-comment1-reply1: @869 and @882. Which means yes.
post303: What does it mean when the professor says that PCA 1 is not good? How is the differentiation between the classes determined by this line?
post303-comment1-reply1: <p>There’s something wrong with your image. $$PCA_1$$ should be the longer one, and $$PCA_2$$ is the shorter one.</p> <p></p> <p>The reason $$PCA_1$$ is not good is that, when data points are <strong>projected</strong> onto it, the classes are not well separated.</p> <p></p>
post303-comment1-reply1: <div> <div>There&#39;s something wrong with your image. $$PCA_1$$ should be the longer one, and $$PCA_2$$ is the shorter one.</div> <br /> <div>The reason $$PCA_1$$ is not good is that, when data points are projected onto it, the classes are not well separated.</div> </div>
post304: Hi, I am confused how to split the data when we get the splitting points 1.5, 2.5 and 3.5. Could someone let me know how to divide the R for each splitting point in each example in detail? a) {(0, 3), (1, 6), (2, 5), (3, 1), (4, 0), (5, 2)} b) {(0, green), (1, green), (2, blue), (3, green), (4, red), (5, blue)} Thank you in advance! 
post304-comment1-reply1: I have same question, if you know the answer please share.
post304-comment2: I have not gotten yet. Please someone let me know if you know the solution
post304-comment3: a) {(0, 3), (1, 6), (2, 5), (3, 1), (4, 0), (5, 2)}How many classes are here?Like from b) I can get there are one feature and three classes, right?
post304-comment3-reply1: For example, one feature and one continuous output so it's (xi, yi) for the (a)
post304-comment3-reply2: For b) I followed the solution, which is provided on the sample paper.But i don't know how to do the first one
post304-comment4: Sorry its a little late but For (b) We divide based on 1.5,2.5,3.5 as the problem states given that datapoints we have points 0,1,2,3,4,5 labeled accordingly to the question then we will separate region where we will calculate the Gini Index Ex: 1.5 left of 1.5 -> only green class therefore we will end up with 0 right of 1.5 -> then we will get p(g)(1-p(g)) + p(r)(1-p(r)) + p(b)(1-p(b)) For (a) I assume the split would be a straight line down seperating the regions Like x = 1.5 and then you would basically do the same thing with the Gini Index Since there is no solution for (a) I won't give a definite answer At least thats how I thought about it my answers seem to match with the solution for (b) so it should be fine?
post305:  Hi, For midterm 2 materials, are all lessons covered or the ones after midterm1? Thanks,
post305-comment1-reply2: It will be focused on material after midterm 1, but ofc everything is connected
post305-comment2: Professor said "In the 3:30-5:20 lecture, I will present Reinforcement Learning, but I WILL NOT HOLD YOU RESPONSIBLE for it in the midterm." So guess the reinforcement will not be on midterm2.
post306: i'd like to have access to the lectures after our course comes to a close to review/leverage them down the line, while i'm working on various ML projects. will we still have access to lectures next semester - and if not - any advice on best way to capture some of the content shared in class (aside from the slides)?
post306-comment1-reply2: It is against policy to keep the lecture up, slides and taking notes might just be ur best options here
post306-comment2: sounds good, thank you. 
post306-comment3: may we know when will be the last date of lectures being available? because I was planning to go through few of the lectures in winter break
post307: Can someone explain why the first Gini value in detail? I am not sure about how to divide the data when Th is 1.5, 2.5 and 3.5. Also I want to know thow the denominator is 4 in the first Gini value. It would be appreciated if someone explain how to solve this problem in detail. 
post307-comment1-reply2: <p>For example when Th is 1.5, we split the data in 2 leaves, {0g, 1g} and {2b, 3g, 4r, 5b}</p> <p>Gini=2/2*0(the left leaf has only one class: green)&#43;2/4(1-2/4)(the right leaf has 2 blue)&#43;1/4(3/4)(one red)&#43;1/4(3/4)(and one green)</p>
post307-comment2: Thank you! I understand your explanation. I have follow up question. Then what about the second split? Where should I set the standard for the second split? Could you explain like you did before?
post307-comment2-reply1: Please share if you knew how the second split calculated
post307-comment3: ok for the second split: 2.5: we use the existing split at 1.5 - there are only 2 greens in the less than 1.5 area so that's a pure node with gini index 0, between 1.5 and 2.5 is 1 blue so another pure node with gini index 0, greater than 2.5 we have 1 g 1 r 1 b so each of them will have value (1/3) * ( 1 - 1/3) hence the 3 * (1/3) * (2/3). 3.5: we use the existing split at 1.5 - there are only 2 greens in the less than 1.5 area so that's a pure node with gini index 0, between 1.5 and 3.5 is 1 blue and 1 green so (1/2) (1 - 1/2) + (1/2) (1 - 1/2), greater than 3.5 we have 1 red and 1 blue so (1/2) * (1 - 1/2) + (1/2) * (1 - 1/2) the sum of gini indexs with the second split at 2.5 is 2/3 and the sum of gini indexes with the second split at 3.5 is 1 so we will choose the smaller one
post307-comment3-reply1: A small correction, "greater than 2.5 we have 1 red and 1 blue", here it should be 3.5
post307-comment4: If you were wondering how the tree is created at the end we do the first split at 1.5 then we split at 2.5 less than 2.5 has majority green, greater than 2.5 is a tie between r g b but since we already used green it can't be that and then we have to break a tie between red and blue and blue has less value so we choose the smaller one
post307-comment4-reply1: Thanks!
post307-comment4-reply2: can you explain what if we use the weight gini index?What is the difference between weighted gini index and gini index?
post307-comment4-reply3: How blue has less value?? I am not getting thisAnd, why are keeping x<2.5 left node as green, shouldn't it be blue?Can u briefly explain..
post308: Hi, I am trying to solve this question in the sample exam, but I am confused about the Recluster part. Based on my knowledge, each data should be reclustered based on the nearest cluster for the KNN. So I think x1 should be classified as cluster 1 because root(2) is bigger than root(4/5), but it is classified as cluster 2. Other data, except x1, are classified as the nearest cluster after calculating the distance. Could someone explain this if I am wrong? Thank you. 
post308-comment1-reply3: Yeah its a mistake, x1 should be assigned to cluster2, and x4 to cluster1. It is just the closest one
post308-comment2: So, what is the final answer?Is it X2 and X4 in cluster 1 & X1 and X3 in cluster 2?
post308-comment2-reply1: additionally, can you also explain the below dendogram?I believe the X4 and X3 should be switched here:
post308-comment2-reply2: So yea I think you may be right. This tree occurs from performing complete linkage instead of single linkage. If we perform single linkage then x3 and x4 would be switched
post308-comment2-reply3: Which formula did you use to calculate dissimilarity? I was able to get the dendrogram visually, not analytically.
post308-comment2-reply4: There's a similar problem in ISLR Ch 12 Q 2, except they give you the distance matrix.
post308-comment3-reply4: <p>If we correctly assign x2, x4 to cluster 1 and x1, x3 to cluster 2, then our new centroids should be the following:</p> <p>Cluster 1 centroid (1, 3/2)</p> <p>Cluster 2 centroid (1, 0)</p>
post309:  Should't it be P(D)P(alpha | D) P(E |D) P(beta | E) P(E|E) P(gamma|E) P(F |E) P(alpha |F) = 1(0.8)(0.8)(0.6)(0.8)(0.4)(0.2)(0.2). This way works for the three other calculations except for this one so I'm not sure what I'm missing here
post309-comment1-reply4: .
post309-comment1-reply4: I think you have the extra D at the start
post309-comment2: The start value comes from pi = [1 00] so p(d) = 1. Then it goes P(alpha|D) = 0.8 , P(E|D) = 0.8. P(beta | E) = 0.6. The only values that are 0.2 in the observations are P(beta | D) or P(alpha | F). The solution also fails to include the 0.4 from P(gamma | E) which should be after P(E|E) = 0.8 and before P(F | E) = 0.2. My logic is correct by the 3 other calculations in the problem so I could really use some help here
post309-comment3-reply4: This is miscalculated, I asked professor after class. The solution is above ^
post309-comment4: Can someone explain why FDEF is not a valid state in addition to the 4 (DDEF, DEEF, DEFD, DEFF) from the solution?
post309-comment4-reply1: because pi = [ 1 0 0 ] we know that the start state must be D
post310: I'm thinking I have some mistake in my code. Or it is just what it is? 
post310-comment1-reply1: You might have assigned a fixed random_state in train_test_split or while defining the KMeans model. Try removing the random_state parameter altogether or if you want consistent results, fixing the random state according to your iteration number.
post310-comment2: I have done as per what you've mentioned already. Using the random seeds. This worked fine until K Means method. 
post310-comment2-reply1: Perform a new train_test_split for each iteration. Because that will introduce significant randomness in your cluster formation. Random_state in KMeans, which is what you did, will help in randomizing cluster initialization. But as Kmeans already does that by default with n_init=10, it won’t make any significant difference. So move your train_test_split inside the for loop.
post310-comment2-reply2: It worked wonderfully. Thank you. 
post311: Does anyone have any idea how to plot the ROC curve for spectral clustering? Would love some guidance on how to do this because I'm not sure if there is a good way to find a continuous score or probability for each instance belonging to a cluster
post311-comment1-reply2: You can take a look at this article, https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py
post311-comment2: I'm am clear about the confusion matrix but I'm still unsure how to do the ROC curve for spectral clustering...Correct me if I am mistaken but I believe the article only mentions confusion matrix.
post312: Are we supposed to re-split the data to get new train and test sets with each iteration of Monte Carlo? Or just split it once and then run all iterations of Monte Carlo on the same train and test set?
post312-comment1-reply2: You need to re-split it for each iteration
post312-comment2: just to clarify, we use 20% positive+20% negative (total 40% for test set) or 10% positive+10% negative? Thanks,
post313: Has anyone encountered long run-time issue with 2(c) homework 7. I am wondering if there is anyway to improve. 
post313-comment1-reply2: Yeah mine took about 1 minute per iteration. You can try playing around with the model parameters but i couldn&#39;t find much improvement. You could also try running multiple iterations in parallel.
post314: 
post314-comment1-reply2: No, they are closely related but slightly different formulas.
post315: 1.(b) 'semi-supervised learning M = 30 times' What does M mean here? 
post315-comment1-reply2: <p>number of iterations for monte carlo</p> <p>we need to perform those actions 30 times and find average of the result</p>
post315-comment1-reply2: <p>iteration amount of monte carlo</p> <p>we need to perform those actions 30 times and find average of the result</p>
post315-comment2: Does it mean we have to do M iterations for cross valiadtion on Training models to find best C value and test models with M iterations with the model or just test the model with M iterantions?
post315-comment2-reply1: I am also confused about this, does anyone know?
post315-comment2-reply2: Train and test M times because we should randomly split train and test M times as well to get varied data
post316:  For the second approach, the stop condition is training loss < validation loss at early stopping point or training loss < training loss at early stopping point? Since to my knowledge, the training loss should be less than validation loss, this stopping condition on the slide makes me confused.
post316-comment1-reply2: Validation loss. Below that is overfit.
post316-comment2: ic. Thanks
post316-comment3: Note: Training loss generally decreases continuously (or stays stable at some value), since your model becomes "more and more fit" to the training dataset as training processes
post317: Hi, I couldn't locate the banknote authentication Data in HW8 Q2 in the data folder, and the link is not accessible to download the data. Need some clarification for this. Thanks,
post317-comment1-reply2: <p><a href="https://archive.ics.uci.edu/dataset/267/banknote&#43;authentication">banknote authentication - UCI Machine Learning Repository</a></p> <p></p> <p>Copy the whole URL to web brower.</p>
post317-comment2-reply2: I also added the dataset to the folder.
post318: Not sure if I got the definitions for these three correct: Hamming score: (total correct individual labels) / (total predicted labels) vs Hamming loss: (total wrong individual labels) / (total predicted labels) vs Hamming distance: (total wrong overall labels) / (total predicted labels) [if all three labels are not correct] ex) [a,b,c] vs [a,b,d] Would hamming score=2/3 , hamming loss = 1/3 , hamming distance = 1/1 be correct? Thanks
post318-comment1-reply2: Yes, the numerical values are correct -but hamming distance should be <strong>1</strong> - the number of positions in which label are different - but not 1/1 (you should not divide anything)
post319: I encountered an issue in section 2(c) and was wondering about the correct approach for calculating the average Hamming distance, score, and loss in each of the 50 iterations. The first approach I considered was to calculate the Hamming distance, score, and loss for each cluster, multiply this result by the number of data points inside the cluster, sum the results for all the clusters, and then divide the result by the total number of data points in the whole dataset. Alternatively, in the second approach, I calculated the Hamming distance, score, and loss for each cluster and then divided this sum by the number of clusters.
post319-comment1-reply2: Solved! 
post320: Do we need to print out the majority labels for each cluster, where we determine the majority family for each cluster, does this imply we will end up with k*50 majority label outputs? In other words, are we expected to repeat this process 50 times, and for each cluster across these simulations, find these majority labels and print out?
post320-comment1-reply2: It does not say you need to make a report for this step so I do not think it requires anything to be printed. If the code works will show up in 2.c. anyways.
post321: I am employing the following range to implement cross-validation slicing, with specified extreme values: lambda_values = [1e-3, 1e1] and sigma_values = [0.1, 2]. This decision is based on my observation that values outside these ranges do not yield a 70% in accuracy. Is this approach permissible? Additionally, I have encountered an issue where both lambda and sigma display the same value for all three labels. Has anyone else experienced this problem
post321-comment1-reply2: <p>I would like to say yes to modify the range, since the example in pdf says &#34;if one found that the accuracy of a support vector machine will not be below 70%&#34;.</p> <p>However, my best C has had 100 and 10 come up, so choose your range carefully.</p>
post321-comment1-reply2: <p>I would like to say yes to modify the range, since the example in pdf says &#34;if one found that the accuracy of a support vector machine will not be below 70%&#34;.</p> <p>However, my best C has had 100 and 10 come up, so choose your range carefully.</p>
post321-comment1-reply2: <p>I would like to say yes to modify the range, since the example in pdf says &#34;if one found that the accuracy of a support vector machine will not be below 70%&#34;.</p> <p>However, my best C has had 100 and 10 come up, so choose your range carefully.</p>
post321-comment1-reply2: I would like to say yes, since the example in pdf says &#34;if one found that the accuracy of a support vector machine will not be below 70%&#34;
post322: Hi! I just wanted to confirm that Exam 2 is still set for 8:00 am - 9:50 am on December 1st. Thanks!
post322-comment1-reply2: @229 applies to both exams
post323: Dear Instructors, I apologize for the interruption. I am writing to inquire if there is a way to check the number of late days I still have available for my assignments. Thank you for your assistance. 
post323-comment1-reply2: @385
post323-comment2: You can always look at the dates you submitted past assignments on Github and compare them to the due dates on the syllabus to figure out the number you've used so far.
post324: I calculated 0.7 for hamming loss and 0.0 for the exact match ratio but those don't seem right... I have inserted how I am calculating the scores below since they are multilabel/multiclass outputs so the built-in accuracy scores / hamming loss functions don't work. i standardized the features also hamming_loss = round(np.sum(y_true != y_pred / y_true.size, 4)exact_match_ratio = np.mean(np.all(y_true == y_pred, axis=1)) 
post324-comment1-reply2: This sounds normal, do realize that hamming losee and hamming distance are different things
post325: Is anyone else getting 100% accuracy for all classes? Here is mine for Species: It just seems very weird to have perfect accuracy... Or is this expected?
post325-comment1: My case never had 100% accuracies. The highest was 99%. Maybe your case has perfect match with C and Gamma values?
post325-comment1-reply1: When I do L1 penalty it gave me about 98%. Not sure why...
post325-comment2: Have you tried using stratified K-fold cross-validation?
post325-comment2-reply1: Yeah, I used stratified and got 100%
post325-comment3-reply1: no free lunch, maybe you are just built diff, or maybe you had leakage
post326: 
post326-comment1-reply1: PyTorch is tool to build Neural Networks and in hw7 we are asked to create SVC.
post327: The steps in the slides are: 1. Use binary splitting to create a large tree 2. Apply cost complexity pruning using different alphas 3. Use K-fold cross validation to choose Alpha 3.1 Repeat steps 1 and 2 on k-1 folds 3.2 Evaluate mean squared prediction error using alpha 4. Choose the subtree corresponding to the best alpha from the set of subtrees found in 2. I'm SUPER confused why we need step 3.1. We already have our subtrees using different alphas using step 2. I understand why we would want to train them k times and average out k errors, but I'm not sure why we would want to create a large tree from scratch and then do pruning again... Any help would be appreciated, thank you! 
post327-comment1-reply1: In Step 2, we are using the entire training set to create our pruned subtrees. When performing cross-validation, we only want to create a pruned subtree based on the k-1 folds and evaluate each subtree based on the left-out fold. If we do not create new subtrees in step 3.1, we are implicitly exposing our cross-validation process to the entire training set, which defeats the point. 
post327-comment1-reply1: In Step 2, we are using the entire training set to create our pruned subtrees. When performing cross-validation, we only want to create a pruned subtree based on the k-1 folds and evaluate each subtree based on the left-out fold. If we do not create new subtrees in step 3.1, we are implicitly exposing our cross-validation process to the entire training set, which defeats the point. 
post327-comment2: Thanks for the reply! Would it make sense to say that the above implementation, and this are equivalent?1. Use k-fold cross validation to choose the best Alpha For each alpha 1.1 Create an entire large tree using k-1 folds 1.2 Evaluate the mean squared error and average it out 2. Create a big tree using the entire training set 3. Prune the big tree using our best alpha Thanks! 
post327-comment2-reply1: 1.1 Create an entire large tree using k-1 folds 1.2 Prune that tree with the selected alpha** 1.3 Evaluate the mean squared error and average it out for all k folds
post327-comment2-reply2: Looks correct to me. Leads me to ask why they decide to create a decision tree based on the entire training set for each alpha when it’s only necessary for the best alpha. 
post327-comment2-reply3: Right, my question exactly! Maybe an instructor can answer :) 
post327-comment2-reply4: One thing I can think of is if we have a test set and we want to evaluate how we did with cross-validation in terms of model selection. It’s possible that the model we selected with cross-validation is not the best model in terms of test error. 
post327-comment2-reply5: Right, that makes sense, but then we'd have to do something like min(test error + cv_avg_error) 
post327-comment2-reply6: I was thinking more along the lines of post model selection analysis on our approach. Like maybe we might want to change the number of folds we use for next time.
post328:  Concerning the three items I have circled above: 1.) Why is x1 considered to be in cluster 1 in this case? should it not be cluster 2 as the distance from x1 to centroid 2 is less than that of its distance to centroid 1? 2.) I believe the distance between x3 and centroid 2 should be sqrt(5/4). I wanted to verify that the circled part read sqrt(5)/2 which is equal the distance I calculated of sqrt(5/4). 3.) The distance I calculated between x4 and centroid 2 is sqrt(13/4). This would change the classification of x4 to cluster 1 instead of cluster 2 as shown above as the distance to cluster 1 is shorter. Maybe I am doing the questions incorrectly but I cannot find the problem in my work despite using the solutions as a guide. Any help would be appreciated. 
post328-comment1-reply6: <p>I have the same question, I think the solution is wrong because you should put points into the closet centroid cluster. for point X1 it is closer to centroid 2 so it should belong to cluster 2 instead the solution shows it belongs to cluster 1. can a TA please let us know if the solution is right?</p> <p></p> <p></p>
post328-comment2: can someone else let us know what you think about this question?
post328-comment3: Although the procedure in calculating is somehow wrong, but the final result for (a) is pretty much the same. After Iteration 1, the cluster will be {x2,x4},{x1,x3}. And for iter2, the centroid for two clusters are(1,1.5) and (1,0). And during iter 2, no change will be made. So the final result is {x2,x4},{x1,x3}. Correct me if I did anything wrong.
post328-comment3-reply1: I got the same final result
post328-comment4: I also think the procedure is wrong, but the final answer is correct. cluster 1 = x1, x2 / cluster 2 = x3, x4
post329: Are there multiple possible solutions for the weight vectors and biases in the vector? Unlike the answer key, I set my four class model vectors to: t1= [+1 +1] t2 = [+1 -1] t3 = [-1 +1] t4 = [-1 -1] as such made more sense to me then the key (essentially all the signs are flipped). Based on the four classes modeled using the above vectors, the weight vectors and biases that I calculated (using single layer perceptron algorithm) does not misclassify any of the given points. Thank you in advance. 
post329-comment1-reply1: What are the initial weights and bias that we should use to calculate f(x) and the error in this question?
post329-comment2: Yes as long as it does not misclassify any of the points and the 4 class model vectors are distinct you're good
post329-comment2-reply1: do you mind attaching a pic of your steps for this? Would love to confirm 
post329-comment2-reply2: yes please, it will be very helpful
post329-comment2-reply3:  
post330: Hi, I wonder will we get extra points if we do Extra Practice in HW7?
post330-comment1-reply3: no
post330-comment2-reply3: you will get extra self-esteem points knowing that you are just better
post331: do we need to include finding the optimal k in part a) as part of the monte carlo? or once we find optimal k, can we apply monte carlo to the rest of the question?part c) says to average the hamming distance, loss, and score. Do we average the distance, loss, and score from each row? and then average that hamming distance 50 times for monte carlo (ex. the value is between 0-3 for distance). OR do we calculate the distance, loss, and score across the entire dataset, and then average that 50 times? (ex. value is in the thousands)
post331-comment1: Similar question, do we: 1. Find optimal k using CH, scree plots, etc. 2. Fit the optimal kmeans model to the full dataset 3. Do a Monte Carlo with 50 simulations (essentially bootstrapping?) 4. For each of 50 simulations, compute the majority label in each cluster, and compare with the true label. 5. Compute the hamming distances for each of the 50 simulations (plus original) based on majority vs true labels 6. Take an average, standard deviation of these 50 scores, and compare to the original score 
post331-comment1-reply1: And does that mean we cannot choose a k < 10 since the label with the highest number of levels is 10 (species)?
post331-comment2-reply1: <p>I included part (a) as part of the monte carlo (I set different random state each time).</p> <p>For each row, I got hamming distance based on a new value which includes all values in family, genus, species. For example, in one row, f=A, g=B, s=C. Then I got hamming distance of this row by comparing [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;] and [majority_label_of_family, majority_label_of_genus, majority_label_of_species]. Got avg(distance in each row) in the total df as res1. Got final avg result by averaging the res1 for 50 times. That&#39;s my understanding. </p>
post331-comment3-reply1: &#34;Perform the following procedures 50 times&#34;, meaning all parts
post331-comment4: So then for part b where we determine which family is the majority for each cluster, does that mean we will have k*50 majority outputs? Without the Monte Carlo at k=22, these are the majorities for my clusters. Are we supposed to do this 50 times and for each cluster take the majority vote of majority votes? 
post331-comment4-reply1: because when we do clustering with Monte Carlos, the clusters are probably going to be different...
post332: Can we take 2 late days (without penalty) for HW 7? I have not taken any late days yet
post332-comment1: The deadline is on Monday, right? 
post332-comment1-reply1: @823
post332-comment2-reply1: @808 @288
post332-comment3: you can take up to three yes?
post332-comment3-reply1: Yes
post333: I'm confused on what the output should be for hw 7 q.b.i. 
post333-comment1-reply1: <p>Problem states &#34;for evaluating multi-classification&#34;.</p> <p>The Exact Match Score and Hamming Loss shall be calculated using the predictions from all three of these best-performing models.</p>
post333-comment2: So we aggregate the predictions of the individual labels, and then calculate the hamming loss/exact match that way?
post333-comment2-reply1: That's what I did. 
post333-comment3-reply1: @820
post334: Do you guys have the same result?
post334-comment1-reply1: <p>@782</p> <p> </p> <p>In my case, it improved the model in the case of minority classes. Also, the model was better overall!</p>
post334-comment1-reply1: <p>@782</p> <p></p> <p>In my case, it improved the model in the case of positive classes. Also, the model was better overall compared to other oversampling techniques in the HW!</p>
post334-comment1-reply1: In my case, it improved the model in the case of positive classes. Also, the model was better overall compared to other oversampling techniques in the HW!
post334-comment2: How did you applied SMOTE? I used pipeline method, which i'm not sure it does job correctly. pipeline = ImbPipeline([ ('smote', sm), ('svm', LinearSVC(penalty='l1', dual=False)) # Name your step 'svm' here ]) clf = GridSearchCV(pipeline, param_grid, cv=10, scoring=scorer) clf.fit(X_tr_scale, y_train) 
post334-comment3: I'm not there myself but i have a feeling it will help one thing but hurt another and not make that much of a difference
post334-comment3-reply1: Every time it has either helped or not hurt with my false negatives at the cost of a higher MSE
post334-comment3-reply2: .
post334-comment4: https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.htmlI used this library 
post334-comment4-reply1: Isn't using imblearn.over_sampling.SMOTE only possible with single label (binanary) classification?
post334-comment4-reply2: It works with multiclass as well
post335: Are we going to use Tensorflow or PyTorch for the last few homeworks/Project?
post335-comment1-reply2: I did not use PyTorch or Tensorflow for HW7 and looking at HW8 it appears to me that it&#39;s not necessary, but I cannot speak to the project.
post335-comment2-reply2: You most likely will not be asked to, BUT you are welcome to use it if you think it is more appropriate/want more practice
post336: Hello, I am wondering when is the HW7 due day? When would the submission link be posted? Thank you!
post336-comment1-reply2: As per the syllabus, it&#39;s due Monday, November 13th.
post337: Hi there, Wondering if there's a time limitation for running the HW7? Mine cost almost half an hour to complete, not sure if that's okay. Btw, when would we get the submission link? Thanks for your time. 
post337-comment1-reply2: <p>For me it took almost 2hrs with both classification and clustering</p> <p></p> <p>Each runs are took me roughly 5~30minutes.</p>
post337-comment1-reply2: For me it took almost 2hrs with both classification and clustering 
post337-comment2: Mine took almost one hour. I think it wouldn't be a problem.
post337-comment3-reply2: No limits, just run the notebook from head to toe before submission just like other HWs
post338: For each of the one-label classifiers we train in parts ii, iii, and iv, are we reporting the score from the cross-validation, that gives us the optimal values for the parameters? Or are we reporting the test error for each of the classifiers? Ex. are we doing: train an SVM for each of Family, Genus, Speciesreport the optimal hyperparameters, and the CV error score that resulted in that parameter OR train an SVM for each of Family, Genus, Speciesrun each of those^ on the test set, and report 3 individual test errors? I'm also confused as to where exact match/hamming loss come into play, as these are 3 one-label classifiers as opposed to a multi-label classifier.
post338-comment1-reply2: <p>I think it would not hurt to list out the optimal hyperparameters, CV result and the test result.</p> <p></p> <p>also, exact match/ hamming score can play in the role when doing cv and test because those are just evaluation metric. it does not contradict the fact that we can use exact match/hamming score.</p>
post338-comment2: ok, I ended up listing all of them But since exact match/hamming loss are useful when evaluating multi-label, instead of one-label problems. 
post338-comment3-reply2: @820
post339: Q1 asks us to report the exact match/hamming loss for the classifiers we train. However, parts ii, iii, and iv. ask us to make SVM's for each label, instead of for the multi-label problem. Here, hamming loss isn't relevant since they are each one-label problems? Does the question want us to: train one-label SVMs for each of the three labelsmake predictions on the test setcombine the predicted test setsthen calculate one exact match/hamming loss value for the combined predicted values, across all 3 classifiers?
post339-comment1-reply2: exact match/ hamming score are used after model made prediction. That means it can be used during CV or the final model evaluation, which is in your number 1 &amp; 2 
post339-comment2: I understand that, but my question is more-so about doing hamming loss on the multi-label test/predictions instead of each label individually. Since doing hamming loss on a one-label problem is simply 1-accuracy, and not really the point of hamming loss. Is any TA able to clarify here?
post339-comment2-reply1: I guess the hamming loss logic would come after the part where we use diff SVMs to predict each label - create a multi-label prediction. Now we would need to take a hamming distance between our multilabel pred and the multi label target.
post339-comment3-reply1: The metrics are for each label
post340: Are we supposed to use OneVsRestClassifier for bii and biii? For example: OneVsRestClassifier(SVC(kernel='rbf')) and OneVsRestClassifier(LinearSVC(penalty='l1', dual=False, max_iter=100000)). Or is it okay to just use SVC(kernel='rbf') and LinearSVC(penalty='l1', dual=False, max_iter=10000)? 
post340-comment1-reply1: <p>I believe there is a parameter that you could choose for one vs rest </p> <p></p> <p>its the parameter decision_funcion_shape</p> <p></p> <p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC</a></p>
post340-comment2: Is it okay to use OneVsRestClassifier thought? Can a TA confirm?
post340-comment2-reply1: I was able to do without it. for i, label in enumerate(['Family', 'Genus', 'Species']): y_train = raw_tr[label] svm = SVC(kernel='rbf') clf = GridSearchCV(svm, param_grid, cv=10, scoring=scorer) clf.fit(X_tr_raw, y_train) best_C = clf.best_params_['C'] best_gamma = clf.best_params_['gamma'] print(f"Best params for {label}: C={best_C}, gamma={best_gamma}") svm_best = SVC(kernel='rbf', C=best_C, gamma=best_gamma) svm_best.fit(X_tr_raw, y_train) y_te_pred = svm_best.predict(X_te_raw) 
post340-comment2-reply2: Great but would love to hear from a TA if it is okay to use it/whether it is suggested/necessary to use OneVsRestClassifier.
post340-comment2-reply3: The defualt for SVC in scikit is a OneVsRestClassifier
post340-comment3-reply3: <div> <div>OneVsRestClassifier is valid</div> </div>
post341: I've pasted the code down below. My problem is that the algorithm is failing to converge, even if i set the max iterations to 100,000. Can anyone provide some insight? # Define a range of values for C to be tested by 10-fold cross-validation. param_grid = { 'estimator__svc__C': [0.1, 1, 10, 100] } # Set up a pipeline with an L1-penalized SVM. pipeline = Pipeline([ ('scaler', StandardScaler()), ('svc', LinearSVC(penalty='l1', dual=False, max_iter=10000)) # Use dual=False when n_samples > n_features ]) # For 'Species' grid_search_family = GridSearchCV(OneVsRestClassifier(pipeline), param_grid, cv=10, n_jobs=-1) grid_search_family.fit(X_train, y_family_train) 
post341-comment1-reply3: The problem states to use the L1-penalized SVM (so LinearSVC in this case as it uses liblinear rather than libsvm as professor suggested in lecture). Try removing the OnveVsRestClassifier as LinearSVC is capable of handling multiple classes. I used OneVsRestClassifier(SVC(kernel=&#39;rbf&#39;)) in (b)ii and used LinearSVC in b(iii) and b(iv).This is my understanding of the problem so please correct me if I&#39;m wrong.
post341-comment1-reply3: It looks like you&#39;re trying to use a classifier on a classifier in the GridSearchCV part of your code, which doesn&#39;t make sense. The problem states to use the L1-penalized SVM (so LinearSVC in this case as it uses liblinear rather than libsvm as professor suggested in lecture). Try removing the OnveVsRestClassifier as LinearSVC is capable of handling multiple classes. This is my understanding of the problem so please correct me if I&#39;m wrong.
post341-comment1-reply3: It looks like you&#39;re trying to use a classifier on a classifier in the GridSearchCV part of your code, which doesn&#39;t make sense. The problem states to use the L1-penalized SVM (so LinearSVC in this case as it uses liblinear rather than libsvm as professor suggested in lecture). Try removing the OnveVsRestClassifier. This is my understanding of the problem so please correct me if I&#39;m wrong.
post341-comment1-reply3: <p>It looks like you&#39;re trying to use a classifier on a classifier in the GridSearchCV part of your code, which doesn&#39;t make sense. The problem states to use the L1-penalized SVM (so LinearSVC in this case as it uses liblinear rather than libsvm as professor suggested in lecture). This is my understanding of the problem so please correct me if I&#39;m wrong.</p> <p></p> <p>Edit (more info):</p> <p>- Try removing n_jobs=-1</p> <p>- if the estimator supports multi-jobs, setting <code>n_jobs=-1 or &gt;1</code> will not suppress warnings regardless of <code>n_jobs</code> in <code>GridSearchCV</code></p> <p>- ref: <a href="https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn</a></p> <p>- i also faced this issue, and this resolve on my side</p>
post341-comment1-reply3: <p>It looks like you&#39;re trying to use a classifier on a classifier in the GridSearchCV part of your code, which doesn&#39;t make sense. The problem states to use the L1-penalized SVM (so LinearSVC in this case as it uses liblinear rather than libsvm as professor suggested in lecture). This is my understanding of the problem so please correct me if I&#39;m wrong.</p> <p></p> <p>Edit (more info):</p> <p>- Try removing n_jobs=-1</p> <p>- if the estimator supports multi-jobs, setting <code>n_jobs=-1 or &gt;1</code> will not suppress warnings regardless of <code>n_jobs</code> in <code>GridSearchCV</code></p> <p>- ref: https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn</p>
post341-comment1-reply3: It looks like you&#39;re trying to use a classifier on a classifier in the GridSearchCV part of your code, which doesn&#39;t make sense. The problem states to use the L1-penalized SVM (so LinearSVC in this case as it uses liblinear rather than libsvm as professor suggested in lecture). This is my understanding of the problem so please correct me if I&#39;m wrong. 
post341-comment2: You can try removing n_jobs=-1 I also faced the same issue, and already resolved by that check out the second answer in this Stackoverflow 
post341-comment2-reply1: did you still use OneVsRestClassifier? 
post341-comment2-reply2: Although it says the data is normalized, I used StandardScaler() to re-normalize and this solved the problem for me
post342: According to Monte-Carlo simulation Perform the following procedures 50 times What is the proper way to do monte-carlo? 1) perform 2a (find optimal k) 50 times, then perform 2c (hammer score, etc.) 50 times with the optimal k 2) perform just 2c for 50 times with optimal k 3) perform 2a along with 2c in the same loop for 50 times 4) else?
post342-comment1-reply2: <p>Perform 2a to find optimal k, refit with optimal k, determine majority for family, genus, species, and then determine hamming distance/hamming score/hamming loss/std of hamming distance for each round of the monte-carlo simulation (ie 50 times in a loop).</p> <p>Make sure to save your hamming distance/hamming score/hamming loss for each round of the monte carlo and then calculate their respective averages for part 2c. This is my understanding of how to approach the problem. Please correct me if I am incorrect. Hope this helps</p>
post342-comment1-reply2: Perform 2a to find optimal k, refit with optimal k, determine majority for family, genus, species, and then determine hamming distance/hamming score/hamming loss/std of hamming distance 50 times in a loop. Make sure to save your hamming distance/hamming score/hamming loss for each round of the monte carlo and then calculate their respective averages for part 2c. This is my understanding of how to approach the problem. Please correct me if I am incorrect. Hope this helps
post342-comment2: I'm also confused by the instructions here. Are we essentially just conducting 2c 50 times?
post342-comment2-reply1: From my understanding for part 2c, you're averaging the hamming distance/hamming score/hamming loss for ALL of the rounds of the monte carlo. hence why I suggest to save all of them to a list as you're doing the monteo-carlo simulation. 
post343: Other than the reduction of false negatives, balancing / SMOTE did not change my error rate very much. I actually found some overfitting due to SMOTE. 
post343-comment1-reply1: @782_f1 
post344:  Hi, Thanks to students who show interest in becoming a CP in future offerings of DSCI 552. I would like to mention that the worst way for applying for such positions is to send me a template email with your resume attached to it. These emails are practically spam, I won't read them, and don't even have time to delete them. Meeting me in person or contacting me via other social media platforms like LinkedIn will not help either. If you want your applications to be considered, please apply on My Viterbi and don't follow up via email. Regards, M R Rajati 
post345: For SMOTE shape of train data will be different for every label, considering this we should be able to calculate exact match and hamming over train data right ?
post346: I am using the CH metric to kind the best K, but for K=1, all the data points are rightly assigned the same label for which the CH metric fails, can we then vary K 2 -> 50.
post346-comment1-reply1: from my understanding, with K=1, all the data points are properly assigned and thus there is nothing meaningful about it. You would need at least 2 clusters so varying K from 2-&gt; 51 in hopes of capturing the optimal number of clusters in our data. 
post347: Just want to confirm I can turn in my HW6 on Monday with late days and there will be no penalty correct? also grace days are for when you don't have late days correct?
post347-comment1-reply1: <p>Per the syllabus, we have 6 free late days that we can budget throughout the semester. However, a maximum of 3 late days can be used on one assignment. No late days may be used for the final project.</p> <div></div> <div>As long as you have at least 3 late days that you have not used, I think this means you can turn in homework 6 on Monday without a penalty. Make sure to leave a comment at the top of your notebook/readme indicating that you want to use your late days.</div> <div></div> <div>If you don&#39;t have late days, grace days have a 10% penalty per day for up to 3 days. Yes it is for when you don&#39;t have late days. All this can be found in the syllabus.</div>
post347-comment1-reply1: I do not think there are any grace days. Per the syllabus, we have 6 free late days that we can budget throughout the semester. However, a maximum of 3 late days can be used on one assignment. No late days may be used for the final project.<div><br /></div><div>As long as you have at least 3 late days that you have not used, I think this means you can turn in homework 6 on Monday without a penalty.</div>
post347-comment2-reply1: Yeah as long as you are not 4 days late, you will be fine, 4 days late is automatically a 0
post348: Hi all! I am a little uncertain as to what I should report when training my SVM models. Right now, I just have the models trained and the following output. Is this problem asking for more results than what I have? 
post348-comment1-reply1: I think Exact Match and Hamming should not be calculated at Label level but rather at model level, in the sense for 1 model we calculate these scores considering the predicted values for all the three labels.
post348-comment2: I thought we had to fit 3 SVM models, one for each of Family, Genus, and Species as the classification label. Am I misunderstanding what is being asked in the question?
post348-comment2-reply1: yes. but Exact match and Hamming Score are supposed to be calculated through all labels at once, or else they would both give the same results as in the screenshot
post348-comment2-reply2: So are you saying we train three different models and then combine them into one model for testing? Or do we just train one model?
post348-comment2-reply3: train three models, predict y, combine those predictions, then calculate score
post348-comment2-reply4: Can a TA confirm this approach?
post349: Hi, I tried to upload the training data set "aps_failure_training_set.csv" file which is more than 25MB. I checked previous Q&A so I tried to use command line, Git Desktop and Git Bash. But they all game me same error. I already purchased the data pack but still doesn't work. 
post349-comment1-reply4: @780
post349-comment2: It may because a hooks problem, you can try git push --no-verify -u origin master:main
post349-comment3: https://www.geeksforgeeks.org/how-to-push-anything-to-github-using-git-bash/ You can follow this article. Remember set email and username https://stackoverflow.com/questions/29685337/git-commit-author-doesnt-work-when-global-not-set 
post349-comment4: I submitted the hw without the training set. Is it okay or will I be penalized for it? Should I use my late days to try and submit again
post350: Could we attach handwritten graphs? 
post350-comment1-reply4: Yes. Just make sure to present and label everything clearly!
post351: When we're imputing missing values in our data, should we train the imputer on the training set, and use it to fill in both training and test values, or should we impute the training and test sets independently? I know this has been asked before, but I've heard multiple differing answers!
post351-comment1-reply4: <p>@722 applies. Only train on the training set, fill in both with that.</p> <p></p> <p>References:<a href="https://stackoverflow.com/questions/57212549/for-missing-value-imputation-why-not-use-the-fit-from-test-data-to-transform-te" target="_blank" rel="noopener noreferrer"></a></p> <p><a href="https://stackoverflow.com/questions/57212549/for-missing-value-imputation-why-not-use-the-fit-from-test-data-to-transform-te" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/57212549/for-missing-value-imputation-why-not-use-the-fit-from-test-data-to-transform-te</a></p> <p></p> <p><a href="https://scikit-learn.org/stable/modules/impute.html" target="_blank" rel="noopener noreferrer">https://scikit-learn.org/stable/modules/impute.html</a></p> <p></p>
post351-comment1-reply4: <md>@722 applies. Only train on the training set, fill in both with that.</md>
post351-comment2: I'll be plenty happy with that answer if an instructor can verify it!
post352: Hi Professor, do we need to submit part 6 Extra Practice in Homework 6?
post352-comment1-reply4: @360
post353: Is the out-of-bag error (1 - oob_score) or is it the oob_score_ ?
post353-comment1-reply4: It is 1 - oob_score<div>https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html#:~:text=The%20out%2Dof%2Dbag%20(,whilst%20being%20trained%20%5B1%5D.</div>
post353-comment2: In my view, the handout says OOB error estimate, so I used 1-OOB score
post354: I perform SMOTE for each folds when doing CV. And I got the result really quick, under 1 minutes. I'm confused about this run time, might be something wrong?
post354-comment1-reply4: Try bugging like add check points for code 1 min too less. Try debugging or even try plotting AUC on test dataset also try print the shapes of your X and Y.
post354-comment1-reply4: Try bugging like add check points for code 1 min too less. Try debugging or even try plotting AUC on test dataset also try print the shapes of your X and Y.
post354-comment2: Performing SMOTE on the dataset is pretty quick actually. Training the algorithm using the SMOTE data is what takes a really long time.
post355: how long is it taking people to run the 6k dataset vs the 60k dataset?
post355-comment1: 10minutes
post355-comment2-reply4: <md>Training on the 60k dataset (118k after SMOTE) is taking me just over an hour. My PC isn't the greatest though. It also depends on how many hyperparameters you are doing CV on.</md>
post355-comment2-reply4: <md>Training on the 60k dataset (118k after SMOTE) is taking me just over an hour. My PC isn't the greatest though.</md>
post355-comment2-reply4: <md>Training the 60k dataset (118k after SMOTE) is taking me just over an hour. My PC isn't the greatest though.</md>
post355-comment3: 10 - 15 minutes
post355-comment4: My laptop runs 6 Minutes with power, 22 minutes with battery.
post356: To avoid any misunderstanding or missing out on something, could anyone please explain what the 'right and wrong way of cross validation' in this question refers to? Can someone provide a clearer explanation?
post356-comment1-reply4: I think the wrong way would be applying SMOTE before doing k-folds CV as doing so leaks information to the test sets in k-fold CV. I applied SMOTE for each training folds during CV. I believe this is the right way but I&#39;d be grateful if someone could confirm this with me :)
post356-comment2: REF: https://towardsdatascience.com/the-right-way-of-using-smote-with-cross-validation-92a8d09d00c7#:~:text=We%20first%20split%20the%20data,cross%2Dvalidation%20and%20test%20scores.
post357: In addition to using class_weight to deal with the imbalance,Can i also use SMOTE?
post357-comment1-reply4: <md>I don't see why not if in your research you found that was a technique for class imbalance in random forests. Although the class weights will be different if SMOTE balances the data set.</md>
post357-comment1-reply4: <md>I don't see why not if in your research you found that was a technique for class imbalance in random forests.</md>
post358: addressing class imbalance on random forests made my classification report worse am I doing something wrong?
post358-comment1: @782 
post358-comment1-reply1: Fixing imbalances is likely to make things worse
post358-comment2-reply1: <p>For my case it worsed the accuracy little bit but improved recall a lot.</p> <p>Yes, as @782, balancing the imbalance isn&#39;t helping much.</p>
post358-comment2-reply1: <p>For my case it worsed the accuracy little bit but improved recall a lot.</p> <p>Yes, as @783, balancing the imbalance isn&#39;t helping much.</p>
post358-comment2-reply1: <p>For my case it worsed the accuracy but improved recall a lot.</p> <p>Yes, as @783, balancing the imbalance isn&#39;t helping much.</p>
post359: Are you guys having similair result? So far Alpha of 0.9 for L1 logi-regularization XGboost with imputed training set without balance worked the best test performance.
post359-comment1-reply1: I had the same result as well.
post359-comment2:  I don't know about this particular model, but there's no guarantee SMOTE does anything for you. -------------------------------------------------------------- Mohammad Reza Rajati, PhD Senior Lecturer, Thomas Lord Department of Computer Science University of Southern California 
post359-comment2-reply1: Thank you Dr. Rajati!
post359-comment3: For me, the training errors were almost the same, whereas the testing accuracy dropped after using SMOTE but it's performing much much better on the positive(rare) class with a tradeoff of a bit poorer performance for negative(majority) class.
post360:  The training set is around 40MB, does anyone know how to upload it to Github?
post360-comment1-reply1: You may try other ways to push to github such as cmd line, VSCode or Git Desktop.
post360-comment2: I am also struggling with the problem... 
post360-comment2-reply1: here's how i ended up handling the problem without needing to push to github through a new method: split the training set into two files "app_failure_training_set.csv" and "app_failure_training_set_2.csv". "Functions/Code (Build Files)" uploaded fine when i just split them in half [30,000 rows in each file].and then to split them back out later in the homework [before we start training out models] - I used the logic found in "Functions/Code (Split Files)" Functions/Code (Build Files): Functions/Code (Split Files): 
post360-comment3: I tried the first Student”s answer by downloading Git Desktop and Git Bash, and it worked.
post360-comment3-reply1:  Hi, I just keep getting this error when trying to use command or Git Bash or Git Desktop. Do you have a similar issue?
post360-comment3-reply2: No，I only use Git Bash to upload the data folder by using “git add data/*”. I upload the notebook just on the website.
post360-comment4: I also try to use Git Desktop to upload the files to the repository, and it worked!
post361: In 1(d), I have used class_weight = 'balanced' to compensate the dataset for imbalances. eg. rfc = RandomForestClassifier(n_estimators = 100, class_weight='balanced', oob_score=True, random_state = 42) However, does adjusting the class weights for minority and majority classes dynamically would make any significant difference in the model's performance? eg. class_weights = {0: 1, 1: 10} rfc = RandomForestClassifier(n_estimators = 100, class_weight=class_weights, oob_score=True, random_state = 42)
post361-comment1-reply2: <md>The class weights can affect the performance.</md>
post362: Hi, Can I use a sample in pairplot? because when trying to plot the whole data it takes over night and it's not finished.
post362-comment1-reply2: <md>@738 @731</md>
post362-comment1-reply2: @738
post362-comment1-reply2: @731
post362-comment2: Try using hue by class. That really cut down my runtime. 
post362-comment2-reply1: Yeah the hue makes a big difference!
post362-comment3: Can an instructor please clarify here if taking a smaller sample, something like 1000 samples, is fine here or not. I don't see any replies by the instructors in the previous posts at @738 and @731. 
post362-comment3-reply1: If the pairplot is taking you a long time to run, there might be something wrong with your implementation. For me, training the model with the SMOTE samples takes 5 times longer than it does to plot the pairplot. And the footnote on SMOTE says to only downsample the data set if you are running out of time, which seems to imply that you will be slightly marked down for it, so it seems like just because it takes a long time doesn't mean you should subsample.
post362-comment3-reply2: Are you using hue as well ? 
post362-comment3-reply3: Yes, I believe we are supposed to anyways based on the example scatter plots in ISLR.
post362-comment3-reply4: So even when you are plotting the plots with hue, I believe the dimension of your dataset is 76000 rows × 14 columns
post362-comment3-reply5: Yes, although the 14th column is the class that is represented by the hue.
post362-comment3-reply6: May I please know the approx time that it takes for you
post362-comment3-reply7: 10-14 minutes, depends on if I leave my PC alone or not since its not very good
post363: My computer didn't have problems running till 10 K fold, until LOOCV it worked at most in 5minutes of processing time. But LOOCV is way too much longer and I had to interupt it after 320minutes of waiting. Do you all have similiar problem or should I take a different approach?
post363-comment1: because the data set is complex and huge. loocv will cause long time. I suggest 10 fold or 5 fold
post363-comment1-reply1: HW06 instruction states that use them. Is it okay to skip LOOCV part?
post363-comment1-reply2: It says to use one of them
post363-comment1-reply3: Yes it write one of 5 fold,10 fold and loocv
post363-comment2-reply3: <md>You only need to use one of the 3 methods of cross validation. I would suggest not using LOOCV since it is much more computationally intensive.</md>
post364: Hi i am trying to use grid search to determine the alphas with a 10-fold cv on xgbclassifier. At the same time, I am trying to speed up the training process. But I’m not sure what n_jobs to set here. There are n_jobs in xgb classifier and in gridsearchCV And the doc in xgb also says: When used with other Scikit-Learn algorithms like grid search, you may choose which algorithm to parallelize and balance the threads. Creating thread contention will significantly slow down both algorithms.
post364-comment1-reply3: <md>The fastest for me was to use n_jobs for both so it's hard to say. The only way to know for sure is to test it out.</md>
post364-comment1-reply3: <md>I would guess that parallelizing the classifier rather than the grid search would be faster since that would keep the same model in cache at all times. But it depends on which of the two is better able to maximize the parallel utilization for your PC. The only way to know for sure is to test it out.</md>
post364-comment2-reply3: Yeah, as student answer says, no free lunch, all depends on your computer
post365: Hi, For this problem, when I used sklearn.svm to find the hyperplane, the margin for the hyperplane, and support vectors, I got 3 support vectors, but from the plot, it seems there are 4 support vectors. Is that due to the rounding error since the coefficients I got from svm for the hyperplane have many decimals? 
post365-comment1-reply3: You should not be using code for the ISLR questions (unless its for graphing)
post365-comment2: could I draw the graph on paper?
post366: I researched on 1d on how to deal with class imbalance on RF and one of the way is to use balanced RF which is usinn imblearn.Balacerandomforest and another way is to use SMOTE to first balance the data. which one is intended for us to use ? Because 1f is asking us to use SMOTE. also I was just curious on how much time did it take for it to oversample? or is it the training with xgb that is taking more time? tks.
post366-comment1-reply3: If the homework does not specify, it is up to us to decide. I used balanced RF.
post367:  Hello, When plotting the ROC, I got the following shape, is that normal? Thanks,
post367-comment1-reply3: I don&#39;t think the shape is normal, it seems like there are only 2-3 point from your tpr and fpr? you could check the array
post367-comment2: my roc like this: 
post367-comment3: @Anonymous Beaker did u figure out what went wrong? Im getting a similar shape as yours
post367-comment3-reply1: I believe it's because you didn't predict the class as probability. Use predict_proba instead of predict
post367-comment3-reply2: Thanks! I used your method and it worked!
post368: I learned that we can get the out of bag estimate directly from the classifier after training by clf.oob_score_. However, I got this error:AttributeError: 'RandomForestClassifier' object has no attribute 'oob_score_' and tried tutorials on online but didn't fix it. 
post368-comment1-reply2: Have you set &#34;oob_score=True&#34; when you initialize your RandomForestClassifier?
post368-comment2: I see! Thanks a lot.
post368-comment2-reply1: No problem.
post369: Is this a warning that I should be concerned about? Should these columns be dropped in the correlation matrix?
post369-comment1-reply1: <md>You need to perform data imputation before you calculate the correlation matrix. The error seems to be because there are missing values.</md>
post369-comment2: I did perform data imputation. But I replaced the missing values with NaN. Is it rather recommended that i replace it with a numeric value? imputer = SimpleImputer(strategy = 'mean', missing_values = np.nan) 
post369-comment2-reply1: The `missing_values` parameter specifies what the missing values are labeled as in your dataset. For this homework, they are given as the string "na". The missing values have to be replaced with a numeric value.
post369-comment2-reply2: Yeah my bad, I misinterpreted missing values. But I see that my imputation is workingThis is why the issue seems to be happening:1) After imputation, missing values are getting updated with numeric values2) But when I am trying to find the correlation matrix, the Nan is getting introduced again. Something similar to this post on stack overflow: https://stackoverflow.com/a/22657319
post369-comment2-reply3: Are you saving the imputed feature to a new variable and then using that new variable to compute the correlation matrix? I don't think the imputation updates in place, it only updates a copy which you then need to save.
post369-comment2-reply4: Yes I am saving it to a new variable and then computing correlation matrix.
post369-comment2-reply5: Are you sure that there are no Nan values left in your features? The stack overflow that you linked is talking about Nan in the resultant correlation matrix, not Nans in the features itself. You should look at a different method of data imputation. There should not be any Nan values left. The "all-NaN slice encountered" error you are getting originally is most likely because there are Nan values in the features when you are trying to calculate the correlation matrix, not because there are Nan values in the correlation matrix itself.
post369-comment2-reply6: Did you replace "na" to np.nan correctly before imputation?
post369-comment2-reply7: @Anonymous Beaker & @Anonymous Poet, Yes, I did. Below is the result of my check for null values in my imputed data & correlation Matrix output which suggests that imputation seems to be working fine
post369-comment2-reply8: It's not really possible for us to debug your code here, all I can say is that there shouldn't be any nan values in your features or correlation matrix. You should try a different data imputation technique, a different way of creating the correlation matrix, or visit OH where you can get some hands on help.
post370: I tried problem b(iv) and found that the variables selected are unusual. Most of the values in these variables are zero and a little of them are extremely large, which leads to a very large CV. I am wondering is this correct? Can I fix this problem by deleting variables that have many zeros or delete samples that have extremely large values? Thanks!
post370-comment1: you could try log scale to transform data
post370-comment2-reply8: <p>Try log scale pairplots, and I used median for imputation since they got so many outliers.</p> <p>Although I could not see much meaningfull interpretations.</p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Flllaz688cl53dd%2Fefa84542cba3fdcc53fd5f35e91175027f975827760fed42d7b4826f7a3f0cec%2Fimage.png" alt="image.pngNaN" width="1394" height="1360" /></p>
post370-comment2-reply8: <p>Try log scale pairplots.</p> <p>Although I could not see much meaningfull interpretations.</p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Flllaz688cl53dd%2Fefa84542cba3fdcc53fd5f35e91175027f975827760fed42d7b4826f7a3f0cec%2Fimage.png" alt="image.pngNaN" width="1394" height="1360" /></p>
post371: I am training the dataset using the following code: rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=5) rf_classifier.fit(X_train, y_train) However, it is taking a very long time to train. I have been waiting for over 15 minutes. What could be the issue? (X_train has 170 features and 60,000 rows.)
post371-comment1-reply8: <p>It took me about 2 minutes running the below code, did you run with imputated dataset?<br /><br /></p> <div> <div>rf1 = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)</div> <div>rf1.fit(X_tr_imputed, y_tr_df)</div> </div> <div></div> <div>(same size of dataset.)</div> <div></div> <div>It might depending on processor performance.</div> <div></div> <div></div> <div>You can also try the n_jobs parameter, that makes the model use multiple cores in parallel to speed it up.</div>
post371-comment1-reply8: <p>It took me about 2 minutes running the below code, did you run with imputated dataset?<br /><br /></p> <div> <div>rf1 = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)</div> <div>rf1.fit(X_tr_imputed, y_tr_df)</div> </div> <div></div> <div>(same size of dataset.)</div> <div></div> <div>It might depending on processor performance.</div>
post371-comment2: I ran it over night and it finished in 30 minutes. Hmm, how did you impute your data? I used .fillna(df.mean()).
post371-comment2-reply1: I used SimpleImputer() and my code completed very quickly. The difference in runtime may have to do with your computer hardware instead of howyou are coding though.
post372: For the random forest model, do we need to use cross validation to tune the hyperparameters like n_estimators, max_depth, etc? Or is it sufficient to just fit a model with the default hyperparameters as long as the model accuracy is acceptable?
post372-comment1: I just fit a model with the default hyperparameters as long as the model accuracy is acceptable. because problem c not mentioned determine hyperparameters using cross-validation
post372-comment1-reply1: you can try use cross validation to improve model performance.
post372-comment2-reply1: Default parameters were accurate enough from my case. Using different parameters running multiple time is very time consuming. Also, HW probleme did not mention about that.
post373: If it's not a big trouble, can we have solution for this midterm1 so that I can study them for my own academic interest?
post373-comment1-reply1: <md>They added it to @12</md>
post374: I have a doubt in one of the questions and want to submit a regrade request. Where can we submit regrading requests?
post374-comment1-reply1: <md>I would expect it to be through a private post like homework regrades.</md>
post374-comment2-reply1: Yes private post, but note that due to the shear number of student that want more points, we will not be regrading unless it is clearly a mistake on our part
post375:  My dataframe used to do the pairplot is 76000 rows * 13 columns, my program just give some empty plots. Can anyone give some advice on this, thanks.
post375-comment1-reply1: Yeah that&#39;s the correct size since the data has to be combined according to @720. Take a look at @738 and @731. You might want to try setting hue and leaving your computer alone. My computer has much less than 32 gb of ram and it was still able to run.
post375-comment1-reply1: Yeah that&#39;s the correct size since the data has to be combined according to @720. Take a look at @738 and @731. You might want to try setting hue and leaving your computer alone.
post375-comment2: After using hue on 'class', my plot looks like the following, it seems very weird. Each plot only have I few points. 
post375-comment2-reply1: I have the similair result. It probably due to lot of points are 0. 
post375-comment2-reply2: Yeah there are a lot of outliers so it makes the graph look weird. The box plots will also reflect this.
post375-comment2-reply3: I tried Logscale pairplot just in case, but still does not help much.
post375-comment3: Aren't we supposed to do scatterplots after data imputation? Mine looks very different (it may be due to different features, but I cannot read the ones in the screenshots, my 13 features with the highest CV are cd_000, br_000, bq_000, bp_000, bo_000, bn_000, bm_000, bk_000, bl_000, ca_000, cb_000, bs_000, dc_000). I did a simple mean imputation and my plot looks like this 
post375-comment3-reply1: Yes the scatter plots are after imputation.
post376: I'm using SNS.pairplot to do 1b) iv., and the code has taken over 30 minutes to run (still not done). I tried using just 2 columns to see if that would help, and it's also taking way too long to run. Does anyone have any recommendations on how to do the pairplot more efficiently?
post376-comment1-reply1: @731
post376-comment1-reply1: I did a pair plot of 1000 samples, since we are doing this visualisation for exploration purposes only. 1000 samples could indicate the nature of the data
post376-comment2: I think could make samples number smaller. Such as 1000 or 2000
post376-comment2-reply1: thank you!
post376-comment2-reply2: I do not agree that you can use a smaller sample since we are plotting a dataset with lots of outliers so the plots will look quite different if you only plot a subsample. @731
post376-comment2-reply3: What do you suppose we do then? It seems like there's no other way to plot the values.
post376-comment2-reply4: I was seeing 30+ minutes of runtime when I was using my computer while it was running. I ran the cell and then left my computer alone and it only took 10 minutes in that case. I also saw a performance improvement when I used the hue. If you are still having issues, I would leave a note in my notebook and make individual scatterplots of the 13 features.
post376-comment2-reply5: gotcha, when I tried the 2x2 scatterplot it took 30+ minutes, and individually doing the scatter plots would be 169 plots so that would take even longer despite them being individually created. I'll try setting the hue element and see if that changes anything.
post376-comment2-reply6: You should not be plotting all 170 features, only the floor(sqrt(170)) features with the highest CV so 13 scatterplots. If you are trying to plot every single feature, I can see why that would take forever.
post376-comment2-reply7: I got that, but a pairwise plot of 13 features is 13x13=169 plots. 
post376-comment2-reply8: Ah gotcha. By individual scatterplots I mean 1 scatter plot for each of the 13 features. So 13 scatter plots only. The question only asks for scatter plots, so pairwise plots are just an added bonus.
post376-comment2-reply9: oh, interesting--when I had asked a TA about the scatterplots for the last HW, they said to do the pairwise plotting (to see interactions between features). I guess the homework does say to do scatter plots, but I'll try and clarify with a TA
post376-comment2-reply10: Yeah, seeing the interaction between features is helpful to answer the question in the problem but I don't think it is necessary. If you leave a note that your computer could not run the pairwise plots, I do not see why they would mark you down if you still provide the scatter plots.
post376-comment2-reply11: sounds good! Thank you.
post376-comment2-reply12: Try running it over google co lab may be since your local machine is not able to computer that fast.
post376-comment3: Hi, try the "diag_kind=None" parameter if you are using sns pairplot. It should help reduce some processing time. 
post377: Hi all, Grades of midterm 1 were just published on DEN. If you have any issues, please let us know!
post377-comment1: Thanks. Can we please get our checked papers? So, we can understand where we made mistakes? 
post377-comment1-reply1: The feedbacks provided should be detailed enough, if you have further questions you can contact an instructor in private
post377-comment1-reply2: Noted. But, it would be better if actual solutions can be released for reference. 
post377-comment1-reply3: The professor said in the first lecture and it is in the syllabus that the solutions for exams will not be posted.
post377-comment1-reply4: Actually there will be an exception this semester, and the solutions will soon be joining the sample exams along with the other two
post377-comment2: can we get to know the average of the midterm?
post377-comment2-reply1: no
post377-comment2-reply2: can I ask you why?
post377-comment2-reply3: I mean if you don't mind!
post377-comment2-reply4: Because there is not really a point in knowing that, just do your best and you will be fine in the class
post377-comment2-reply5: But, being aware of average grades can help us better prepare for Endsem. Please let us know what the average marks are. Also when will the option to regrade be accessible?
post377-comment2-reply6: Syllabus says there may not be a formal regrade, so you probably have to make a private post on your own or go to OH. The syllabus also provides the grading breakdown and grading scale, although the professor said he may curve I would try not to worry about how others did and just do the best you can based on the grading scale in the syllabus.
post377-comment2-reply7: @742, once again you just need to take care of yourself, no need to compare to other students!
post377-comment3: In Piazza, it says that Midterm 1 is 10% of our final grade. But in the syllabus it says Midterm 1 is 20% of our grade. Could you let us know which one it is? Thanks.
post377-comment3-reply1: Sorry, I meant on DEN
post377-comment3-reply2: Always follow the syllabus, DEN is not always reliable.
post378: For the pairwise scatter plot in this question, can we only plot a subset of the whole data set for visualization? Say, just 2000 samples where 1000 from the positive class and 1000 from the negative class, since pairwise plotting for over 70k data is really time-consuming and even makes my computer crash. Or here are we no longer needed to make pairwise plots? Just one scatter plot for each top feature? Thanks.
post378-comment1-reply2: You only need to plot sqrt(170) features, so a 13x13 pairwise plot shouldn&#39;t be too bad. I found that specifying the hue made them plot much faster, although it was still around 10 minutes. I also started the cell running and then did not touch my computer until it finished, otherwise it would take a really long time. If your computer really isn&#39;t able to handle the 13x13 pairwise plot, I think it&#39;s acceptable to leave a note about that in your notebook and just make individual scatter plots of the 13 features. Since we are picking the features with the highest CV, there are a lot of outliers so plotting only a subset of the data would likely miss those outliers and your graphs would look different, which would impact any conclusions you draw.
post378-comment1-reply2: <p>My understanding is we do 170x170 (Attribute cols). Still takes time but not a thing crashes my computer.</p> <p>Or sample some portion of records?</p>
post378-comment1-reply2: My understanding is we do 170x170 (Attribute cols). Still takes time but not a thing crashes my computer.
post378-comment2: I have taken a sample of 5000 records for the pairwise plot. Hope that is okay!
post378-comment3-reply2: You could use the diag_kind=none argument to remove the diagonal, this will make the plot much faster
post379: In HW6 Question (f), we are asked to present the result of the training set and test set. I understand we need to use SMOTE in the CV process to choose the best alpha. However, to present the result of the training set on the model, I am confused that after choosing the best alpha and training the training data after SMOTE using the best alpha, are we supposed to predict the result on the raw training set without applying SMOTE, or on the resampled training set after applying SMOTE? And could someone also explain the reason behind the right choice. Hope my question is clear. Thanks.
post379-comment1-reply2: <md>If you are reporting the training error, I think it makes the most sense to report the error on the training set before SMOTE. Obviously the model should be trained with the SMOTE training set, but to me it does not make sense to report the training error on the SMOTE set because a lot of those samples are synthesized, so incorrectly classifying them does not say much about the true accuracy of the model. Additionally, since we need to compare the SMOTE model with the uncompensated case, it makes the most sense to compare the training error using the same training set, which would have to be the data set before SMOTE for a fair comparison. With all that said, only an instructor can give the true answer so take my reasoning with that disclaimer. You can also just report both training errors if you want to be safe.</md>
post380: Do we have to use the pROC package for calculating the ROC and AUC of the datasets or can we use sklearn?
post380-comment1-reply2: I used sklearn. I couldn’t find documentation for pROC so decided against using it. 
post380-comment2: I used sklearn in the previous homework to get ROC/AUC and got full points for that. As long as you're correctly showing what the question is asking for, I don't think the method matters.
post380-comment3-reply2: that absolutely fine to just use sklearn
post381: I am getting an accuracy of 1 and an error of 0 for Test Data. Is that correct or am I making some error? 
post381-comment1-reply2: The image says train accuracy, not test. My error was very close to 0 for training data, but was a bit higher (still very low) for test data.
post381-comment2: Apologies! I need to fix the print statement, though it is for Test Data only. I got this for Training data: 
post381-comment2-reply1: Although possible, it seems unlikely that the test error would be perfect when the train error is not. I would ensure that the model is being set up and fit correctly.
post381-comment3: Are you sure your code didn’t switch test and train? 
post381-comment3-reply1: Yeah that's what I was thinking as well. Make sure that your model is trained with the training data.
post381-comment4-reply1: This is for sure possible
post382: Hi Class, Unfortunately, due to a severe health issue and consequent hospitalization, I am unable to attend classes in person this week. I will do my best to give lectures online, but in case I am too weak to do so, I will ask DEN to post videos of the topics I am planning to cover this week. For now, I am assuming that I will have both Monday and Wednesday lectures 12-1:50 on DEN. I need to cancel the 3:30-5:20 lecture, as all of you have access to the DEN lectures and I am way too weak now. I am sorry about any inconvenience this may cause. Regards, M R Rajati
post382-comment1: Get well soon Professor
post382-comment2: I hope you get better soon, Professor.
post382-comment3: Hope you get well soon fast!. Dr. Rajati
post382-comment4: Take care!
post382-comment5: GetGet well soon, Professor! 
post382-comment6: Fight on professor 
post382-comment7: Get well soon, professor
post382-comment8: Get well soon, professor! Thank you for this effort you are making, health is first!
post382-comment9: Get well soon, professor.Thanks for giving us really good teaching and sharing. Health is the first priority of all 
post382-comment10: Get Well Soon!
post382-comment11: hope you get better soon！
post382-comment12: Get well soon Professor!
post382-comment13: Get well soon professor! I am so sorry to hear about your illness!
post382-comment14: Get well soon professor!
post382-comment15: Hope you get better soon!
post382-comment16: I'm truly sorry to hear that you're not feeling well. Wishing you a speedy recovery and looking forward to your return.
post382-comment17: Take care and get well soon professor!
post382-comment18: Feel better and take care of yourself!
post382-comment19: Sorry to hear professor, hope you feel better soon!
post382-comment20: Hope you can get well soon！
post382-comment21: Hope you feel better soon :( 
post382-comment22: Get Well Soon, Prof
post382-comment23: Hope you get well soon! Take care.
post382-comment24: Get well soon！
post382-comment25: Hope you feel better soon professor!
post382-comment26: Get well soon prof.
post382-comment27: Get well soon professor!
post382-comment28: Get well soon, professor!
post382-comment29: Hope you get well soon, Professor!
post382-comment30: Get well soon Professor!
post382-comment31: Hope you get better soon, Professor.
post382-comment32: Please take care, Professor.
post383: On the homework instructions it says to plot scatter and boxplots like on p. 129 of ISLR. However, p. 129 in my copy does not have any plots. Is there anything different about what is expected from these plots, or can I do a standard pairplot and boxplots like what was done in the last HW?
post383-comment1-reply1: <md>I believe that it is referring to the scatter plot and box plots on page 131. They should just be the same pairplots and boxplots as previous homework. The only difference that I can see between hw6 and hw5 is that hw5 had regression so hue was not possible, but in hw6 you can use hue for the different classes. ![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fkdscium7ywh4i8%2Fae0e4332e526685bed539f0e3519fb78d7d9f0e173953d601aa00b89a3d9aed4%2Fimage.png)</md>
post383-comment1-reply1: <md>I believe that it is referring to the scatter plot and box plots on page 131. They should just be the same pairplots and boxplots as previous homeworks. ![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fkdscium7ywh4i8%2Fae0e4332e526685bed539f0e3519fb78d7d9f0e173953d601aa00b89a3d9aed4%2Fimage.png)</md>
post384: Do we have to train Train and Test both data sets or just Train?
post384-comment1-reply1: I understand it as we need to train the random forest on the training data and calculate the error metrics from the test data.
post384-comment2-reply1: It is basic model training practice to NOT leak information between train and test 
post385: Hi,I am getting the same NegMSE for two alpha values when using XGBoost. I was wondering that can I select the alpha value which is largest to prevent features. Can someone let me know which type of alpha values to select. Thank you
post385-comment1-reply1: you should test many alpha values to find the best one, AKA CV
post385-comment2-reply1: If two alphas are giving equally good models, you can select the one you prefer. Generally a simpler model that does not sacrifice accuracy is preferred.
post386: Should we combine the train and test data for 1 (b) ii. - v.? Thank you!
post386-comment1-reply1: That&#39;s what I did to keep it consistent with the instructions from the last homework.
post386-comment2-reply1: yeah it the same idea as hw5
post386-comment3: But wouldn't this leak information about the testing data into the training data?
post386-comment3-reply1: There is no training of models going on in 1 (b) ii. - v., it is simply visualizing the data.
post387: Should 0 be treated as a missing value in the HW6 dataset?
post387-comment1-reply1: no
post387-comment2-reply1: <md>The missing values are marked as the string "na" in the dataset.</md>
post388: We are already assuming the number of clusters in K-mean clustering, so what are we minimizing here? Can someone explain? 
post388-comment1-reply1: <p>we are minimizing the sum of squares of data points within each cluster. though we know the the number of clusters ahead of time but we do not know where the location of those cluster should be place. by using the k-means clustering algorithm, it iteratively assigns data points to clusters and updates the centroids to minimize this within-cluster sum of squares.</p> <p></p> <p>There are k cluster, and therefore what I believe is that we need to minimize the distance of points within the cluster from it&#39;s centroid for k cluster. </p>
post388-comment1-reply1: There are k cluster, and therefore what I believe is that we need to minimize the distance of points within the cluster from it&#39;s centroid for k cluster. Can an instructor confirms this? 
post388-comment1-reply1: <p>we are minimizing the sum of squares within each kth cluster (centroid).</p> <p></p> <p>Although we defined the number of clusters ahead of ourselves, we do not know where the location of the centroid is. by using the k-means clustering algorithm, it assigns data points to clusters and updates the centroids to minimize this within-cluster sum of squares</p>
post389: After submitting homework on github output is clearly shown as expected but for Q1 b) I am getting this extra line:In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Can anyone tell what is the above?
post389-comment1-reply1: it looks fine
post390: Sk-Learn multi-label decision tree: Decision tree after applying Label Powerset Transformation: Is there something I'm not doing right for the LabelPower set transform?
post390-comment1-reply1: <p>Its because the best decision tree cross validation score is without any pruning, I manually chose the second best tree (different alpha value) to show pruning</p> <p></p> <p>@683 might help</p>
post390-comment1-reply1: Its because the best decision tree cross validation score is without any pruning, I manually chose the second best tree (different alpha value) to show pruning
post391: In comparison to other models like LASSO and Ridge my MSE for Linear Regression is very high (0.78), is this the case for anyone else ?
post391-comment1-reply1: Maybe try another data imputation method?
post391-comment2: I get like 0.0178 for the MSE. maybe you referring R squared?
post391-comment3: I observed the same results (i.e. getting relatively high MSE for linear regression compared to others) and this trend persists even after experimenting with different imputation techniques. Is there anything else that I could try perhaps?
post391-comment3-reply1: same here
post391-comment3-reply2: maybe impute on entire dataset and then split? I tried splitting and then imputing but the first method gave me better scores 
post391-comment3-reply3: I would be hesitant to do that, wouldn't it cause data leakage issue? I imputed the train and test separately.
post391-comment3-reply4: i did that at first but then my linear regression MSE gets rlly rlly high 
post391-comment3-reply5: Yes, I am experiencing the same thing. Splitting and then imputing is giving me higher MSE scores than the case where we impute first and then split. Please note that this is for sklearn library. I noticed that using ordinary least squares (OLS) provided by statsmodels library is providing MSE closer to the ones we are getting for other models (ridge and lasso etc.) So can we use the statsmodels library for this subquestion or is it necessary to use the sklearn library?
post391-comment3-reply6: Using statsmodel instead of sklearn is fine. There should not be a lot of difference between the expected value of MSE and the value you might get by using a different method though. As long as your method is correct, it should be fine. Personally I found that splitting the data first into train and test and applying simple imputer on the train and test dataframes had given pretty low MSE.
post391-comment4: I also got this for Linear Regression but my ridge and Lasso were much lower 
post391-comment4-reply1: I had it as well, it is a sign that the model is overfit. I used techniques described in lecture to improve the linear model.
post391-comment4-reply2: How does high train error indicate that the model us overfit ?
post391-comment4-reply3: I was under the impression that you were talking about test error. I did not see a high train error, that should not be the case.
post391-comment4-reply4: For me the issue was train error was high and test error was low which I thought was weird 
post391-comment4-reply5: If you are seeing train error much higher than test error, that is probably a sign that there is something wrong with how the model is being set up. It would take a very weird dataset and a lot of luck for this to actually happen naturally since the goal of the model is to lower the training error. https://stackoverflow.com/questions/57489100/is-it-possible-to-have-low-test-error-and-high-training-error-for-a-machine-lear 
post392: Dealing with Discrepancies in Missing Values Between Training and Test Data Typically, we fit the imputer to the training data and then apply this transformation to both the training and test data in order to handle missing values. However, there could be cases when columns in the test data have missing values that weren't missing in the training data, or vice versa. Question: How should we best approach the situation where the columns with missing values in the test data aren't the same as those in the training data? What are the best practices to ensure consistent preprocessing across both datasets?
post392-comment1-reply5: There are various imputers for you case where the missing values in the test data that aren&#39;t in the training data, common imputation includes: mean, median mode, most_frequent. mean and median, mode take cares of the missing values in test data when training data. another method is using knn imputation where it uses n_neighbors to generate a value for it. I used SimpleImputer() which handled those concerns well.
post392-comment1-reply5: There are various imputers for you case where the missing values in the test data that aren&#39;t in the training data, common imputation includes: mean, median mode, most_frequent. mean and median, mode take cares of the missing values in test data when training data. another method is using knn imputation where it uses n_neighbors to generate a mean value for it. I used SimpleImputer() which handled those concerns well.
post392-comment1-reply5: There are various imputer for you case where the missing values in the test data that aren&#39;t in the training data, common imputation includes: mean median mode, most_frequent. mean and median, mode take cares of the missing values in test data when training data. another method is using knn imputation where it uses n_neighbors to generate a mean value for it. I used SimpleImputer() which handled those concerns well.
post392-comment1-reply5: I used SimpleImputer() which handled those concerns well. 
post393: Ok. I used imputation method to fill the void using training set then applied to test set then concated. So for from 2. (c) ~ to the end of homework, are we okay to use this imputed dataset? Thank you!
post393-comment1-reply5: yes, without imputation, you would actually encounter some errors, and the technique of imputation is up to you 
post394: Could I do data cleanning(handle missing values.) before split data sets as the training set and the rest as the test set?
post394-comment1-reply5: I would suggest to strictly follow the homework guidelines
post394-comment2-reply5: @659
post395: Getting parameters of all variable equal to 0, don't know where went wrong..
post395-comment1-reply5: It&#39;s difficult to diagnose why this might be without seeing your code. Is this happening before or after you standardize your data?
post395-comment2: did you check your lambda value? If it's too high, it might force all coefficients to zero.
post396: Since for this data set feature already standardized, "both cases" refer to ?
post396-comment1-reply5: Standardization and normalization are not the same. You need one case where its the given data, then one when it is standardized.
post396-comment2: ic. Thanks
post397: 2(b) says ignore nonpredictive features. Should we ignore those featrues only when imputing missing values, or should we ignore those features for all the rest of the questions?
post397-comment1-reply5: Ignore for the rest of the questions. You should not use variables as predictors if they do not have predictive value.
post397-comment2: can a TA verified this? Thank you 
post397-comment3-reply5: If they are nonpredictive, then there is no point in doing anything with them
post398: "The data description mentions some features are nonpredictive. Ignore those features." According to the dataset intro, there are 5 nonpredictive features. I'm confused are we using 128-5=123 features to the entire question 2? So that the (c) correlation matrix should be a 123*123 plot? But (e) mentions sqrt(128), I am confused. Thanks!
post398-comment1-reply5: The correlation matrix should be 123x123. The plots for 2e should be for the top features using sqrt(128)
post398-comment2: Same doubts here. Can TA confirm this? 
post399: Is 1.c asking us to use the example from the link for 'Representing the Model as Pseudocode'? I'm not really sure what is being asked
post399-comment1-reply5: Using the code on the link, write if/then statements that give instructions for the decision tree.
post400: If I'm plottin pairwise scatter plots, should I include hue using the 'goal' column? or just plot the pairwise scatter without hue?
post400-comment1-reply5: I just plotted the scatter without hue since we do not have distinct labels.
post401: In this question are we plotting the correlation for all the features? Does that mean we would expect a 120*120 plot?
post401-comment1-reply5: Just the sqrt128 features from before
post401-comment2: I don't think the sqrt128 is before. it's from question after 2(c). does that mean we still need to plot 120*120?
post401-comment2-reply1: My bad. Yes, plot for 120 * 120 plot
post401-comment3-reply1: yeap 120*120 sounds about right
post401-comment4: I am confused as to why it's 120*120. Shouldn't it be 128 - 5 (non-predictive) = 123 * 123?
post401-comment4-reply1: Yeah 123*123 is the more accurate answer (@701)
post402: Does the problem simply mean 11 features?
post402-comment1-reply1: In a way yes.
post402-comment1-reply1: I a way yes.
post403: I forgot to mention that I needed to use the grace days for HW4, and unfortunately, my grades were deducted for a 2-day delay. I wanted to confirm whom I should contact regarding this matter. Thank you to anyone who shares a similar experience.
post403-comment1-reply1: They should be able to fix it with a regrade request.
post403-comment2-reply1: Yeap a regrade request, and it is late days you are using, not grace days
post404: Hello, I'm getting the best alpha value = 0 after using GridSearchCV. This gives the same decision tree as before. I just wanted to confirm if it is supposed to be like that or if I am doing something wrong. Thank you
post404-comment1-reply1: I think it&#39;s supposed to be like that as the initial tree itself is shallow and not too deep.
post404-comment2-reply1: This is possible, just make sure your steps and logic are correct
post405: Do we have to use an imputer like simple imputer for this? or can we just find the mean of the columns in train data to replace NaNs and then use those same means in the test data to impute as well?
post405-comment1-reply1: For me, I use a SimpleImputer using the mean strategy.
post405-comment2-reply1: you can impute however you like
post406: Hello,Has instructor Ian Wu's office hours been rescheduled? I do not know if it is an issue on my end, but the instructor is unavailable. Can someone please confirm?
post406-comment1-reply1: @46_f43
post406-comment2-reply1: Office hours are in-person every other week. Please refer to the Piazza post your classmate referenced.
post407: It says to use the "data set" for these questions but does that mean we have to use the whole dataset or can we just use the train dataframe to answer these 3 questions?
post407-comment1-reply1: I think you need to use the imputed crime dataset (which is the whole dataset).
post408: Is the question asking for a tree for each label? I think we just want a minimal tree and its decision rules right? not 2 different ones? 
post408-comment1: That's what we have asked in @683
post408-comment2-reply1: correct @683
post408-comment2-reply1: correct
post409: What range of Lamdas are we supposed to use for this? No info is given.
post409-comment1-reply1: I am using 0.001, 0.01, 0.1, 1, 10 for now, but I have not had an instructor verify that it is correct.
post409-comment2: I think it's totally up to us. I am also using something like the above mentioned answer.
post409-comment3-reply1: Use whatever you feel like, do some of your own research and testing!
post410: I am getting a huge MSE (6.148689597021077e+19) for the linear model in part (f). Is anyone else getting a similar result? I'm using KNNImputer to fill in missing values, and using fit_transform for the training set and transform for the test set. Am I doing something wrong?
post410-comment1-reply1: You might want to plot the regression line to see what is up with it
post410-comment2-reply1: I&#39;m having the same problem 
post410-comment3: facing the same issue
post410-comment4: I spoke to a TA, the solution is to impute on the entire dataset and then split into train and test. 
post410-comment4-reply1: thanks a lot!!
post410-comment4-reply2: Are you sure this is the correct way because this causes data leakage? Can someone please explain? I think we should first split, then impute the test set based on the training set. Not sure though
post410-comment4-reply3: first splitting and then impute gives large mse on test set
post410-comment4-reply4: Try using SimpleImputer. Maybe it will help
post410-comment4-reply5: I tried splitting and then using SimpleImputer, but I'm still getting a large MSE on the test set. Did that work for you?
post410-comment4-reply6: So am I
post410-comment4-reply7: use median instead of mean works for me
post411: Hi, I was wondering if we are still in schedule in terms of the syllabus? when is the last day we will meet in-person? Tks.
post411-comment1-reply7: Our second exam is December 1, so lecture will have to be done by then.
post411-comment2: I believe the purpose of the joint lectures has been so that we will not have any lectures after Thanksgiving break. The exam on Dec 1 will be in person though so that should be the last day we have to meet in-person.
post412: Question 1d is asking for a minimal decision tree. My best alpha gives a decision tree of the same size as the original. Is the question asking for a tree with fewer nodes, even if the fit is not as good to satisfy the "minimal" instruction?
post412-comment1-reply7: That&#39;s what I have assumed and changed the parameters to get a very simple tree even when it is not good to satisfy the minimal instructions.
post412-comment2: Can an instructor please let us know if the question is asking for 2 decision trees (one with optimal ccp_alpha and another with high interpretability) or only 1 decision tree.
post412-comment2-reply1: it is just one tree, then you should convert the true into decision rules (as in text in if else format)
post412-comment2-reply2: Should I use the optimal tree (no pruning) or can I manually change alpha to show some pruning?
post413: Can someone explain this algorithm using sample data and some assumed alpha values. Or suggest some resources where they have practically implemented this? It would be a great help. 
post413-comment1-reply2: ISLR walks through an example starting on pg 328. There are also videos on Youtube that will walk through examples that can be helpful.
post413-comment1-reply2: ISLR walks through an example starting on pg 328.
post414: can we use mean as our data imputation method for each feature or does it have to be something more complicated like something predictive
post414-comment1-reply2: I think we can use mean, I use mean to impute
post414-comment2: The instructions don't specify which technique to use, so it is up to us to decide.
post414-comment3-reply2: use whatever you feel like is correct
post414-comment4: Following Q for 2b, "The data description mentions some featuresare nonpredictive. Ignore those features" Can we just import data from the 6th column? Because we don't want 1~5 columns?
post415: Hi, from question 2b to 2e, it only said data set. Should we use train set or test set or the or the entire data set(1994 rows) for these questions?
post415-comment1-reply2: I assumed the entire dataset. Not sure if it wants us to concatenate the train and test data after imputation though or if we can just imputate the entire dataset in one go
post415-comment1-reply2: I assumed the entire dataset.
post415-comment2: Train the imputer on the training set and fill the training data in. Then use the same imputer for the test set.
post415-comment2-reply1: you mean 2b? How about 2c, 2d, 2e? someone said we use concated data in 2c. But I am not sure
post415-comment2-reply2: @659
post416: Do we plot the correlation matrix with training data or test data or concated data (assuming all data are imputed from the training set)?
post416-comment1-reply2: I believe it&#39;s the concated data.
post416-comment2: I believe that you should avoid getting information from the test set. Hence, rendering a plot using only the imputed training data is a reasonable choice.
post416-comment3-reply2: @665
post417: I noticed there's multiple variables for output in the inflammation diagnosis data set. While it's clear that scikit handles this, I'm wondering how exactly, and how this problem is generally handled in classification. Is the model fit independently to each variable, or are the two variables combined into a single multi-label column when the model is fit?
post417-comment1-reply2: <p>It depends on how you structure the data and which functions you use.</p> <p></p> <p><a href="https://scikit-learn.org/stable/modules/multiclass.html" target="_blank" rel="noopener noreferrer">https://scikit-learn.org/stable/modules/multiclass.html</a></p> <p></p>
post418: When we fit the models, do we include all the features or only the ones identified in the previous question?
post418-comment1-reply2: I believe we have to include all the features.
post419: For 1(d), we need to find a minimal decision tree, so what parameters we need to get to answer questions? For example: Best Score, Best Alpha for Tuning, Max Depth
post419-comment1-reply2: I am presenting: Best Score, Best Alpha for Tuning, Max Depth...also plotting the tree, printing its tree_to_code format and printing out the trained and test accuracies
post419-comment2: Do you guys have 100% at alpha=0? Mine does, I'm not sure I did it fine.
post419-comment2-reply1: yes same and i think it's fine 
post419-comment2-reply2: I also have alpha = 0, but I think this may cause overfitting so I choose next alpha. Should we choose alpha =0 or next one?
post419-comment2-reply3: Mine gives like alpha = 0.01 , 3 level depth, but honestly alpha = 0 gives me 4 level depth but still interpretable. Should I reduce them? 
post419-comment2-reply4: ccp_alpha is primarily used to prevent overfitting by pruning the tree. As my tree is not overfitting for various values of ccp_alpha, I don't see a difference. Moreover, the tree is relatively shallow and not too deep, so when ccp_alpha in this case can be 0 is what I understand.
post419-comment3-reply4: @670
post420: In Q 1 d while using cost-complexity pruning, is there any specific way that we should follow to find best ccp alpha , for ex - test accuracy or CV?
post420-comment1-reply4: <p>Here is what I am doing.</p> <p>1. splitting the data into train and test<br />1. use GridSearchCV with cv values as 5 different parameters like ccp_alpha values and max_depth in order to get the optimal parameters on the train dataset<br />2. use the above ccp_alpha and max_depth values and prune a decision tree on the train dataset</p> <p>3. check the test accuracy on the prune dataset</p> <p></p> <p>For the second part of the question I am using max_depth = 1 and pruning a decision tree based on this to get a set of decision rules with high interpretability</p>
post420-comment2: Could you please elaborate as to why are you taking max_depth = 1 when you have got the best results by doing the GridSearchCV??
post420-comment2-reply1: For me the value of ccp_alpha is 0 and max_depth is 4 (from greed search). This creates the same tree as before, as ccp_alpha = 0, means that the tree is unpruned in a way. Now if we use the ccp_aplha with 0 value then setting the max_depth = 1 will create an unpruned, shallow decision tree that makes only one split, leading to a highly interpretable but potentially oversimplified model. 
post420-comment3-reply1: You can just use GridSearchCV with min_impurity_decrease<br /><br /><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#:~:text=of%20leaf%20nodes.-,min_impurity_decrease,-float%2C%20default%3D0.0" target="_blank" rel="noopener noreferrer">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#:~:text=of%20leaf%20nodes.-,min_impurity_decrease,-float%2C%20default%3D0.0</a>
post420-comment3-reply1: You can just use GridSearchCV with min_impurity_decrease<br /><br />https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#:~:text=of%20leaf%20nodes.-,min_impurity_decrease,-float%2C%20default%3D0.0
post421: Is this tree we are supposed to built a multi class decision tree with 4 outcomes (yes, no; yes, yes, no, no; no yes)?
post421-comment1-reply1: <p>In the spec, it said that we can build a tree for each label.</p> <p></p> <p>addendum: the footnote also says it&#39;s okay to combine into a single label column that has 4 labels</p>
post421-comment1-reply1: In the spec, it said that we can build a tree for each label.
post422: for anyone interested in working with tracking data, or sports data in general - check out the NFL's big data bowl. https://operations.nfl.com/gameday/analytics/big-data-bowl/ https://www.kaggle.com/competitions/nfl-big-data-bowl-2024/overview https://twitter.com/nflfootballops For the past 6 years, the NFL has put on a kaggle competition where they release a subset of GPS tracking data, and offer up a series of prizes to whatever team can devise the most innovate approach to a specific challenge. it's a fun and challenging dataset to work with - and many participants have found jobs in the NFL or other spots based off their submissions.
post423: Hello, for (c) and (d) are we going to calculate the correlation matrix and CV for each feature in the training set or the whole data set? Thank you!
post423-comment1-reply1: I am understanding that the correlation matrix and CV is for the full imputed dataset, then we use the split data for later questions, but could an instructor confirm that?
post423-comment2: Upon more research, I think we are only imputing data for the training set, so I think the matrix and CV are for the imputed training set only.
post423-comment2-reply1: According to @659 we are imputing both train and test data, so I think the matrix and CV are for the concatenated data as well and it does not specifically specify that we need to do it for the training data.
post423-comment3-reply1: The correlation matrix should be for all data
post424: The Acute Inflammations dataset contains two output column for two diseases. How should we interpret the output? Should we separately fit decision trees for both and predict yes/no for each? or set the output pairs to {0,1,2,3} ? 
post424-comment1-reply1: You may want to look at footnote 1 on the homework instructions. I think that answers your question.
post425: Hello, The footnote for 1c suggests that we should use code from this link 'https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python' but when I click on it, the link is broken. Is there an alternate source or should I just create the code manually? 
post425-comment1-reply1: <p>Here&#39;s the link: <a href="https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html">https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html</a></p> <p>(there should be .html at the end)</p>
post425-comment1-reply1: Here&#39;s the link: <a href="https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html">https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html</a>
post425-comment2: It's in original HW shared folder as pdf file.
post426:  
post426-comment1-reply1: I think they can be pairwise.
post427:  Hello, Can someone guide me how to choose lambda in point (h) Thanks.
post427-comment1-reply1: The question asks you to choose with CV
post428: I believe that we need to split the data into train and test here and then evaluate the accuracy of the pruned tree with optimal_alpha on the test set. Let me know if this is the correct way and if yes then is it fine to randomly split the data, like 80-20 split between train-test and if we need to take care of anything related to the class imbalances of last two columns (the targets or the labels).
post428-comment1-reply1: <p>yes, i think splitting like that is one way of finding the optimal alpha.</p> <p>Alternatively, to get more accurate result, one of the most common ways is to use K-fold CV to get more varied test dataset. Sklearn also has a GridSearchCV to do that.</p> <p>About class imbalances, there is no requirement in assignment asking that. So i guess it is up to us whether to try it.</p>
post428-comment2: Is this for 1d rather than 2d? I am also wondering if we are allowed to split the data in order to better test the pruned model?
post428-comment2-reply1: My bad..yes its for 1(d).And yes I am splitting the data here to test the pruned model 
post429: According to 2(b) The data set has missing values. Use a data imputation technique to deal with the missing values in the data set. Since we have been asked to split test & train data in 2(a), I wonder which is the proper way to perform data imputation (i'm using KNNImputer) Fit the Imputer with only train data and use it to transform test data (treating it as an unseen)Fit the Imputer with the whole data set and fit_transform to all of them For missing value in non-predictive column, we are asked to ignore those. I'm just curious that is it going to skip these entire rows in the later calculation or fitting model? or what should do we do to handle them? 
post429-comment1-reply1: Train the imputer ONLY on the training set. Then impute the test set using that imputer.
post429-comment2: what if we are not training anything? Like, I am just using the mean of each column to fill in NaN values. 
post429-comment2-reply1: I think the better way is to split the data, use the mean of each training data column to fill in NaN values in the training data and use the mean of each test data column to fill in NaN values in the test data. If you use mean to fill in NaN values first and then split the data into train and test, the filled value in the test set is affected by the train set value, which is bad for predicting. 
post429-comment2-reply2: Oh that makes sense, thanks!
post429-comment3: If i train the imputer on train set & then use this imputer to impute the test, I am getting Test MSE around 8 for linear model. However, if I impute on the entire dataset, I get test MSE around 0.78. Not sure why this difference.
post429-comment3-reply1: I am using SimpleImputer
post430: Hi, I noticed that the first column of the diagnosis dataset is [35,5 35,9 35,9 36,0 ... ]. Since it represents the temperature, should we manually change it into [35.5 35.9 35.9 36.0 ... ]? Thanks! 
post430-comment1-reply1: I manually changed them to decimal , I think you can do it manually or with python up to you
post430-comment2: You can also use search and replace to change them.
post430-comment3: I converted them all into floats: if diagnosis_df['Patient Temp'].dtype != 'object': diagnosis_df['Patient Temp'] = diagnosis_df['Patient Temp'].astype(str) diagnosis_df['Patient Temp'] = diagnosis_df['Patient Temp'].str.replace(',', '.').astype(float)
post430-comment4: Just use decimal parameter when you import. pd.read_csv('../data/diagnosis.data', sep='\s+', encoding='UTF_16', header=None, decimal=',') 
post430-comment4-reply1: Hi, I followed your instruction, but it still got the same result, I want to know hhow to solve it
post430-comment4-reply2: I figure out the problem when I changed the file from .dat to .csv. I do not know why
post431: Hi Class, I am unwell today and as a result, I will not be able to attend the 12:00 noon class, so I cancel it. Instead, I will only do 3:30 PM class on Zoom. You are welcome to join: https://usc.zoom.us/j/94051898406?pwd=K2wrcWxFem5KYVhDdkt3Mk5XcE5EUT09 The video of this lecture will be uploaded on DEN as well. Regards, M R Rajati
post432: After we partition the data points in to specific regions, we can calculate RSS by using (y-y^) where y^ is the mean in that region. So, RSS becomes Why do we have to use another summation here which runs from j=1 to j? 
post432-comment1-reply2: i think the another summation here which runs from j=1 to j is summation over all regions. It adds up the RSS values of all individual regions to get the total RSS for all the partitioned regions combined. The purpose of this is to measure the overall error across all regions.
post433: I was deducted marks for 1 c - iv question stating that I did not mention the reason for selecting the time domain features. Explanation was not asked in the question, also I did provide the definitions for the selected features. 
post433-comment1-reply2: <strong attention="ky5ciwcmjq41rg">@Siddharth Aggarwal</strong> 
post433-comment2: Atleast some reasoning should have been there to justify your selection of the features...Your fellow students have also provided these explanations even though it was not asked in question. I have given a very minute penalty for just simply writing down the features. I feel to be fair to others the grade is correctly calculated
post433-comment2-reply1: I understand, but when I crosschecked with my friends they weren't deducted any marks. So I thought it seems valid to ask. 
post434: I lent a Casio calculator to someone, but I can't recall who it was. If you've borrowed a calculator from one of the TAs, please lmk.
post434-comment1: lmao u forgor
post434-comment1-reply1: yep lol
post434-comment1-reply2: Someone gave me a calculator and I put it on the podium. Hope the calculator is still there...
post434-comment1-reply3: There was a DB exam today in that room too, someone found a calculator there LOL and it's listed as lost and found, I can give you details of that person to retrieve it :) (if you haven't found yours still), it's a casio calc.
post434-comment1-reply4: Hey, sure, can you give me his info? You can just email me.
post434-comment1-reply5: There you go, You can mail him: "ritomgup@usc.edu"
post435: Hi All, DEN Students: Can you please join this zoom link instead for the exam Zoom Link: https://usc.zoom.us/j/97040902730 Thanks Siddharth
post436: Can anyone tell me how to solve this question? 
post436-comment1-reply5: <img src="https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Flll6cacyxjfg3%2Flckytvsfx5l2xn%2Fjtkvyykawowf%2Fpublic_2023_10_19.jpg" /><p></p>
post436-comment2: Where did you get that exam? It’s not in the sample exams folder.
post437: Below is from the sample exam, I was wondering what is the sigma being calculated? the variance (sigma^2) or the standard deviation (sigma)? if it is variance, why would the denominator of the exponent 2* 1/2? I thought is 2 * sigma^2 which is 2 *root (1/2)
post437-comment1-reply5: Yes you are right that the denominator in the gaussian distribution has a $$2\sigma^2$$. But we computed $$\sigma = \sqrt{\frac{1}{2}}$$ and so $$\sigma^2 = \frac{1}{2}$$
post437-comment2: so the \hat(\sigma) is not calculated using the formulas provided?
post437-comment2-reply1: It is computed using the same formula. If you square the value right at the end it would be the same thing. It's just about taking a square or not right at the end otherwise all of the computations in between are the same. Try computing this yourself and you will understand better!
post437-comment3: I was wondering why we are calculating different variances for different classes. Because question says to calculate unbiased estimate of variance, so I thought we should calculate variances for both predictors using all 4 data points. Is my understanding correct?
post437-comment3-reply1: In Naive Bayes, given a particular class, each of the predictors follows a gaussian distribution with their own means and variances. Therefore, given $$Y = 0, X_1$$ follows a gaussian distribution and given $$Y=1, X_1$$ follows a gaussian distribution each with their own means and variances which you estimate using the data points for each of the respective classes. You won't use all 4 data points.
post437-comment3-reply2: Well, I think we should estimate of $$X_1$$, $$X_2$$ in each class of $$Y$$. Refer to the definition of Naive Bayes classifier, $$P(Y=k|X_1,X2) \alpha \pi_k f_k(X_1,X_2)$$ Based on the assumption of conditional independent, $$f_k(X_1,X_2)=f_k(X_1)f_k(X_2)$$ Here $$f_k(X_1)=P(X_1=x_1|Y=k), f_k(X_2)=P(X_2=x_2|Y=k)$$. Thus, for each $$Y=k$$, we should calculate distribution of $$X$$ in class $$k$$.
post438: Hello, Can someone clarify this question and answer to me? 
post438-comment1-reply2: <p>Essentially, the regularization is making the line more vertical with respect to the x2 axis. L2 does not make the line more vertical so it is not the result of regularization. L4 would have done a better job if it were reflected across the axis, so it is not the result of regularization. That leaves L3, which is more vertical than L1.</p> <p></p> <p>For b, both b1 and b2 are being regularized. The answer is i) because the x1 axis is less important for the classification and b1 will be pushed to 0 first without much penalty. As lamba gets bigger and bigger, eventually b2 will also be pushed to 0.</p>
post438-comment2: "i) because the x1 axis is less important for the classification". Why b1 is less important?
post438-comment2-reply1: Look at the graph. Where it is on the x2 axis separates the classes much more than where it is on the x1 axis. If we take away x1 and had this in a single line, we would still be able to classify very easily. 
post438-comment2-reply2: I suppose it is "the x1 axis is less important" based on the graph. If you push b2 to 0 at first, apparently it will give more penalty on log-probability.
post439: Hello. For the sample midterm question 2, the question did not explicitly mention that the data followed normal distribution. However, in the solution I saw it used the normal distribution pdf. I wonder if there'd be similar cases in the actual midterm too? When we are given (or can compute) mean and variance, then we could assume Gaussianity? Thank you!
post439-comment1-reply2: Using the Naive Bayes’s assumption means continuous features follow normal distribution.
post439-comment2: Also the problem mentions that X1 and X2 are continuous, which is a hint.
post440: Hi, Would the details about where and how to upload midterms on D2L be posted during or just before the midterm tomorrow? Thanks. 
post440-comment1-reply2: Yes you will be informed and given 10 min to upload
post441: If you try to classify the point at 0,0 we will use itself and the one directly above it. Which point is the third? there are two points with euclidean distance 1 one above and 1 to the left. We get it right or wrong depending on which one we choose 
post441-comment1-reply2: You do not train KNN on itself.
post441-comment2: so what's the case when you use the point itself?
post441-comment2-reply1: when k = 1 is it the point itself or or is it its nearest neighbor? 
post441-comment2-reply2: I think that's when you're testing your kNN model on data its seen before would appreciate if someone could confirm 
post441-comment2-reply3: If you train the model using the training data and then use then predict on the training data, the training error will be 0 as all of the data has been seen before. In that case KNN is just a look up table. 
post441-comment2-reply4: yup. i think i was thinking back to hw1 when that happened because we weren't splitting the data yet
post442: In lecture 3, slide 67, the formula for multiclass logistic regression is given as below. How come the denominator does not have 1 + summation of the e's? 
post442-comment1-reply4: <p>For binary logistic regression, the &#34;1&#43;&#34; in the denominator arises because it&#39;s implicitly comparing two categories.</p> <p>the event and its complement. The &#34;1&#34; is representing the base category, which doesn&#39;t need its own set of coefficients because it&#39;s determined by the absence of the event of interest.</p>
post442-comment2-reply4: Further explanation is also present in the book (pages 144-146) in the Multinomial Logistic Regression section here for reference:<br /><br /><a href="https://drive.google.com/file/d/1ajFkHO6zjrdGNqhqW1jKBZdiNGh_8YQ1/view" target="_blank" rel="noopener noreferrer">https://drive.google.com/file/d/1ajFkHO6zjrdGNqhqW1jKBZdiNGh_8YQ1/view</a>
post443: While calculating t statistic value, (t=beta/SE(Beta)), if beta value is negative the entire t statistic value is negative. In this case, can we take abs value of t?
post443-comment1-reply4: If it&#39;s two tailed then yes.
post443-comment2: Which means its in the left side of the plot, yes just take the abs of t value and compare them. 
post444: I am trying to understand how beta0 is calculated conceptually for Logistic Regression and am hoping to get some clarification. If we don't have any features and are using just beta0, would that be based on the occurrence of the labels in the training data or a prior knowledge of the probability/distributions? Also in the case of no features, would logistic regression just assign everything to the more likely class, or would it work proportionally based on the probabilities? Also, if anyone can clarify the role of likelihood estimation for multi-class/logistic regression that would be very helpful! Are we using the probabilities derived from the sample data in our likelihood estimates? Thanks!
post445: Dear Instructors, I recently submitted my ipynb file (along with data, etc.) to GitHub before all the code sections had finished running, to avoid exceeding the two-day deadline extension. Consequently, some results aren't displayed in the submission. Could you please confirm whether the teaching assistants will execute the ipynb file to verify the results, or will they grade it based on the results already visible on GitHub? Thank you for your guidance.
post445-comment1-reply4: @333 seems related. You might want to go to office hours to confirm, today should be the last day that you can submit since it is 3 days after the original deadline. Might be worth submitting again depending on what the instructor in office hours says and if you have late days remaining.
post445-comment1-reply4: @333 seems related. You might want to go to office hours to confirm, today should be the last day that you can submit since it is 3 days after the original. Might be worth submitting again depending on what the instructor in office hours says and if you have late days remaining.
post445-comment1-reply4: @333 seems related
post445-comment1-reply4: @133 seems related
post445-comment2: Good call, thanks!
post446: Just to confirm, for the DEN student exam proctoring on Zoom, do we use the same Zoom link as we use for lecture? That is the only one that I can find on D2L. 
post446-comment1-reply4: @533_f7
post446-comment1-reply4: @553_f7
post447: For Question 2 in the sample midterm spring 2022, why we don't need to multiple P(Y=0) = 1/2 to the first row since I believe the formula should be P(Y = 0 | (X1, X2) = (0,0)) ∝ P(X1 = 0 | Y = 0) * P(X2 = 0 | Y = 0) * P(Y = 0)? 
post447-comment1-reply4: <md>P(Y=1)=P(Y=0), so you can just compare the rest part.</md>
post448: Can someone clarify how we get the results highlighted? i got confused. 
post448-comment1-reply4: <p>1. To find $$\beta_1$$<b>,</b> we need to get minimum value of (a), i.e. find derivative of (a) by $$β_1$$<b> (</b><b>$$\frac{\partial J}{\partial \beta}$$)</b> and take it equal to 0</p> <p>2. simplify equation from 1.</p> <ul><li>divide 2 to the whole equation</li><li>multiply $$\sum x_i$$ into $$(y_i-\beta_1x_i)$$</li></ul> <p></p> <p></p> <p><b></b></p>
post448-comment1-reply4: <p>1. To find $$β_1$$<b>,</b> we need to get minimum value of (a), i.e. find derivative of (a) by $$β_1$$<b> (</b><b>$$\frac{\partial J}{\partial β}$$)</b> and take it equal to 0</p> <p>2. simplify equation from 1.</p> <ul><li>divide 2 to the whole equation</li><li>multiply $$\Sigma x_i$$ into $$(y_i-β_1x_i)$$</li></ul> <p></p> <p></p> <p><b></b></p>
post448-comment2: Alternatively, you can multiply out the squared value in the parantheses, and then use that to take the derivative. I personally find that more straightforward
post449: Hello, I'm trying to solve the sample exam, but I am stuck on Q4. Can someone please help me solve it? I tried to understand the solution the professor has posted, but I still don't get it. How are the values of estimated y calculated? 
post449-comment1-reply4: The estimated y-values are just beta-hat 0, i.e. the average of the Y values in the training set. So, Y1 = (-1&#43;2-2&#43;0) / 4 = -1/4
post449-comment2: When you set the model to y=b0, the best guess for b0 is just the average, and this minimizes errors.
post449-comment3: Would someone mind explaining the solution? I am also trying to understand how the formulae is used.
post449-comment3-reply1: When using LOOCV, we need to predict the y for the one we have left behind as test set. So we use the rest of the data points to predict the y. Since this is a simple model y=b0, to reduce the overall error we simply take the mean of all the values from the train set. We do the above for all the data points and calculate the MSE for each.
post450: For this question, can we just use the more basic Probability based version of Naive Bayes that doesn't involve pi(x) and f_k(x), but just probabilities? I did this and got the right answer, but wondering why'd we have to do this a a more complicated way
post450-comment1-reply1: There may be a more detailed answer, but I imagine that you&#39;d have to use Naive Bayes since the question specifically asks you to use Naive Bayes to solve the problem. 
post451: I am having problem watching the recording of "DSCI 552 Lecture Video 10/04/2023 - Supplemental" lecture. I can access all other recordings perfectly fine, however this one does not load. Am I the only one having this issue? I tried using both personal network and USC network. UPDATE: Using a different browser solved the issue. Thank you for the students' answer!
post451-comment1-reply1: I just tried and it can work.
post451-comment2: Try Ctrl + F5 maybe lol.
post452: How was the confusion matrix for (a) Logistic Regression 2 1 1 2 calculated? 
post452-comment1-reply1: The decision boundary for logistic regression is linear. So if you draw a line through the linearly separable data, you will classify two points correctly and one wrong on each side of the boundary. 
post453: What's the difference between forward stepwise selection and forward selection, as well as, backward stepwise and backward selection? Previously I was taught about forward, backward, and stepwise selection as three separate methods. But I'm a little confused as to what the difference between the aforementioned selections' is?
post453-comment1: I remember professor saying that forward/backward stepwise selection taught in lesson 5 is more detailed than forward/backward selection taught in lesson 1. But I think basically, they are doing the same thing.
post453-comment2-reply1: <p>I don’t think there is a difference between forward/backward selection and forward/backward stepwise selection. Forward selection and backwards selection are two types of stepwise selection methods.</p> <p></p> <p>Relevant link: <a href="https://www.geeksforgeeks.org/stepwise-regression-in-python/amp/" target="_blank" rel="noopener noreferrer">https://www.geeksforgeeks.org/stepwise-regression-in-python/amp/</a> </p> <p></p>
post454: Hi,I was wondering if anyone can explain how to apply a LOOCV top a knn algorithm. Thanks
post454-comment1-reply1: Here&#39;s an example on the 2.0 point: You leave out the label for the x value 2.0 and look at it&#39;s nearest neighbors. For k = 1, the nearest would be the point 1.6 which has a - classification so then we would classify the 2.0 point as negative which is wrong, for k = 3, the nearest neighbors would be 1.6, 2.5, and 1, 2/3 of them are &#43;, so we would classify it as &#43; which is right. 
post455: Hello, I was working on trying spring exam 2022 today and I did not really understand how to approach part b of question 5. I looked at the solution and did not understand the first part of what it was doing. I do not see how we go from the equation in part a to what is seen in part b. How was that simplification done? 
post455-comment1-reply1: <md>$$ \cfrac{\partial J}{\partial \beta_1} = \cfrac{\partial}{\partial\beta_1}\left[\Sigma{(y_i-\beta_1x_i)^2}+\lambda\beta_1^2\right]=0 $$ It's partial derivative.</md>
post455-comment1-reply1: <md>$$ \cfrac{\partial J}{\partial \beta_1} = \cfrac{\partial}{\partial\beta_1}\Sigma{(y_i-\beta_1x_i)^2}+\lambda\beta_1^2=0 $$ It's partial derivative.</md>
post455-comment2: 
post455-comment3: Derivative the function with Chain Rule
post456: Hi, for the midterm, will the questions involving t-tests provide the critical t-value, or should we include the t-table on our cheatsheet?
post456-comment1-reply1: We will given tables if needed @533
post457: Lesson 1 had a interpretability vs flexibility chart, where do the new models land on that chart? I understand that since naive bayes is good with many p it will be on the inflexible side. I know LDA is good for multi class classification and linearly separable data. I know logistic regression is bad with linearly separable data and small n. My intuition says it's Naive Bayes as the most inflexible, LDA in the middle, and Logisitc Regression on the flexible side. but how do these compare to the original slide in Lesson 1 ? 
post457-comment1-reply1: and sorry LDA would be more flexible than logistic regression because data does not have to be linearly separable
post458: I'm a liitle confused when builing knn model and set k = 1. According to the lecture, in this case, training error will be 0 as its nearest neighbor is itself. But shouldn't the nearest neighbor for each data point exclude the point itself? 
post458-comment1-reply1: Well when your model tries to classify the training set it will use the whole training set so it will find the y value corresponding to the point. I think what you&#39;re getting at is that LOOCV would be the smart way to counteract this inherent problem when kNN = 1 
post458-comment2: I see. Thanks a lot.
post459: Hi instructors, Q3 and Q4 in Sample Midterm 1-DSCI 552 test knowledge about SVM and Decision Tree. Just to confirm according to our midterm range of chapters 1-5, we won't include these knowledge points out of the range, right? 
post459-comment1-reply1: @593
post460: When we doing Laplace correction, do we need to do it on class prior probability as well?
post460-comment1-reply1: <p>I think it depends.</p> <p>If you encounter a situation that a class have 0 instance in the training data, I think it is good to do that.</p> <p>Otherwise, I believe Laplace only affects the likelihoods, not the priors.</p>
post461: Hi, This is my first time being proctored on DEN, will we be joining a zoom or webex call and then taking the exam live while being proctored? 
post461-comment1-reply1: <md>Yes that is how it will work. You'll need to be camera on for the duration of the exam.</md>
post462: Hi all! I am confused on the Naive Baye's practice midterm problem. The solutions show that the estimated standard dev is sqrt(1/2). However, when I calculate it I get sqrt(1/3). I am using the formula given in the problem. var(Y) = 1/(4-1) [(0-1/2)^2 + (0-1/2)^2 + (1-1/2)^2 + (0-1/2)^2] = (1/3) * (1/4+1/4+1/4+1/4) = 1/3, so std(Y) = sqrt(1/3) Where am I going wrong? Thanks! Edit: I realized I was calculating for Y rather than the conditional distribution. I will leave my post up in case anyone else makes the same mistake! 
post462-comment1-reply1: <p>Yes, for some more clarification:</p> <p></p> <p>We are calculating for X1&#39;s when Y=0, so you only need to consider the 2 datapoints where Y=0 (n=2). </p> <p>X1 | X2 | Y</p> <p>-----------</p> <p>0 | -1 | 0</p> <p>1 | 0 | 0</p> <p>~ignore other datapoints where Y=1</p> <p></p> <p>This gets you to sd = sqrt( [1/(2-1)] * [(0-1/2)^2 &#43; (1-1/2)^2] ) = sqrt( (1/4) &#43; (1/4) ) = sqrt(1/2)</p> <p></p> <p>We do this same process for X2, Y=0 as well as X1, Y=1 and X2, Y=1 to calculate sd. Then we plug each condition&#39;s mean and sd into the function to calculate our probabilities.</p>
post462-comment1-reply1: <p>Yes, for some more clarification:</p> <p></p> <p>We are calculating for X1&#39;s when Y=0, so you only need to consider the 2 datapoints where Y=0 (n=2). </p> <p>X1 | X2 | Y</p> <p>-----------</p> <p>0 | -1 | 0</p> <p>1 | 0 | 0</p> <p>~ignore other datapoints where Y=1</p> <p></p> <p>This gets you to sd = sqrt( [1/(2-1)] * [(0-1/2)^2 &#43; (1-1/2)^2] ) = sqrt( (1/4) &#43; (1/4) ) = sqrt(1/2)</p> <p></p> <p>We do this same process for X2, Y=0 as well as X1, Y=1 and X2, Y=1 to calculate sd, then plug it in to the function to calculate our probabilities.</p>
post462-comment1-reply1: <p>Yes, for some more clarification:</p> <p></p> <p>We are calculating for X1&#39;s when Y=0, so you only need to consider the 2 datapoints where Y=0 (n=2). </p> <p>X1 | X2 | Y</p> <p>-----------</p> <p>0 | -1 | 0</p> <p>1 | 0 | 0</p> <p>~ignore other datapoints where Y=1</p> <p></p> <p>This gets you to sd = sqrt( [1/(2-1)] * [(0-1/2)^2 &#43; (1-1/2)^2] ) = sqrt( (1/4) &#43; (1/4) ) = sqrt(1/2)</p> <p></p> <p>We do this same process for X2, Y=0 as well as X1, Y=1 and X2, Y=1</p>
post463: We have posted the grades for HW3 on DEN. We will accept regrade requests until 10/25/2023. Make a private post on Piazza if you need a regrading.
post463-comment1: Can't see the grades. 
post463-comment1-reply1: Got updated 7 minutes ago. Thank you
post463-comment2: I don't have grades for hw 2 and hw 3 either 
post463-comment2-reply1: I can see them published on my end.
post463-comment2-reply2: they just appeared. Thank you!
post463-comment3: I cannot see my grades either.
post463-comment3-reply1: can you check again?
post463-comment4: I can't see my grades either
post463-comment4-reply1: updated
post463-comment5: I cannot see my grades either.
post463-comment5-reply1: updated
post464: In a lecture awhile ago, it was recommended that we read section 3.3.3 of the textbook, covering nonlinear predictors, correlation of error terms, non-constant variance of error terms, outliers, high-leverage points, and collinearity in linear models. For the most part, these aren't topics we've talked about in class or worked with in the homework. Are these fair game for the upcoming test?
post464-comment1-reply1: <p>He didn&#39;t teach them but recommended that we read them so I&#39;m assuming they are not on the exam</p> <p></p>
post465: Hi, We know that if we have 2 classes, say 1 and 0, we can write the likelihood function as given below: But, how do we write the likelihood function if there are more than two classes to predict, say 1, 2, and 3?? Thanks for your time!
post465-comment1-reply1: We’d use one hot notation and write it as p(xi) ^ yi <div><br /></div><div>Where yi is the ith bit of the one hot number y. </div><div><br /></div><div><br /></div>
post465-comment2: Could you please write the notation and share the image of how it might look
post465-comment2-reply1: i think the professor breaks it down in lecture 9/20? I remember it was somewhere around that time 
post465-comment3: The above shows the maximum likelihood for a Bernoulli distribution. I think we have to do the same with categorical, but for a multinoulli distribution. 
post466: Can we write exam with pencil?
post466-comment1-reply1: Of course, this is America after all
post467: Hello, I have some questions about Sample Exam 1. First of all, for question #1, I am a bit confused how for part b we can determine that Beta 1 would go to zero first without any other information. I understand that this is a lasso regularization due to the L 1 norm, but I don't know how to determine which coefficient goes to zero first. We have no information about the RSS. The solutions say that it has 'high log-probability by looking at the value of x2 alone' but wasn't the model with zero training error the one with both of them? Additionally, I saw that there was a question on decision trees, but that wasn't covered in Lessons 1-5. Should I still study that? Thanks
post467-comment1-reply1: I&#39;m pretty sure decision trees are not on the exam. And Beta1 would go to zero first because the data happen to be linearly separable in x2 meaning that we don&#39;t actually really need x1 to make classifications ie it&#39;s an irrelevant variable. 
post468: Will any formulas be given on the exam or are we expected to have them all in our cheat sheet?
post468-comment1-reply1: <md>Based on the sample exams, I wouldn't rely on the exam to provide formulas beyond the distribution charts that can be found at the back. There are a few questions where a formula is given, but not all of them do. I would write any formula you think you might need on your cheat sheet.</md>
post469: Is there any correlation between joint distribution and convariance matrix, like from one we can know the other?
post469-comment1-reply1: Let&#39;s use X and Y to explain. The joint distribution tells us P(X=x, Y=y) and provides some information about the relationship between the two variables. The covariance matrix provides information about the joint variability of X and Y.
post470: By the “SAL computer lab” does that mean in-person office hrs are usually in SAL 126? Thanks in advance
post470-comment1-reply1: Whatever the large SAL study area is called, the instructors will post what they are wearing usually
post471: The null is usually zero, correct?
post471-comment1-reply1: yes because in most cases we are wondering if something shares a statistical significant relationship with the output and 0 would be the case that it does not
post471-comment2: Could be any number, say we have factory manufacturing diameter of 80mm rod, then the hypothesis regarding whether we are in spec or not can be observed testing them samples with 80mm as null.
post471-comment3: Not necessary. But Null Hypothesis is usually "equality" (i.e., we declare some x = some value), while Alternative Hypothesis is "inequality" (e.g., x > some value)
post472: Hi everyone, I am rescheduling my OH to next week (Thursday) due to personal reasons. Sorry for the inconvenience.
post473: I don't understand the last sentence of this slide. Is sharing of coefficients the same as grouping correlated variables, i.e. to make those variables have similar β?How does the curved contour help that? Thanks. 
post473-comment1-reply1: In L2 Ridge regression, the square in the term allow for meaningful grouping. Let&#39;s say two correlated variables together with their coefficients made 10. Well grouping would give us 1,9 or 2,8, 3,7, 5,5 .... for potential values for those coeffs. In L1 Lasso, all of those would have the same value of 10 because there is no squared term so then we wouldn&#39;t be able to meaningfully select a best grouping. But in L2 Ridge, we would see 1^2 &#43; 9^2 = 82 , 2^2 &#43; 8^2 = 70, and the best one 5^2 &#43;5 ^2 = 50 and so we would select 5 and 5 for those correlated variables. The curve is a physical interpretation of the squared terms being included in the objective function. So pretty much yes to your first question and here&#39;s the math behind what the curve is doing when RSS strikes it for a specific value of lambda.
post473-comment2: I had similar questions before; I guesss it's because when the quadratic form meets the curved contour for the first time, the βs are very similar
post474: In logistic regression, is Negative log likelihood the same thing as the cross-entropy loss function? Thanks.
post474-comment1-reply1: yes
post475: Hi, I'm confused about the correct process of cross-validation. If we want to pick the best 100 features to predict the response by using k-fold cross-validation, which way is correct? First way: 1) separate the training data into k-folds; 2) for every subset containing 100 features, each time we train the model on (k-1) folds and calculate the validation error on the rest fold; 3) finally calculate the average validation error and choose the 100 best features which have the lowest average validation error. Second way: 1) we first separate the training data into k-folds; 2) we train the model on (k-1) folds to find the best 100 predictors (like using backward selection) and calculate its validation error; 3) select the best 100 predictors which have the lowest validation error. 
post475-comment1-reply1: Remember CV is just for model assessment so option 1 is the correct way. 
post475-comment2: I'm pretty sure the only difference between proposed 1 and 2 is the first method uses subset selection and the 2nd method is suggesting backward selection, as long as you're doing model selection (selecting predictors) during cross validation and not beforehand both methods are valid
post476: Hello, When will today's lecture be available on Den? Thanks. 
post476-comment1-reply1: Not sure when it was available but it&#39;s up now!
post477: Will handouts be included in the midterm? There was mention of us referencing calculus that wont be covered in lecture...
post477-comment1-reply1: There will be the T and Z tables but that is it, calculus review was posted at the start of the semester!
post477-comment2: I meant do we need to explicitly study the handouts? Thank you
post477-comment2-reply1: You should be familiar with all things calculus yes
post478: Why do we assume Gaussianity of X1 and X2? All the question says is use the Naive Bayes Assumption which means assume conditional independence of y with x1 and x2. 
post478-comment1-reply1: I am confused on this too. My best guess if that since the problem says X1 and X2 are continuous and it does not define the distribution, normal would be the natural assumption. Maybe an instructor can clarify though?
post478-comment2: I agree with your answer and would love a confirmation, I think pg 132 of lesson 3 says this is a valid technique to estimate fk(x) as long as the data have a mean and a variance. In practice I believe we need to do something called a QQ test to verify gaussianity, which takes into account things like mean = median = mode and making sure inflections occur in the PDF
post479: Solution: For very largeλ, we obtain a separator that is very close to vertical (withnegative slope) and in the limit, entirely vertical (linex1= 0 or thex2axis). L4here is reflected across thex2axis and has a positive slope, andtherefore represents a poorer solution than its counterpart on the other side.For moderate regularization we have to get the best solution that we canconstruct while keepingβ2small. L4is not the best and thus cannot come asa result of regularizing 
post479-comment1-reply1: I think it&#39;s because L1 is negatively sloped to begin with and increasing lambda would make it less and less reliant upon x2 driving the slope more and more negative 
post479-comment2: I would appreciate if anyone could confirm that it has to do with L1 being negatively sloped to begin with 
post479-comment3: Same doubts here. L2 regurlarization is square of parameters, shouldn't we look at aboslute vale(slope)?
post479-comment4: The problem itself states that L1 is the result of logistic regression. So I guess the problem wants us to explain plausible solution, let be the NLL alone we would get L1 boundaries, then as we increase the lambda to the extreme (infinite) then the boundary rotates to vertical which initially was minus slope. And obviously it won’t go beyond the positive side.
post479-comment4-reply1: so you agree that it not going to positive slope has to do with the L1 line being minus slope to begin with 
post480: Hi all, In the question (C ) I tried to use 'saga' solver, but keep receiving warning about Model can not converge and reach the Max iteration times. I already set maxiter = 3000 and scaled the data. I wonder why and is there anything else I can do? Thanks!
post480-comment1-reply1: <p>What is the penalty you are using!! please check sklearn documentation for with penalty to use with which solver attaching the link below!!!!<br /><br /><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html" target="_blank" rel="noopener noreferrer">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html</a></p> <p></p> <p>Scroll down a bit and you will find it.</p>
post481: Just to confirm, we do not have to submit the code for HW3 in HW4 (except for the part of code that is required for HW4 processing), right? I do not have to submit how I processed the three important features in HW3 then?
post481-comment1-reply1: <md>The "Important Note" at the top says you do not need to submit anything from HW 3 in your HW 4 submission. It only specifically mentions code that you may want to copy over from HW 3 in order for your HW 4 to run.</md>
post482: For b(i) and c(i), I wonder if we should use LogisticRegressionCV or we can use LogisticRegression, manually set the range of the c-values, and loop through the c's to address this problem? 
post482-comment1-reply1: they both function the same, just note that the range for the regularization for logisticregression is just an int, but the Cs in logisticregressionCV is logarithmic scale to that int
post482-comment2: If I use logisticregression and custom the range of C values to for example like c_values = np.arange(0.1,1.1,0.1), is it a valid choice? 
post482-comment2-reply1: sure, but the regularization would have more significant effect if it is on log scale. 0.1, 1.1 would not have a dramatic effect. in application log scale is the common choice to do.
post482-comment3-reply1: if you want to use LogisticRegression, then use RFECV with it
post483: Can someone explain what it meant by "Break each time series in your training set into two (approximately) equal length time series. Now instead of 6 time series for each of the training instances, you have 12 time series for each training instance"? I have my training set as follows: How do I supposed to split it, or did I do my training set wrong?
post483-comment1-reply1: <md>notice each file has 6 timeseries and 480 observations, splitting to 2 equal length that means split the data set in half two part 1 and 2 which is from 0:240 and 241:480 so no that you have two parts each with their own 6 time-series. total of 12 timeseries. e.g. (feature1\_part1, feature2\_part1, feature3\_part1,..... feature1\_part2, feature2\_part2...) There is further explanation in (a) iii. gives important information as well: > Remember that breaking each of the time series does not change the number of instances. It only changes the number of features for each instance. Might be good to look through @374, @383, and @470. Also right now it looks like you are combining all of the data points into one dataframe, you're going to want to extract the time features from each individual instance before you combine them.</md>
post483-comment1-reply1: <p>notice each file has 6 timeseries and 480 observations, splitting to 2 equal length that means split the data set in half two part 1 and 2 which is from 0:240 and 241:480 so no that you have two parts each with their own 6 time-series. total of 12 timeseries. e.g. (feature1_part1, feature2_part1, feature3_part1,..... feature1_part2, feature2_part2...)</p> <p></p> <p></p>
post483-comment1-reply1: notice each file has 6 timeseries and 480 observations, splitting to 2 equal length that means split the data set in half two part 1 and 2 which is from 0:240 and 241:480 so no that you have two parts each with their own 6 time-series. total of 12 timeseries. 
post483-comment2: Would it be an 18 * 18 pair plot then? (3 time series selected * 3 features selected * 2 parts)
post483-comment2-reply1: Yeah it should be 18x18 for the second pairplot
post484: Hello, are we allowed to submit an image of our written solutions for these questions?
post484-comment1-reply1: <md>Yes @540 @456 @187</md>
post484-comment1-reply1: <md>Yes @540</md>
post484-comment1-reply1: <md>Yes @533</md>
post485: I am encountering several potential issues with my use of logistic regressions. I am not sure whether it's due to linearly separable data or something wrong in my code/approach. Here's my code for this section: Here's the printout of one of the summaries: Here are some of the errors: Any help is appreciated! 
post485-comment1-reply1: it seems like you have the answer to your question already. but there are more to just that, some can be perfectly separable data, one might be multi-collinearity, or might be even small sample size.
post485-comment1-reply1: it seems like you have the answer to your question already. but there are more to just that, some can be perfectly separable data
post485-comment1-reply1: it seems like you have the answer to your question already. but there are more to just that, some can be perfectly separable data
post485-comment2: You will get unreliable p-values with linearly seperable data
post486: What chapters should we study for this midterm? to resampling or to model selection? Thanks in advance.
post486-comment1-reply1: <md>Lessons 1-5 from the lecture according to @533. Lesson 5 was model selection.</md>
post486-comment2: So up to October 2nd based on the syllabus?
post487: What is the recommendation for which package to use for logistic regression on this homework assignment. It seems that there is no easy way to obtain p-values with sklearn, but I also have read that other students have been having a lot of errors using statsmodels. Any guidance is appreciated :)
post487-comment1-reply1: Stats model GLM
post488: In ppts, there're some topics that the professor didn't cover in lecture but they're in textbooks. Will the midterm cover those topics? 
post488-comment1-reply1: everything taught in class is fairgame
post489:  Hello, The runtime takes so long, is that normal?
post489-comment1-reply1: <md>I'm getting about 6-8 minutes for my runtime. I'm not sure what runtime you consider long but I don't think that is too bad especially since my PC isn't great. If it is taking a long time, it can be a sign that something is going wrong or you can look into using the `n_jobs` parameter to parallelize the model fit if you aren't using it already.</md>
post489-comment2: Is your cross validation properly coded? That could be messing with runtime.
post489-comment3: from a iii, all the loops are taking like more than 5minutes for me except plotting the results.
post490: Hi, below is the question 6.3 from ISLR:I am confused how increasing s will affect the training RSS. What I think is that, when we increase "s" then the sum of Betas will increase. This will eventually increase RSS. Therefore, I think that "iv" is correct. However, I am not exactly sure about these behavior. Can someone let me know if this is the correct interpretation.Thanks 
post490-comment1-reply1: Would&#39;nt the answer be ii ?
post490-comment2: Well, ii also sound good. Can you provide your interpretation?
post490-comment2-reply1: Not the original answerer but the shape of the RSS is essentially a bowl as described in lecture, so it should initially decrease in a U shape as it goes down the sides of the bowl. Although I do not see why the training RSS would ever increase since the model would eventually reach the ideal beta values and stay there since that is the minimum.
post491: Why doesn't one hot encoding work with linear regression? I understand that all the variables are linearly related, but why would that make our regression have no unique solution? 
post491-comment1-reply1: The very definition of linear dependence is that there are many solutions that will satisfy a given set of equations. B = (Xtranspose * X) ^-1 * Xtranspose * y works in linear regression only if the features in X are linearly independent, which means there is a unique solution to Bx= y, which also means that (XtransposeX)^-1 is invertible. 
post492: Hi, Regarding a(iv), i'm wondering if I understood the question correctly. a(iv) is assessing training error, that is, after I find out the best (l,p) pair, I train the logistic model (with the best variables found before) on the 69 files' training set (selected from 88 files in total), and calculate its performance on this same training set. Because I'm getting perfect AUC = 1, and perfect hits in confusion matrix. I'm wondering if i'm understanding the problem incorrectly. thanks. 
post492-comment1-reply1: Of course this is possible
post492-comment2-reply1: It would be expected that the model perfectly predicts the data it was trained on.
post492-comment3: Same here.
post492-comment3-reply1: same!
post493: With CV_Factor as 5, I get a warning when I train my multi-class classifier. UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.This is because bending2 has only 4 samples.Should Ignore the warning or merge the 2 bending labels, or set CV to 4 
post493-comment1-reply1: <md>Bending should be combined @524</md>
post494: When I try to run logistic regression I am getting error Singular Matrix. I have adjusted the max-iter to 10 and yet I am still receiving this error. I have looked through the other forum posts and it seems that a clear answer has yet to be given. Could an instructor please help with this issue? 
post494-comment1-reply1: maxiter can be anything from 100 to 1000 in many cases, you can also set method=&#39;bfgs&#39; in statsmodels
post494-comment1-reply1: maxiter can be 100 to 1000 in many cases, you can also set method=&#39;bfgs&#39; in statsmodels
post494-comment2: I see changing the method solved it thanks. Really wish I had that like 2 hours ago haha. 
post494-comment2-reply1: sorry I seem to not be able to extract any P-values. It just says NaN for result.summary() 
post494-comment2-reply2: I am encountering the same issue, it seems to be because we are dealing with linearly separable data. I'm looking into different methods in statsmodels that can help deal with this more robustly, I'm going to circle back and try method=powell after my cross validation loop finishes running (it's been 40 mins with 3 more splits to go ><)
post495: Hello, I am getting a lot of scores as 1, and my p-values differ greatly when compared to those of my peers, is this an acceptable output? 0(1, 6)1.0000001(2, 1)0.9714292(3, 12)0.9714293(4, 1)0.9714294(5, 11)0.9714295(6, 5)0.9857146(7, 14)0.9571437(8, 4)0.9571438(9, 11)0.9571439(10, 19)0.95714310(11, 1)0.97142911(12, 6)1.00000012(13, 12)1.00000013(14, 16)0.98571414(15, 19)0.98571415(16, 13)1.00000016(17, 12)1.00000017(18, 4)0.98571418(19, 5)0.98571419(20, 4)1.000000
post495-comment1-reply2: @474 if you have difficulty choosing the pair
post496: Hello, I keep running into this error while oversampling and am unsure how to proceed. ValueError: high <= 0 this is the line where the error seems to be occurring: ---> minor_ds_oversampled = resample(minor_ds, replace=True, n_samples=major_count, random_state=42) edit: it got resolved, thanks
post496-comment1-reply2: I don&#39;t think a(iii) calls for oversampling. a(vii) does though
post497: 
post497-comment1-reply2: <md>The error term is irreducible error so its ignored for the objective function since it can't be fixed by any model.</md>
post497-comment1-reply2: <md>The error term is irreducible error so its ignored for the loss function since it can't be fixed by any model.</md>
post498: I do not think It's a graphical calculator but I still to be on safer side, I wanted to confirm once, since i have never worked with graphical calculators before.
post498-comment1-reply2: <md>Looks like that model is a scientific calculator so not a graphing calculator.</md>
post498-comment2-reply2: No, this one is fine.
post499: Has anyone else found that when printing the confusion matrix for the classifier on the test set, the classifier does not make any predictions to the positive class even after applying a resampling method? Is this to be expected or am I doing something wrong?
post499-comment1-reply2: <md>There might be something wrong with the model or there are steps you are missing. If I am understanding you correctly, your model has a true positive rate of 0%. That seems too low to be expected.</md>
post500: Hi,I have one question regarding Laplace smoothing. When we are given any problem, then do we always needs to apply Laplace smoothing for naive bayes? Thank you
post500-comment1:  When programming, you better switch it on. If paper and pencil problems, you will be told in the problem. M R Rajati’s phone 
post500-comment1-reply1: Thanks Professor.
post501: DO we need to plot Train ROC curve for 2b and 2c?or only the test ROC?
post501-comment1-reply1: <md>2b does not specifically ask for ROC so it is up to you for your comparison. For 2c @399</md>
post502: Should we compute confusion matrix, ROC, AUC etc on the resampled data? Or just train the classifier on the resampled data then calculate these values on the original data.
post502-comment1-reply1: Generally with resampling you use the resampled data for the train set and keep the test set as original data.
post503: A part of the question says to "Calculate the p-values for your logistic regression parameters in each model corresponding to each value of l and refit a logistic regression model using your pruned set of features." Does this mean we remove features that have non-significant p-values? I'm a bit confused because in the lecture on 9/18 (Week 5, Monday) I wrote in my notes that when we have linearly separable data, the negative log likelihood approaches negative infinity. Professor Rajati mentioned in this case, our p-values are unreliable, but the betas are decent. So, should we prune based on p-values (in this case a potentially unreliable significance indicator) or betas (a measure of effect size)? Thank you for any clarification!
post503-comment1-reply1: We prune based on the support of the model. You are correct that the p-values are unreliable, but I believe that is what the problem is trying to demonstrate.
post503-comment2: All of my p-values are insignificant (between 0.15 and 0.9), and I can only get p-values for the first few models (l1-l6) even after using different methods and maxiter values. Does anyone else have this issue?
post503-comment2-reply1: Yeah that is normal since the data is linearly separable and the p-values become unreliable. You might want to look into using sklearn feature selection instead of doing feature selection by p-values.
post504: Good evening instructors, In HW4 I’ve added two images in “notebook” file to show my answers of the last two problems in the jupyter notebook. May I ask is this permitted or will this influent my score of HW4? The code can work correctly for this part. Thanks in advance!
post504-comment1-reply1: for islr? that is absolutely fine
post504-comment2: Yes, thanks!
post505: On this question, it is not specified whether we should display ROC and AUC for test or train data or if we should run CV again with balanced log reg. I have it as using the split number and features extracted from (c)(i) and scores returned for test data since we used test data in c(v) and this is after that. Others have asked this question, and only students have responded. Can the instructors please clarify? Thanks happy friday 
post505-comment1-reply1: ROC and AUC should be on the test here, and the instruction says to run it again if you encounter imbalance
post506: It's really hard to write the mathematical symbols. I reached to the answer and can I only write the number for the answers skipping the process?
post506-comment1-reply1: <md>Instead of using markdown, you can write it by hand and attach a picture. Every class I've been in required all the steps to be shown (plus showing your steps gives the opportunity for partial points and feedback if the final answer is not correct) but I guess only an instructor can confirm.</md>
post507: backward section using sklearn.feature.selection is giving all false. is anyone stuck on the same thing? 
post507-comment1-reply1: <md>If it is saying all features are false, then it does not use the features at all to predict, which does not make sense for the problem. The homework provides an example of how to use sklearn.feature_selection.RFE, you might want to start there.</md>
post508: How do I calculate the p-value of a coefficient? I have been trying to google it but I cannot find a good answer.
post508-comment1-reply1: You can try statsmodel Logit
post509:  Why do we compare y bar with y hat and true y here? Also, in my opinion, RSS = n * MSE and MSE contains bias and variance and variance of noise, which will both represent the error caused by noise and imperfect model. Why do we need to use Regression Sum of Square?
post509-comment1-reply1: I think you are right about the RSS = n * MSE, while in this formula is to help calculate the total sum of square and the graph on the next page of the powerpoint could easily visualize why regression sum of square helps.
post509-comment1-reply1: I think you are right about the RSS = n * MSE, while in this formula is to help calculate the total sum of square and the graph on the next page of the powerpoint could easily visualize why regression sum of square helps.
post509-comment1-reply1: I think you are right about the RSS = n * MSE, while in this formula is to help calculate the total sum of square and the graph on the next page of the powerpoint could easily visualize why regression sum of square helps.
post509-comment2: Note that TSS is the difference between predicted_y and true_average_y - that is, it can be viewed as an estimation of "variance" 
post510: I keep getting very high p-values for all features across values of l. I have tried using RFE to select features but the p-values from the selected features are still very high (.9). This is the same when I call R in python and use statsmodels. When I call R, I get the error: glm.fit: algorithm did not converge Any leads on how I would solve this issue?
post510-comment1-reply1: Yaa that&#39;s the case since the model says at the bottom that the dataset has some kind of quasi separation or quasi seperation and hence not able to find correct p values(cannot be predicted).
post510-comment1-reply1: Yaa that&#39;s the case since the model says at the bottom that the dataset has some kind of quasi separation or quasi seperation and hence not able to find correct p values(cannot be predicted).
post510-comment1-reply1: Yaa that&#39;s the case since the model says at the bottom that the dataset has some kind of quasi separation or quasi seperation and hence not able to find correct p values(cannot be predicted).
post510-comment2: I'm facing a similar problem. My p-values are 'nan'. I got the following note: Possibly complete quasi-separation: A fraction 0.36 of observations can be perfectly predicted. This might indicate that there is complete quasi-separation. In this case some parameters will not be identified. Does anyone have a solution?
post510-comment2-reply1: I also have this issue :( 
post511: 2(a)iii taking over 30 min to compute and I am wondering if this is normal or not because I just changed my code to compute cv scores instead of rfe.score because many of the p values were coming up as 1.0: print(f'l={l}, p={X.shape[1]}, Mean CV Score={cv_scores.mean()}, {chosen_cols}, Class balance: {Counter(y)}')but not sure if this is causing my code to take extremely long as l value increases. 
post511-comment1-reply1: using cv score is completely valid
post512: Hi, I am trying to use r to calculate the p-values but am running into install errors on jupyter.... it works in spyder though and im not sure if im configuring it incorrectly or what... has anyone done the r method/ ran into any issues like this? 
post512-comment1-reply1: Well you can&#39;t be using r and python on the same notebook, id suggest you to stick with python&#39;s statsmodel package, it is an r wrapper
post513: After checking @470, I still have some doubts. For 2(a), When breaking time series in the training set, should we break it every time we input a dataset, or we can break it after all datasets are in the dataframe. Besides, do we need to shuffle the data?
post513-comment1-reply1: <p>1. I guess you did put all raw data into single dataframe before extracting them. In that case, you should break it every time inputting a dataset because our goal is to split time in each dataset into L part, then extract the features</p> <p>2. No, not before extracting them. Since it is a time-series data, the sequence shouldn&#39;t be adjusted</p>
post514: I have tried multiple L, p options, getting some values as NaN, if i use method = 'bfgs' in sm.fit(), getting all values as NaN. With this warning : C:\ProgramData\anaconda3\Lib\site-packages\statsmodels\base\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals warnings.warn("Maximum Likelihood optimization failed to " 
post514-comment1-reply1: <md>There are several other posts like this that you might want to look into. Some of the common responses are that it could be an issue with your features or you might need a lower number for the max iterations.</md>
post515:  D:\anaconda\Lib\site-packages\statsmodels\base\model.py:595: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available warnings.warn('Inverting hessian failed, no bse or cov_params I have used regularization and standardization, but this warning still appears. Is there any other way? Or can I ignore this warning?
post515-comment1-reply1: <md>I have not run into this issue but in previous homework they said that warnings can be ignored as long as your results are correct (@114). If you are worried about the regularization or standardization of your features, you can manually inspect them before passing them into your model.</md>
post516:  for c , do i need to see t distribution or z distribution?
post516-comment1-reply1: <p>Since the sample size is 200 which is greater than 30, I think we should use z-test.</p> <p>T-test is used when the sample size is less than or equal to 30.</p>
post516-comment2: Yeah, but as per slides, it says use z distribution. So even if sample size is less than 20, would I still use z distribution..?
post516-comment2-reply1: Can you provide more information on which slide you are referring to?
post516-comment2-reply2: Anonymous Helix, here you go
post516-comment3: Hi, as per my understanding, we have to use Z statistic for logistic regression. Since, this is a logistic regression problem, so we are using Z statistic. Can an instructor confirm this ? 
post517: Hi, Should we use `accuracy` or `f1_macro` as the scoring metric for gauging the best parameters? Since the dataset is imbalanced, it makes sense to use `f1_macro`, right? What should we use in this assignment? Thanks
post517-comment1-reply2: @472
post518: Would the multiclass classification confusion matrix in this case be a 7*7 matrix containing 7 types of activities in the column and row or a 2*2 matrix including TP, FP, NP, and TP as we do in the binary classification problem? Thank you.
post518-comment1-reply2: It should be 6*6 since 1.b says to combine both bendings
post518-comment1-reply2: @389
post518-comment2: Oh, I got it. Thanks!
post518-comment2-reply1: Since the prompt says "classify all activities in your training set", is it ok if I did not combine both bendings but instead already treated bending 1 and bending 2 as separate activity?
post519: Do we only use the 3 most important time domain features selected in hw3 throughout hw4? So that in a iii), when l = 20, there're 20 * 6 * 3 = 360 features in total? Thanks. 
post519-comment1-reply1: @392
post519-comment2: Is it okay if we use all of the time domain features instead of the 3 selected in hw3 for (a) iii? I get really bad results when I use the 3 features that I selected but really good results when I allow RFE to prune from all the features.
post520: Is anyone getting an issue with dataset sitting_8? And does anyone have a running list of all of them? on hw 3 when I cleaned the data, I only found I believe 2 issues - commas, and a column that had all the information stored in a list. I am wondering if I missed cleaning methods that anyone else may have found. 
post520-comment1-reply1: <p>i only found 3 files need cleaning</p> <p>- 2 files having excessive comma</p> <p>- 1 files using space as separator (which will be stored as list in a column as you mentioned)</p>
post521: Do we have to do feature selection using RFECV for sections (b) i -L1 penalized, (c)i - multinomial and ii Naive bayes or should we simply use best l? 
post521-comment1-reply1: <p>The instruction reads</p> <p>&#34;Note that in this problem, you have to cross-validate for both l, the number of time series into which you break each of your instances&#34;</p>
post521-comment2: Ok. I believe my question is that I don't get that phrasing completely. So let me rephrase my question again. Is it enough if we chose the best l-value based on the mean validation score when we do 5-fold cross validation or do we need to recursive feature elimination for the f-features we get for each l-value? do we need to find the (l,p) here?
post522: Just to confirm, the due date for HW4 is Monday, October 16th and not tomorrow, right ?
post522-comment1-reply1: @449
post523: which calculator are we allowed to bring? scientific?
post523-comment1-reply1: Scientific only, more exam details to come, everything will be mentioned
post524: how do we know what to pick for the number of features for the rfe model?
post524-comment1-reply1: CV chooses it for you, always look for <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#:~:text=support_ndarray%20of%20shape%20%28n_features%2C%29" target="_blank" rel="noopener noreferrer">the documentation</a>
post525: Do we have to see train accuracy or test accuracy for finding the optimal L ?
post525-comment1-reply1: CV should be done with training data
post526: Hi, While running the stats_model's logit object's fit method, I am getting the following error: ``` LinAlgError: Singular matrix ``` Can you please help me with the same? Additionally, there is a warning before the error: ``` Warning: The maximum number of iterations has been exceeded. Current function value: inf Iterations: 35 ``` Thanks
post526-comment1: The following change solved the issue for me: Set logit object's fit method's parameter `method='bfgs'` [reference](https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.fit.html#statsmodels.discrete.discrete_model.Logit.fit)
post526-comment2-reply1: Try to use the method lbfgs in the model, it could also be your data was not cleaned properly
post526-comment2-reply1: Try to use the method lbfgs in the model.
post526-comment3-reply1: @511
post527: I am getting the following error and online says that it is due to multicollinearity. did anyone else have this problem? I'm stuck on how to approach. LinAlgError: Singular matrix thanks
post527-comment1-reply1: <a href="/class/lll6cacyxjfg3/post/497" target="_blank" rel="noopener noreferrer">https://piazza.com/class/lll6cacyxjfg3/post/497</a> 
post527-comment2: Same error when fitting statsmodels.logit(). Don't know what to do
post527-comment3: Stats model did not work for me due to the data having high multicollinearity, but scikit learn Logistic Regression and stats mode GLM work.
post528: Hi, In this question, it said that we are strongly recommended to use the liblinear package, but I didn't find much introduction about how to use the liblinear package. I'm wondering if we can have some examples of using the liblinear package. Thank you!
post528-comment1-reply1: Go sklearn logistics regression hope can help you
post529: Hello, in the syllabus it is written that Homework 8 is due on Monday, November 28th. November 28th is a Tuesday. Is my assumption correct that the homework due on Monday, November 27th? 
post529-comment1-reply1: Yeah 27th, that might have been a typo
post529-comment2: Not certain but it seems to actually be tomorrow based on what’s published on drop box 
post529-comment2-reply1: Screen_Shot_2023-11-27_at_8.59.31_PM.png
post529-comment2-reply2: 
post530: Hello, if I am finding that one of the features identified in homework 3 - 1(c) iv is not a good one, should I keep this feature or use a different one in homework 4?
post530-comment1-reply2: you can use a different one
post531:  In the instructions "Depict scatter plots of the features you specified in 1(c)iv extracted from time series 1, 2, and 6 of each instance, and use color to distinguish bending vs. other activities." Do time series 1, 2, and 6 refer to avg rss12, var rss12, ar rss23 across each instance (dataset in the test set)? ,
post531-comment1-reply2: It is just the 1st, 2nd, and 6th feature columns yeah
post532:  Can anyone help me to solve the 4. d)?It's in the Sample Midterm 1-DSCI 552.pdf 
post532-comment1-reply2: <p>Do we have decision tree for the exam?</p> <p></p> <p>We don&#39;t @593</p>
post532-comment1-reply2: Do we have decision tree for the exam?
post532-comment2: I don't know, I came across this question in the sample paper, thought of asking in piazza
post532-comment2-reply1: Dr. Rajati said this midterm will be chap 1~5 so no decision tree for this very midterm but we eventually getting the chapter anyway next exams.
post533: what is our x and y for the logistic regression we are running? I am assuming y is whether or not the data is coming from a bending dataset, but I am not understanding what x is so that I can use LogisticRegression(max-iter=1000)
post533-comment1-reply1: I think x is the time-domain features that you extracted from 88 instances(like min1, max1, std1, median1, etc.).
post533-comment1-reply1: I think x is the time-domain features that you extracted from 88 instances(like min1, max1, std1, median1, etc.). 
post533-comment2: are we running the linear regression 20 times (l = 1-20)? I have 20 dataframes but linear regression can only be run on one dataframe?
post533-comment2-reply1: not really understanding what you are asking, l=20 just means that you are breaking the time series into 20 segments creating more features
post534:  Hello, Can someone explain, when to use each formula of the confidence intervals below. I'm a little bit confused. [B^1 - 2 * SE(B^1), B^1 + 2 * SE(B^1)] [B^1 - tn-2, α/2 * SE(B^1), B^1 + tn-2, α/2 * SE(B^1)] [B^1 - tn-p-1, α/2 * SE(B^1), B^1 + tn-p-1, α/2 * SE(B^1)] 
post534-comment1-reply1: <p>As per my understanding, </p> <p>1. This one is used when we are considering a 95% confidence interval.</p> <p>2. This one is used when we are not aware of the confidence interval, i.e., a more generalized form in terms of t-distribution and alpha. Also, when there is only one feature, Simple Linear Regression.</p> <p>3. The generalized form of finding the Confidence Interval in case of Multiple Linear Regression.</p> <p></p> <p>Let me know if I am missing out on some information.</p>
post535: When I fit my logistic regression model and call summary(), I keep getting NA's. What is the issue? 
post535-comment1-reply1: I faced this issue as well, I switched to sklearn logistic regression instead of the statsmodels, and it worked fine then!
post535-comment2: How do you get the p-values and parameters if you are using sklearn Logistic Regression?
post535-comment2-reply1: you can calculate them using the coefficients and the chisquared method, a TA recc'd that way to me and I just googled how to do it! 
post536: I'm doing L1 multinomial regression with the same method i did in 4(b)i. However the running time is now taking too long (more than an hour). I've tried reducing max_iter, C values' size. It helps, but still took more than 30 minutes here is the config code for each l (the rest is quite similar to 4(b)i : model = LogisticRegressionCV(Cs=Cs, cv=skf, penalty='l1', solver='saga', multi_class='multinomial', scoring='accuracy', max_iter=100) selector = RFECV(estimator=model, step=1, cv=skf, scoring='accuracy') selector = selector.fit(train_X_resampled, train_Y_resampled) do you have any recommendation on optimizing anything here? or did i do anything wrong in the config?
post536-comment1-reply1: it looks like you are running a CV within a CV, that is prob why it is really slow, and that is incorrect too BTW
post536-comment2: Thank you! a bit follow up question which one is the correct/proper way in this case? - using LogisticRegressionCV() with RFE - using LogisticRegression() with RFECV, then use LogisticRegressionCV() to find best C afterward?
post536-comment2-reply1: LogisticRegression() with RFECV is all you need
post537: Hi, If I put max iter to a low amount the pvalues are all high .90s range. If I leave it at a larger range 100-1000, I get the issue "Singular Matrix" like some of the other posts on here. I doubled checked my data and it all seems accounted for, so I'm not sure where the error is. Did anyone figure this out, I can't find any followups on the other piazza posts. Thanks!
post537-comment1-reply1: Is the singular matrix problem before or after feature selection? I have similar problems before but resolved after RFE
post537-comment2: +1
post537-comment2-reply1: do you fit the rfe model prior to training your data?
post537-comment3: did anyone have this issue but not have it resolved by RFE? I tried RFE but seems to still be an issue...
post538: I'm a local DEN student and am wondering in which classroom the midterm on 10/20 will take place. I am unsure as the syllabus does not state an exact location. Only the lecture locations are listed (SGM 123 and MHP 101). Thank you in advance
post538-comment1-reply1: Keep a lookout on Piazza...The information should be shared soon
post538-comment1-reply1: Keep a lookout in Piazza...The information should be shared soon
post539: I'm not sure what happened but I keep on getting the following error now and im not sure how to move forward with this error. Has anyone encountered this? ImportError: cannot import name 'concat' from '<unknown module name>' (unknown location)
post539-comment1-reply1: I had this issue as well, I just restarted the kernel and it was ok! <div><br /></div>
post539-comment2: Maybe you can reinstall the dependencies and restart the kernel and then try running the code again
post540: Hi, this question asks us to build a logistic regression model based on case-control sampling and adjust its parameters. From my understanding, once a case-control sampling method is used, the training data will be changed. So, should we redo the process of K-folder cross-validation to choose the best pair of (l, p) again based on the new training data? Also, for reporting the confusion matrix, ROC and AUC of the model, is it for testing data? Thanks.
post540-comment1-reply1: yes I think so
post541: Hi, I am confused about the following sentence. "Remember that the classifier has to be tested using the features extracted from the test set. " When performing testing on the test sets, why not using the features extracted from the train set?
post541-comment1-reply1: I believe it means to test the model using test dataset. In a iii we have to find the best (l,p) and refit the model using this best (l,p). Then using this model we need to predict using the test dataset
post542: Hello, I'm trying to turn in my hw 4 but something seems to be wrong with the repo. I'm using the same method to push as hw 1 -3 but now when I do git push -u origin dev the terminal returns: error: RPC failed; HTTP 408 curl 22 The requested URL returned error: 408 fatal: The remote end hung up unexpectedlyCan you please provide guidance?
post542-comment1-reply1: <p>try attending an OH or follow <a href="https://stackoverflow.com/questions/22369200/git-pull-push-error-rpc-failed-result-22-http-code-408">https://stackoverflow.com/questions/22369200/git-pull-push-error-rpc-failed-result-22-http-code-408</a></p> <p></p> <p>Edit: if you are unable to figure it out, you can just upload them manually on github&#39;s website for this HW</p>
post542-comment1-reply1: try attending an OH or follow <a href="https://stackoverflow.com/questions/22369200/git-pull-push-error-rpc-failed-result-22-http-code-408">https://stackoverflow.com/questions/22369200/git-pull-push-error-rpc-failed-result-22-http-code-408</a>
post543: Hi, For problem a (vii), are we going to find the confusion matrix, ROC, and AUC for the test data or training data? Thanks! 
post543-comment1-reply1: They asked for training set but its good to plot for both as there is no harm in plotting both.
post544: Can we bring some notes in exam? Given it is in person and there are a lot of formulas to remember for someone new to this field. 
post544-comment1-reply1: Yes, if you watch the 1st lecture professor describes the size of the &#34;cheat sheet&#34; we can bring to the exam
post544-comment2: It is also explained in the syllabus: > Exams will be closed book and notes. Calculators are allowed but computers and cellphones or using any devices that have internet capability are not allowed, except for writing the solutions or being proctored are not allowed. One letter size cheat sheet (back and front) is allowed for Midterm 1. Two letter size cheat sheets (back and front) are allowed for Midterm 2.
post544-comment2-reply1: what is the syllabus for the exam? Is it till the lecture of Oct 11th?
post545: Due to the linearly separable data, I obtained the error message "Singular matrix" when calculating the p values of logistic regression in HW4 (a) iv. The problem asks us to report the parameters of logistic regression βi’s as well as the p-values associated with them. From my understanding, in such a case, the p values cannot be calculated or there is no meaning to calculate them, isn't it? If i am right, can I just write the p values cannot be calculated or there is no meaning to calculate them due to the linearly separable data for the answer of HW4 (a) iv? Thanks.
post545-comment1-reply1: I also have same problem, but when I use chi-square to solve p-value. It works! 
post545-comment2: Are you using sm.Logit() or LogisticRegression()?
post545-comment3: I've also got the same problem. Statsmodels doesn't converge, scikit has no built-in p-value calculator. Even the github / stackoverflow solutions to this specific problem don't seem to work in this case. Can an instructor verify that the chi2 https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html calculation from scikit is a valid way to determine the p-values?
post546: To get our 12 time series we split our former features from 6 to 12 When we redo the plot for time series 1,2,6 are we doing the new 1,2,6 or do we want to include 1,2,3,4,11,12 where we include all of the former data from time series 1,2,6 if this makes sense?
post546-comment1-reply1: If I understand correctly we should do the latter one, total 6 features from the splited data
post547: How to compute P(word|c)? 
post547-comment1-reply1: I believe on this side it is given to us, but generally it is useful to use either Baye&#39;s or law of total probability when solving for conditional probabilities.
post548: The requirement is : iv. Report the confusion matrix and show the ROC and AUC for your classifier on train data. Report the parameters of your logistic regression βi ’s as well as the p-values associated with them. Should we report all the confusion matrix, ROC, AUC for these 20 splits data? Which means we should output 20 confusion matrixs, ROCs, AUCs. Or just find best best model with one split method?
post548-comment1-reply1: I believe we have to find the best value of l, split the data using that value of l, fit the model &amp; then report confusion matrix, roc auc
post549: I'm a little bit confused about the data splitting part where the prompt says "break each time series in your training set into l ∈ {1, 2, . . . , 20} time series of approximately equal length", and I try to print a dataframe to see if my splitting is done correctly. So does that mean that for example, when l=5, I should have 6time series*5splits*3features = 90 columns in total? 
post549-comment1-reply1: Yes&#xff0c;when l=5, should have 6time series*5splits*3features = 90 columns in total.
post549-comment1-reply1: Yes&#xff0c;when l=5, I should have 6time series*5splits*3features = 90 columns in total.
post549-comment2: when you say "when l=5" does that mean that we have to have 20 dataframes?... I was under the impression that we just wanted to split the data into 20 equal segments 
post549-comment2-reply1: @504_f1
post549-comment3: I thought we only do with 3 features so 3*5*3 = 45 times series.?
post549-comment3-reply1: There is no limitation on time series 1, 2, and 6 stated in a(iii). I think we need to use all features from all split time series for feature selection.
post549-comment3-reply2: I have the same thing @Seungil Hong. If we have 3 time-features*3variables*5parts = 45 columns I am also wondering, when it says "break each time series in your training set into l ∈ {1, 2, . . . , 20} time series" does it mean number of splits of number of parts? If number of parts, then I=1 would have "0" split (the whole data) and I=20 would have 19 splits (be in 20 parts). Can someone clarify?
post549-comment3-reply3: @504_f1
post549-comment3-reply4: What I did was, from my crude understanding. 1.Traning set with the 3 times series (3 out of 6 columns of original dataset) with my selected 3 most important features calculated from those ['avg_rss12', 'var_rss12', 'var_rss23']. This is L=1 for example I got 9 colunms (=3*3). 2. then slice those OG rows by for example L=2 (which cut at approx 245th row?) of each datafiles. then calculate the same features (3*3 colunms of both first and second part of OG dataset) append them to the right side of (1)'s table structure. I got 3*3*2 = 18. The same logic applied to all L's so I got the same 69 rows of training and 19 rows of test set no matter how many slices I used. if we required to try features of all calculated colunms then it goes 6(OG columns)*3(important features)*2 = 36 as it is the way @Anonymous Beaker mentioned. I just wanted to be clear if I'm following the right track.
post549-comment3-reply5: I did both, and from 2a iii using all time series (6*3*2=36 for L=2) is actually works much better and higher accuracies than using only 3 time series across all L values. But I wish some of TA clarifies for double check though.
post550:  During the lecture, professor mentioned that there are 2 kinds of randomness: the randomness in training sample and randomness in y0 given x0. In my understanding, the first randomness is represented by variance and bias. However, which variable does the second randomness represent? Is that refers to Var(ɛ)?
post550-comment1-reply5: I remember the other one is “act of god”
post551: I get better precision when I use 0.25 for the logistic regression. Do I have to try them all? or just keep it as default, 0.5? 
post551-comment1-reply5: Its always good to try them all! No free lunch!
post551-comment2: Do you mean I need to loop through all the different threshold values or just try few numbers from visually looking at ROC curve then that's okay?
post552: For RSE we use sqrt(RSS/n-2), but should it not be n-3 as in n-p-1 p will be 2 if we count both beta0 and beta1 ?
post552-comment1-reply5: <md>p represents the number of features. Betas are not features, they are values that are multiplied with the features to get the predictions (except for beta 0 which is just the intercept).</md>
post553: I'm kind of confused about where we should submit our homework 4. I understand that homework 4 is based on the same dataset as homework 3, but I didn't see a homework 4 submission link from Piazza. Should we submit the homework file through the same link we submitted homework 3? Or is there any other link that we submit our HW 4 to?
post553-comment1-reply5: It should be posted soon, <a href="https://piazza.com/class/lll6cacyxjfg3?cid=42">@42</a><div><br /></div><div>edit: it has just been posted, sorry for the delay</div>
post553-comment1-reply5: It should be posted soon, @42
post554: When using alternative approach, i get many l-p pair of score 1, is this possible? Thanks
post554-comment1-reply5: Yeah that is possible for the test score, in that case you might want to look at the cv score
post555: As per 2. a) vii) We have a imbalanced dataset for classes so can we use imblearn library or we have to code the sampling on our own??
post555-comment1-reply5: Yes of course
post556: Hi. I have a question about the choice of evaluation score when performing cross-validation on a Classification Problem. For regression problems, the average MSE is used, which I understand. But for regression problems, professors use average error(the average misclassification times). For my understanding, it is equivalent to using the accuracy. My question is, is it better to use f1-score or AUC-ROC(Area Under the Receiver Operating Characteristic Curve) when the data is imbalanced? I remember the professor said in the lecturer, accuracy is not a good indicator when dealing with imbalanced data, right? So, for HW4 (a) iii, when performing 5-fold Cross-validation, should we use f1-score or AUC-ROC(Area Under the Receiver Operating Characteristic Curve) instead? Thanks.
post556-comment1-reply5: in a.iii, you can just use accuracy as the metric, since you have not discovered class imbalance in your data. The part for that comes in a.vii
post557: how does splitting the training set into 20 segments affect how we should be calculating the time-domain features? I am not understanding these sentences - Remember that breaking each of the time series does not change the number of instances. It only changes the number of features for each instance.
post557-comment1-reply5: <p>I think it means the instances (files) remain the same, so for training the number of rows corresponds to the number of training files, but with each l, the original feature set is split into l-features.</p> <p></p> <p>So like with (ii) where we have 2 splits for every original feature we keep splitting them, but the number of rows is ultimately the same because we&#39;re not changing how many files we&#39;re using, just splitting the features more.</p>
post557-comment2: So we are simply doing (ii) but now just splitting each feature into 20 parts?
post557-comment2-reply1: My understanding is we are splitting into L parts, not just 20 parts. L is 1,2,3,...20. So we will do logistic regression 20 times for 20 datasets.
post558: Hello, I have a doubt, when splitting the dataset into 2, should we calculate the time domain features first and then split into 2 columns or should we split the raw data first into rows of 240-240 each and then apply the time-series feature extraction on those ? 
post558-comment1-reply1: Split first then apply time-series feature extraction
post558-comment2: By the rubric，break time-series first
post558-comment2-reply1: U mean the same as above reply right ? Or u telling opposite? Sorry got confused
post558-comment2-reply2: Essentially, we split a time series into the considered number of equal pieces and each of these pieces is added as a feature. Thus break the time series first and now do feature extraction(the min,max,etc) on these features..
post559: Getting Error 'Input has NaN' values. Did anyone get this issue?
post559-comment1-reply2: <md>@384 @408 Check which dataset is giving this issue. Mostly it will be bending2 dataset 4.</md>
post559-comment1-reply2: Check which dataset is giving this issue. Mostly it will be bending2 dataset 4.
post559-comment2: yes in the bending 2 dataset 4, the delimiter is space which is causing this NaN error. Addressing that fixes the issue.
post560: In question 2 a vii, do we have to perform oversampling or undersampling?
post560-comment1-reply2: You can find more information on how to treat unbalanced data in the lecture and on the slides, refer to Lesson 3 slide 49 - 66
post560-comment2-reply2: Oversampling is seems good option though! oversampling the minority class though!
post560-comment2-reply2: Oversampling is seems good option though!
post561: Are there still going to be office hours this coming Thursday and Friday (the 12th and the 13th)?
post561-comment1-reply2: There will be no official OH during school holidays, since everyone is on break. But there might be instructors that are willing to host them still, you can check the OH post for more uodates
post562: I used the backward selection and cross-validation in the previous part to get the best value of (l,p). When I trained the model in part (iv), I still got a p-value for a parameter greater than 0.05. Would that be acceptable? Thanks! 
post562-comment1-reply2: That is ok, as long as you followed proper feature selection and cv
post563: when I try to build the 90% bootstrap confidence interval, I am getting back nan for both upper and lower bound Did anyone else experience this? I have tried using spicy.stats bootstrap, and python's bootstrapped and still have got nans
post563-comment1-reply2: I have also tried to run on vscode and Jupyter, not sure if its supposed to make any differences
post563-comment2: Try checking number of columns obtained from each csv file. For one of them, it is 8 which gives rise to the above issue
post563-comment2-reply1: thank you! that fixed it
post563-comment3: I think it's because your data contains nan values, try to replace nan value with 0.
post563-comment4-reply1: You likely did not clean you data properly
post564: For the question - Do you see any considerable difference in the results with those of 2(a)i? - is there anything in particular we are looking for? My trends/ graphs are looking pretty similar, but I don't know if we were supposed to be looking for something specific when comparing the resu
post564-comment1-reply1: Just try your best to look for differences, if they are indeed similar then you can state so
post564-comment2-reply1: <p>Mine look similiar too.</p> <p></p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Flllaz688cl53dd%2F8d0f1bcde4e96e151eb481bf4a7d4fa88d7ca2ce6918c87f59f6d059ec359f09%2Fimage.png" alt="image.png" /></p> <p></p> <p>and then,</p> <p><img src="/redirect/s3?bucket=uploads&amp;prefix=paste%2Flllaz688cl53dd%2F18b7fdfad4d281c493e26194378f9d39b39af90c8e2e2b38a5e63281c2104925%2Fimage.png" alt="image.png" /></p> <p></p>
post564-comment3: For a.ii, are we supposed to have a 18*18 pairwise plot instead of a 9*9 one?
post564-comment3-reply1: For the question breaking into half, it will have 18 statistics in total, so 18x18 is fine
post564-comment3-reply2: Oh yes, I corrected the second one. 
post565: Hi, when I deal with the 5-fold cross-validation, I standardized the features in each (k-1) training set, do I need to standardize the features in the validation set? Thank you! 
post565-comment1-reply2: <md>Yes, if you standardized the training values then the model is built on the standardized values. The model will need standardized inputs to make its predictions.</md>
post566: 2(a)(i) makes mention of 1(c)(iv) which uses all of the data, but mentions the training set. Do we rebuild the time series 1-6 structure of (c)(iv) with just training? Thanks!
post566-comment1-reply2: <md>The question says to depict each instance so I think you should use all the data.</md>
post567: Can we attach handwritten pic since the fomulas are difficult to type?
post567-comment1-reply2: Yes, but make sure the image(s) you attach and your handwriting are clear and legible. 
post567-comment2: Its good to learn markdown/latex anyway :))
post567-comment2-reply1: How should we properly attach images to the markdown? I've searched online but still couldn't upload them in the correct way.
post567-comment2-reply2: This is what worked for me in vscode, Create a new markdown cellOpen your image and copyPaste to target markdown cell
post568: Did not facing problems of instability in calculating logistic regression parameters as the profossor suggested in class. Am I doing something wrong?
post568-comment1-reply2: This is possible, if you are sure then you can proceed forward
post569: I'm curious if there are any debugging tools available that allow me to view the variable values at each line during debugging.
post569-comment1-reply2: Each IDE/editor has their own version, I personally recommend using VSCode/VSCodium, which you can find the variable debugger here:<div><br /></div><div><a href="https://code.visualstudio.com/docs/datascience/jupyter-notebooks">https://code.visualstudio.com/docs/datascience/jupyter-notebooks</a></div>
post569-comment2: Thank you so much for the answer. It seems I had some issues using Jupyter Notebook in Visual Studio and couldn't get it to run. Are there any extensions recommended for the online version of Jupyter Notebook? If not, I will continue troubleshooting the issue with Visual Studio.
post569-comment2-reply1: Are you running Visual Studio or VS Code? It's confusing but they are different editors. The VS Code extension that the instructor recommended only works in VS Code. It is what I use as well and it is able to inspect the variables at any step.
post569-comment2-reply2: If you want to debug line by line code in ipynb files then I would say you use. JETBRAINDS dataspell it has a built in debugger.
post570: There are mutiple pairs of (l,p) in terms of same best_score, which one should I use to report performance for a (iv)
post570-comment1-reply2: There is no one correct answer for this, just explain your reasoning behind what you choose
post571: Is there class tomorrow?
post571-comment1-reply2: Yes, it is not a school holiday
post572: "Calculate the p-values for your logistic regression parameters in each model corresponding to each value of l and refit a logistic regression model using your pruned set of features. Alternatively, you can use backward selection using sklearn.feature selectionor glm in R." If I understood clearly, "alternatively" here means that we can use p-value to select the hyperparameters, or, the other way is to use sklearn.feature_selection.RFE to select the hyperparameters. Do the hyperparameter selection either way, instead of implementing both p-value calculation and RFE?
post572-comment1-reply2: This is correct, use either one of the methods
post572-comment2: If I use RFE, for question b (ii): "Compare the L1-penalized with variable selection using p-values.", can I Compare the L1-penalized with variable selection using RFE instead of p-values as the question suggeted?
post572-comment2-reply1: yeah that should be ok, if there are not other questions asking to use p-value to select
post572-comment3-reply1: <md>To clarify, for this question are we supposed to do feature selection for all 20 models? I.e. for each L in {1,2,...,20} we 1. perform logistic regression, then 2. perform feature selection to "prune" features, then 3. re-fit a logistic regression model.</md>
post572-comment4: following up on the original discussion post - what is meant by 2 - perform feature selection to 'prune' features? if the original logistic regression using all time domain features and then feature selection is cutting it down to the three features we chose in last homework? 
post573: Hi, Do we need to show confusion matrices and ROC curves for train and test both, or just for test data? Thanks
post573-comment1-reply1: @399
post574: Hi, Just wanted to confirm the HW4 due date. Is it Oct 16 (according to Syllabus pdf)? 
post574-comment1-reply1: Correct, because of fall break, we wouldn&#39;t want you to stress over the break!
post575: hw 4 a.ii do we need to go back to the original data sets provided to divide into two? meaning we would need to recalculate the summary data (3 features) as well? or can we just randomly divide the two summary data sets like we did in part i? 
post575-comment1-reply1: Yes the instruction says to devive the 6 time series into 12
post576: According to instructions for hw4 2a i) and ii) we have to use selected features used in Hw3. But for 2a iii) can we use all features as it is not specified?
post576-comment1-reply1: i think we need to use all 6 time series , but only use the 3 features you have selected.
post576-comment2: Can we use 6 features instead of 3 as it is not specified?
post576-comment2-reply1: @392
post577: I forgot to create a data folder above the AReM folder :(. Does this structure cause me to lose points in HW3? Do I need to modify my path as"../data/AReM...." and submit again? I post my path here: 
post577-comment1-reply1: it should be ok, follow instructions clearly next time, if you get points ducted open a regrade requesta
post577-comment1-reply1: this should be ok
post578: Are we expected to choose 3 out of the 43 features (or) can we aggregate and choose MIN or MAX or … across all 6 features?
post578-comment1-reply1: @359<div>Aggregate and choose min or max or … and so on</div>
post579: Is it normal to take long runtime for (a) iii and getting lot of features like below? 
post579-comment1-reply1: It will be a lot of features yes, since you are splitting up each into multiple, but it shouldn&#39;t take too long since its just df manipulation
post580: Should we include test data for calculating bootstrap confidence interval?
post580-comment1-reply1: Yes , we have to include both train and test data for calculating Confidence intervals .
post580-comment2: Just to be sure, can an instructor clarify this?
post580-comment3-reply1: @307, @316
post580-comment3-reply1: @306, @317
post581: Is the submission of HW3 due tonight (11:59 pm) or tomorrow (Oct 7-according to github) 11 pm?
post581-comment1-reply1: Tonight, refer to the syllabus
post582: Hi, I am trying to get more practice for the upcoming exam. It would be very helpful to have answers to the problems in the back of the chapter from the book. I was wondering if it is possible to get these. Perhaps, we can get the answers to the homework questions that are "extra practice"? If that isn't possible, what is the best way to practice for the midterm? Thanks
post582-comment1-reply1: The professor said he does not provide homework answers. Maybe check with a TA at office hours if you need feedback?
post583: In question 1b) we were told to split the dataset into train and test. In 1cii) can we extract the time domain features for tran and test seperately or do we need to combine them for this question?As per my understanding it shouldnt matter but would just like to confirm for consistency.
post583-comment1-reply1: You should display all the statistics together (you will need to find the summary of all the data later)
post583-comment2: I am getting all the statistics together but just displaying them seperately to maintain consistency will this be fine
post583-comment2-reply1: yeah thats ok
post584: How do we keep track of late days? Do the TA's keep track of them or do we document them in the ReadMe? Thank you!
post584-comment1-reply1: We track them, if you want to use any then put it at the top of your notebook
post585: Is the training set referring to the one that we build in 1 (b)? If so, since when selecting the three features in 1(c)iv we are using all the data, do we need to re-compute the three features for time series 1, 2 and 6 on the training set 
post585-comment1-reply1: yeah its all the same as 1.b, you should run any part of HW3 if its needed for HW4
post585-comment2: To clarify, do we use the features we calculated in 1(c)iv for HW3 on the entire dataset here or do we recalculate them using the training set only?
post585-comment2-reply1: If you are training a model, then you should ONLY use the training set as input
post585-comment2-reply2: great, so we have to recalculate 1c ii. with training dataset only.
post586: Hi All, Due to some unexpected circumstances, I will start my OH with a delay of 20 min. New timings: 8:20pm-10:20pm Thanks
post587: Remember that the classifier has to be tested using the features extracted from the test set. Does this mean we extract features from test data or does it mean we use the features extracted from 2a) iv), which were extracted from train CV data?
post587-comment1-reply2: It means to use the time domain features of the test data, the selected features are from iv
post588: We have posted the grades for HW2 on DEN. We will accept regrade requests until 10/11/2023. Make a private post on Piazza if you need a regrading.
post588-comment1: I do not have a posted grade yet. Is there anything I should do about this? Thanks!
post588-comment1-reply1: Your HW is still being graded!
post588-comment1-reply2: Thanks!
post588-comment1-reply3: Hello! My grade still has not been posted. Is there any update as to when I can expect to see my grade?
post588-comment1-reply4: no need to worry, we will get it graded no matter how long it takes
post588-comment2: May I know if mine is still being graded too? Thanks. (NaiYun Tung)
post588-comment2-reply1: looks like there was a mix up with your name (Nai-yun vs NaiYun), it is now being graded 
post588-comment2-reply2: Thank you!!
post588-comment3: hi - can someone help check on my homework? I got feedback saying "NO SUBMISSION." I am still fairly new to coding/ GitHub, but I just double checked and see that I have my homework posted when opening the submission link. Can someone let me know if there is an issue with the way that I am submitting my homework? thank you!
post588-comment3-reply1: It seems like you did everything correctly, but GitHub did not mark for some reason, we will get it graded ASAP 
post588-comment3-reply2: ok thank you! I will make it to office hours as well to see if there is a nuance in the way I am submitting homework on GitHub desktop. 
post588-comment4: Hello, I don't see my homework grade as well. The feedback says: "Could not find submission for HW2" but I submitted it and double checked it. Can someone also let me know if there was an issue with my submission. Thank you!
post588-comment4-reply1: It seems like you did everything correctly, but GitHub did not mark for some reason, we will get it graded ASAP
post588-comment4-reply2: Thank you so much!
post588-comment5: Having a similar issue as above where my grade/submission is missing on DEN. Could someone look into it? Thanks!
post588-comment5-reply1: looks like your grader missed you grade while inputting, I have updated it
post588-comment5-reply2: Got it, thanks!
post588-comment6: Having a similar issue can't see my grades missing on DEN, can someone look on to this issue?Thank you!
post589: I'v created a logistic regression for the various splits and my p-values for the coefficients are coming out to be a mix of nans and predominantly values in the high 0.90's (i.e. extremely insignificant). Is there any underlying cause that is known for this?
post589-comment1-reply2: <md>@384. Could also be an issue due to linearly separable data if the number of features is larger than the number of samples.</md>
post589-comment2: Won't we all encounter a case where we have 69 samples (i.e. the training set) with 360 features/columns since we are supposed to take 3 metrics for the 6 time series in each column split into 20 segments or 6*20*3=360 or am I not performing the splitting correctly?
post589-comment2-reply1: Yes
post589-comment2-reply2: Yes, I'm doing it incorrectly?
post589-comment2-reply3: Yes, it's expected to have the case with more features than samples.*
post589-comment2-reply4: Sorry, I accidently hit submit prematurely
post589-comment2-reply5: Do you know if changing the maxiter can help at all?
post589-comment2-reply6: Also, if we are supposed to "prune" the features, how are are expected to do that if all the features are highly insignificant? 
post589-comment2-reply7: give maxiter a try!
post590: Do we need to find std of all time domain features extracted separately for train and test datasets or all datasets together?
post590-comment1-reply7: <md>@316</md>
post591: Where (or when) can we see the recording of the 3:30 session today (Oct 4)?
post591-comment1-reply7: <md>The last time there was a joint session, the video eventually showed up in the panopto videos on DEN. It might take a bit longer since the 3:30 class is technically not a DEN session.</md>
post592: Hi, I'm noticing a handful of nans in my data after I create the new dataframe with min1, max1... min6, max6 etc. What's the expectation for handling this? Should I convert to 0s or handle elsewhere beforehand? Thanks!
post592-comment1-reply7: <md>@384. Could also be an issue due to linearly separable data if the number of features is larger than the number of samples.</md>
post592-comment2-reply7: I don&#39;t think you should be getting NAs in your data? Especially with min max since the original data has no empty values
post592-comment3: you might want to check the data formatting in the file. like the values are not algined in the columns properly 
post592-comment4: In order to combine columns they have to be equal lengths, that may be the culprit. Make sure your appending comes with equal lenghts
post593: On Monday professor mentioned we will be missing a class next week for fall break and that is why there was a joint session today, however it seems that fall break is only Thursday/Friday. Can the instructor team please clarify if we will have both lectures next week or only Monday?
post593-comment1-reply7: <strong attention="iyb66l86qwb64f">@Mohammad Reza Rajati</strong> 
post593-comment2: I misspoke. I mixed up Wednesday and Thursday classes. So, we will have two meetings next week. However, having a joint session one week before the break was a good idea, we managed to finish lesson 5 and students will have enough time to review the lectures during the break if they have any backlog.
post593-comment2-reply1: thanks professor! 
post594: I'm trying to use the Scipy library to perform bootstrap, but I'm getting this error ImportError: cannot import name 'bootstrap' from 'scipy.stats' And I also couldn't use bootstrapped library as well. anyone has the same issue?
post594-comment1-reply1: try reinstalling the packages?
post594-comment2:  samples = res_df['avg_rss12_min'] bs_std = bootstrap(samples,stand_dev,confidence_level=0.95,random_state=1,method='percentile') bs_std am getting the following error - ValueError: each sample in `data` must contain two or more observations along `axis`. Am using scipy version - 1.11.3 . Am not sure where am going wrong. Any idea ? 
post594-comment2-reply1: Scipy bootstrap requires at least two values along the axis that the statistic is being calculated on, so it is warning you that only a single value is provided. It seems like you are providing a single column so you might want to try setting `axis=1` so that it calculates along the column.
post594-comment2-reply2:  import bootstrapped.bootstrap as bs import bootstrapped.stats_functions as bs_stats I used this library and was able to move forward
post594-comment2-reply3: @Scale - I tried doing that - but it didn't quite work
post594-comment2-reply4: make sure that you convert the data into the data type 'sequence.' was getting the same error before I did thatex from the documentation: 
post594-comment2-reply5: thank u
post595: If I rerun I get different p-values for the same 'L'. Is it an acceptable behavior? Also, the accuracy differs by 2 or 3%. Again is it acceptable?
post595-comment1-reply5: If you want the same results, try setting a fixed random value
post596: What are we trying to predict in HW3? Are we predicting particular instance, or type of activity (sitting, lying, cycling, etc.)?
post596-comment1-reply5: There is no predictions of classes in HW 3, that&#39;s for HW 4. HW 3 is mainly just exploring the data space. This homework uses time series data which is a bit different than the previous homeworks.
post596-comment2: I'm not sure I understand this question if there's no predictions How are we supposed to gauge the importance of features if we don't know what we're trying to predict?
post596-comment2-reply1: The previous step was to estimate the standard deviation and confidence intervals which implies those might be used to help you select. You can also look at HW 4 to see what we will predict and be influenced by that. Although based on instructor responses @359 and @314 I think the most important part of your answer is your explanation as to why you judged them the three most important features.
post597: https://usc.zoom.us/j/94051898406?pwd=K2wrcWxFem5KYVhDdkt3Mk5XcE5EUT09 
post597-comment1: zoom link not working
post597-comment1-reply1: same
post597-comment1-reply2: you need to open it in safari if you are on mobile
post597-comment1-reply3:  A lot of students are already attending via Zoom. M R Rajati’s phone 
post598: for c(iii) - it says use Python’s bootstrapped or any other method to build a 90% bootstrap confidence interval for the standard deviation of each feature....i understand that i need to find std dev of each column - but then how do i use it further for bootstrapping ?
post598-comment1-reply3: Like the instruction mentioned, you can use the packages, please refer to the documentations<div><br /></div><div><a href="https://pypi.org/project/bootstrapped/">https://pypi.org/project/bootstrapped/</a></div><div><br /></div><div><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a></div>
post599: Should we calculate the confusion matrix, ROC, and AUC of multi-class classification on training data as similar to (a)iv? Or should we calculate those on test data?
post599-comment1-reply3: Do it for the test, since the sentence just before this talks about the test error
post600: Hi,I wanted to confirm the syllabus for midterm exam. According to the syllabus pdf file will it be the below topics that are covered until October 18, 2023?Introduction to Statistical Learning Linear Regression Classification Resampling Methods Linear Model Selection and Regularization Tree-based Methods Support Vector Machines Unsupervised Learning: only K-Means Clustering and Hierarchical ClusteringThanks
post600-comment1-reply3: It is whatever professor has taught in class, its all fair game, refer to the lectures
post600-comment1-reply3: It is whatever professor has taught in class, refer to the lectures
post600-comment2: Professor has mentioned in a lecture that syllabus is 99% until lesson 5. Could you please confirm this? 
post601: So far, I have generated 20 data frames on time domain features, one for each L value. I now am trying to run a logistic regression on each data frame to find the p values, but I'm running into a problem. Here is the error I'm getting: --------------------------------------------------------------------------- LinAlgError Traceback (most recent call last) Cell In[85], line 23 20 # Fit logistic regression model using statsmodels 21 model = sm.Logit(y, X) ---> 23 result = model.fit_regularized(method='l1') # Lasso regularization 26 # Print summary which includes p-values 27 print(result.summary())After doing some research, I discovered that LinAlgError: Singular matrix, suggests that there's an issue with multicollinearity in the data.Can you please advise me on what I should do? Thanks
post601-comment1-reply3: <md>Another student had a similar error in @384. It could also be an issue due to linearly separable data if the number of features is larger than the number of samples.</md>
post601-comment1-reply3: Another student had a similar error in @384. There might be an issue with how you set up the dataframe since it should not be creating a singular matrix.
post601-comment2: I'm having the same issue. Did you end up solving this?
post602: Can someone please post the Q2). The pdf for ISLR is not loading. Thank you. 
post602-comment1-reply3: Try opening ISLP it&#39;s the same !!!!
post602-comment2: 
post602-comment3: Screenshot_2023-10-04_at_12.05.45_PM.png
post602-comment4: Thank you 
post603: Hi Class, We will have a joint session tomorrow. This means all students need to attend/ watch both 12:00 and 3:30 lectures. Students who attend the 3:30 lecture must first watch/attend the 12:00 lecture. Both lectures will be recorded and will be made available on DEN. I will announce the Zoom link for the second lecture if you want to attend online at 3:30. If you want to attend the lecture in person, the location is THH 201. Regards, M R Rajati
post603-comment1: Will both sessions be recorded? I am part of the 330pm lecture
post603-comment1-reply1: The note says both will be recorded!
post604: I'm a little bit confused about performing cross validation on feature selection. When we apply forward stepwise selection to select features, we will create a list of models and each one has different number of features. The slides say we can perform cross validation to find the single best model. But it means, for a given model, we determined what features should be used before we do cross validation, which is not the right way to do it, right? Can anyone elaborate more on this part? 
post604-comment1-reply1: You should not do feature selection before doing cross-validation (leads to leakage), instead do it during cross-validation. As the HW says, there is a right way and a wrong way to do it. You need to figure out what the right way is.
post605: According to the question 4a(iii) Use 5-fold cross-validation to determine the best value of the pair (l, p) Which score should we use to measure among these models? Should we use F1, Accuracy, or else?
post605-comment1-reply1: that is up to you
post606: Hi, for HW4 (a) i, ii, it is explicitly stated that we should only use time series 1, 2, and 6, and the three time-domain features specified in HW3 1 (c) iv. For HW4 (a) iii and the following questions, are we supposed to use all the time series from 1 to 6, and the seven time-domain features(min, max, mean, median, std, first quartile, and third quartile), or just the same as HW4 (a) i, ii? Thanks.
post606-comment1-reply1: Use all 6 time series when the question says &#34;for each time series&#34;, but only use the 3 features you have selected
post606-comment2: Let me double check, so for (a) iii, it is still required to use 1,2 and 6. Correct? Thank you!. 
post606-comment2-reply1: I think it should be all 6 series since at the beginning of (a)iii it says "break EACH time series."
post607: Hi, I am confused about the scatter plots of HW4 (a) i. Will it be a 9*9 pairwise plot or just a single plot where the x-axis is the feature name of 9 features and the y-axis is the value of the feature? Which one should be right? I have attached the images of these 2 kinds of plots. Thanks. 
post607-comment1-reply1: I did a 9x9 pairwise plot. I think your first image is what is being asked for.
post607-comment2: If the answer is 9*9 pairwise plot, does it mean we should plot an 18*18 pairwise plot for HW4 (a) ii since the number of features double in this question? Thanks.
post607-comment2-reply1: I believe so. 
post607-comment2-reply2: Is it possible that we need to draw 3 3*3 scatter plots?(each one represents one kind of time series?)
post607-comment3: You mean like this? 
post607-comment4: When I used sns.pairplot() it has this error: LinAlgError: singular matrix. It seems that the matrix has high correlation data...
post607-comment4-reply1: Did you remove timestamp columns? I got the same problem but gone with when I removed timestamp columns obviously they are correlated.
post608: Hi, I went through the textbook page 143 ( Linear Discriminant Analysis for p = 1 ). In it when calculating the discriminant function ($$\delta_k(x)$$) then we have assumed that $$f_k(x)$$ is normal or Gaussian. Afterwards we got the discriminant function after taking log of $$p_k(x)$$ and maximing it. My question is that if we don't assume that $$f_k(x)$$ is normal or Gaussian and let say we have some (function of $$\sigma$$ , $$\mu$$, and $$x$$ ) $$f_1(x)$$ and $$f_2(x)$$ with only one predictor for simplicity. Then how to calculate the discriminant function. Will it make sense if I do the following: 1. find ($$\delta_1(x)$$) and ($$\delta_2(x)$$). 2. add them to find the final result Can someone please let know what can be done in such a case. Thank you 
post609: for this problem, i meet the error about non-convergence in logisticRegression. I know we should modify the max-inter parameter to solve this problem. But how big should we set max-iter to be reasonable?
post609-comment1-reply1: When I researched about it online, I saw people say somewhere between 100-10000. I believe 100 is the default option most of the time.
post610: Hi instructors, I noticed that in the syllabus we are allowed a 6-day grace period for late submissions to homework. How do I keep track of how many days I have left for this grace period? Thank you.
post610-comment1-reply1: It is actually late days not grace days, grace days are for the 3 days where -10% each day. In terms of tracking it you will just have to remember, I trust that 6 isnt too many days to do that with!
post611: I split each column in file that was designated "training data" in the homework in 1,2,3,....20. I built the logistic regression and I'm getting a "LinAlgError: Singular matrix" error. In addition, all my p-values are coming out as 1's or NAN in the model. Is this possibly due to the fact that there are "only" 69 training files and when splitting the rows you get 18 features (3 features* 6 time series), then 36 features (3 features *12 time series)..... up to 360 features (3 features times 120 time series). My data is clean there are no "NAN" values. Perhaps I misunderstood this part of the question: "Break each time series in your training set into l ∈ {1, 2, . . . , 20} time seriesof approximately equal length and use logistic regression5to solve the binaryclassification problem, using time-domain features. Remember that breakingeach of the time series does not change the number of instances. It onlychanges the number of features for each instance. Calculate the p-values foryour logistic regression parameters in each model corresponding to each valueof l". Any clarity would be greatly appreciated.
post611-comment1-reply1: It sounds like you have an error in the data, if you can&#39;t figure it out try OH!
post611-comment2: I do have same issue, What is OH?
post611-comment2-reply1: Office Hour
post612: Hi, For this question, are we going to split each dataset in the training set into two pieces and calculate their min, max, std, and etc.? For example, for bending 1 dataset 3, since there are 480 rows, should we separate it into two parts by considering the first 240 rows as part 1 and the rest as part 2 and then compute min, max, std, and etc. for them separately? Thank you!
post612-comment1-reply1: correct @374
post613: Hi instructors, when reviewing ISLR, I found that it says: When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other.I can understand that we'll train highly correlated model based on almost identical observations, but why the outputs are highly correlated? In LOOCV, each iteration will have a distinct test set. How can we conclude that based on different test set, we gain highly correlated outputs?Also, I cannot actually understand what this 'outputs' refers to. Is it refers to MSE in each iteration of LOOCV? 
post613-comment1-reply1: Because the prediction is not only a function of the test set. It is a function of the model, which is based on training data.<div><br /></div><div>PS: This is a public question. Please stick to the rules of the syllabus that ask to use private posts only when it’s about an issue that is specific to you. Make the post public.</div>
post613-comment2: Thank you professor. By the way, what is the meaning of highly correlated prediction (or test error), can you provide an example?
post613-comment2-reply1:  Watch the lecture. I provided an example in which all random variables were equal (so 100% correlated). In LOOCV, all n models are so similar, so their predictions are highly correlated, even though they have different test sets. M R Rajati’s phone 
post613-comment2-reply2: Hi professor, I have watched the lecture before and was able to understand that example. However, I have some thoughts in the statement that ‘n models are so similar, so their predictions are highly correlated’. In my understanding, the prediction is also the MSE of each iteration, and the correlation between predictions means that if all n models change, all the MSEs in each iterations will also change to similar extent, is that correct?Thank you so much for your reply!
post613-comment2-reply3:  Not sure if we are communicating effectively here. See me after class. M R Rajati’s phone 
post613-comment2-reply4: Hi professor, I think I understand the logic. Since the models are so similar, when we obtain predictions, the test data will follow the pattern of each highly correlated model, so the predictions of each model are correlated. I hope I correctly understand it this time😃
post613-comment2-reply5:  I think you got a good understanding! M R Rajati’s phone 
post613-comment2-reply6: Thank you so much for your patient reply!😄
post614: Hi, class. As we're getting into homework 3 and 4, we're going to start getting into some significant computation times, so I wanted to recommend a extremely useful module called 'tqdm' that I used when I was doing 552 homework and for the final project. What this module does is replace your for loops with something very functionally similar, but also prints a dynamic loading bar at the bottom of the cell of your notebook or in the CLI output of your terminal. It allows you to monitor the progress of your notebook's execution, give you an ETA on completion and reassure you that your code didn't crash, it's just taking a while. And just to re-emphasize, none of this is a requirement. This is just a helpful recommendation for future homework going forwards. https://tqdm.github.io/
post615:  Is the following output considered a valid output for the question? Thanks
post615-comment1-reply6: that looks like a CI to me
post616: for this question, we split training set into two (approximately) equallength time series. Now instead of 6 time series for each of the training instances, we have 12 time series for each training instance. So how would our columns be like? for instance. our inital columns are: 'avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23' after split into 2 would it be like: 'avg_rss12_part1','var_rss12_part1','avg_rss13_part1','var_rss13_part1','avg_rss23_part1','var_rss23_part1','avg_rss12_part2','var_rss12__part2','avg_rss13__part2','var_rss13__part2','avg_rss23__part2','var_rss23__part2' is this the correct way to do?
post616-comment1-reply6: and what does it mean two (approximately) equal length time series? Like would the value for avg_rss12 of 1 instance be split into 2 and would be stored in avg_rss12_part1 and avg_rss12_part2? 
post616-comment2-reply6: This is the idea yep!
post616-comment3: How should we deal with files that have an odd number of observations and can't be split evenly? 
post616-comment3-reply1: notice the instruction says "approx" same size
post616-comment3-reply2: Then our data will have some NANs since we're keeping the number of observations the same. Won't that mess up the data further down?
post616-comment3-reply3: The time domain features are essentially summarization of the data, so it won't be affected. If you take a look at the table example in 1.c.ii, you will see that.
post616-comment3-reply4: Oh I didn't understand we were only going to use the summary data and none of the raw data. That makes sense. Thanks!
post616-comment4: Having dividng all 6 time series, we only need to plot for time series 1,2, and 6?
post617: I realize that cycling datasets 9 and 14 need to be fixed, however, I am having a hard time detecting what the issue with these files is when I open them up in Excel. Pretty straightforward issue but could use some pointing in the right direction :)
post617-comment1-reply4: I had the same issue, you have to open it in a text editor like notepad to see the errors
post617-comment2: I found out the issue in these files. Both of these datasets have a comma at the end of the dataset as given in the image screenshot below. Just remove them and you would be fine.
post617-comment3-reply4: Yes, that&#39;s correct. These both files have a comma at the end. Note, this is a common type of issue that we data scientists face while handling CSV files. Therefore, it&#39;s a part of this assignment to learn such types of data handling procedures. Here, you can either just open the file and edit it by removing the extra comma and save it for further use. Or, one may write a code in their script that will automatically handle such exceptions while reading the file, when they occur. It&#39;s upto the student which one they want to do. 
post618: Since hw 4 is partially based on hw 3's setup, will we get any feedback/ pointers to make sure that we were on the right track for hw3? That way we are set up for success on hw 4 regardless of how we did on hw3? thank you.
post618-comment1-reply4: Unfortunately, that is not possible due to the grading timeline. Also that is why HW3 is given 2 weeks so you can get started on HW4 right after you finish
post618-comment2: Is it okay if I change my selected features from HW3 in HW4?
post618-comment2-reply1: that is ok, just stay consistent within HW4
post619: Hello, For problem c iii, I used the bootstrapped package as the homework problem recommended but when it returns a result, it is in the form of a "BootstrapResults" object and displays the estimate and the confidence interval. Is there a way to select such that you only get the confidence interval? You aren't able to do traditional indexing because it is a different type of object. In addition, how should be display the confidence intervals for all of the features? 
post619-comment1-reply1: bootstrapped.bootstrap.bootstrap() should just return a printable CI value. And you should display it in a table like a pandas df
post619-comment2: I am having the same issue where the results from my bootstrapped.bootstrap.bootstrap() produces something like:9.515445066931843 (8.263039213839253, 10.812056798847726) where the first number is the standard deviation of the first feature, followed by the low CI and then the high CI, but I am not sure how to produce this into a pandas dataframe.
post619-comment2-reply1: That is ok, you can just store that directly
post620: After going through the dataset, I found 3 files that need cleaning. Could someone confirm this number?
post620-comment1-reply1: I think I cleaned datasets 9, 14 from cycling <br /><br />edit: bending2 dataset 4 as well
post620-comment1-reply1: I think I cleaned datasets 9, 14 from cycling <br /><br />and did not get any errors after that 
post620-comment2: I think bending2-dataset4 also has issues.
post620-comment2-reply1: Oh bet thanks!
post620-comment3: What issue did you run into with bending2-dataset4? Mine has been running fine and when I look at it manually I can't seem to see what the problem is. 
post620-comment3-reply1: Wait never mind I see that it's space separated, not comma separated now!
post621: Hello class, I am postponing my today's OH to Tuesday 6pm -8pm. I am currently having network issues as I am out of town for a conference. Sorry for the inconvenience . Thank you, Vedanvita G
post622: For some reason on of my rows is coming up empty in my extraction table... as someone not familiar with cs... is there any way to easily check why? thank you
post622-comment1-reply1: @300 might render some problem of yours.
post622-comment2: Have you properly cleaned all of your files? Check out @296. 
post623: Aren't metrics like mean and std dev normalized features? 
post623-comment1-reply1: mean and std dev are metrics/summaries, they are not normalized features
post623-comment2: so for example subtracting every value of avg_rss12 by its mean and dividing by std dev? 
post623-comment2-reply1: sure that would be one of the simple ways of normalizing!
post624: If we are finding the k that minimizes the negative loss, I don't understand in what scenario we would be using macro precision or recall. Or is it some kind of metric we can use to choose a model? 
post624-comment1-reply1: <p><a href="https://stackoverflow.com/questions/68708610/micro-metrics-vs-macro-metrics" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/68708610/micro-metrics-vs-macro-metrics</a></p> <p></p>
post625: I just wanted to clarify whether it's optional for us to complete the extra practice questions or if they are required.
post625-comment1-reply1: They are called <strong>EXTRA </strong>because they arent required
post626: How are we supposed to pick the most important features (such as min, mean, and max) if there is a min, mean, and max standard deviation for each time series? For example, in the table below for time series #1 the standard deviation of min and mean are high, while for time series #2 the standard deviation of min and mean are low. When you say that you want us to pick the three most important time-domain features, can we pick the mean for one time series and the min of another? 
post626-comment1-reply1: How you want to pick is completely up to yourself! You should be picking 3 features overall
post626-comment2-reply1: do we have to include the CI range in our C iii answer??
post627: hw3 | 1.c..ii - What is the size of the df we are supposed to output? Is it 88 rows x 43 columns? Where 1 column is the Instance, and there are 6 sets of 7 columns (min, max, mean, median, std, 1Q, 3Q)? Or is it 88 rows by 7 columns? Where 1 column is the instance and then there are 6 columns with each column having a list of the mins, list of all the maxs, list of all the means,... etc.? Thanks
post627-comment1-reply1: Please follow the instruction and separate out the columns instead of condensing into a single
post628: Are we using the 7th dataset in bending 1? All of the instructions say to do things for the 6 time series in each file 
post628-comment1-reply1: You are mixing up dataset vs time series, time series are the columns in a dataset
post629: Is there a solution key to HW1 from which we can learn from our mistakes? Most technical courses I have been in at USC have been curved, but I have not seen anything in the syllabus so I was curious? 
post629-comment1-reply1: I believe Prof stated in lecture there will be no HW solutions
post629-comment2-reply1: No HW solutions will be provided.<div><br /></div><div>It&#39;s only been the first HW, don&#39;t worry about your grades just do your best, I trust that prof has also said the same thing in class. And we believe in leniency. </div>
post630: Hi, So I got an error trying to do part 1.c.ii of could not convert string to float - particularly happens when im trying to find the mean - and so I tried to convert to numeric and still could not convert... is anyone having these issues? I assumed maybe I need to clean the data again but I have no idea how/ what im looking for... thank you
post630-comment1-reply1: you might have done something wrong while importing maybe, it would be better to go to an OH for more detailed debugging like this, we have OH almost every hour of the week!
post630-comment2-reply1: <p>I also encountered this issue.</p> <p>In case you wonder whether the cleaning is correct, I used this way to check non-numeric rows</p> <pre> #check if it&#39;s not numeric or null check_df = df.apply(lambda s: pd.to_numeric(s, errors=&#39;coerce&#39;).notnull()) #filter only rows containing false check_df[check_df.sum(axis=1).lt(len(check_df.columns))] </pre> <p>another guess is that you could have added a non-numeric column (probably the label) into dataframe before finding the mean. don&#39;t forget to exclude that too.</p>
post631: Do we have to use any particular library for bootstrap function like scipy or bootstrapped, or can we use any one of these?
post631-comment1-reply1: You are use any of them, if you choose an unpopular library make sure to include it in requirement.txt
post631-comment2: I also wondering how many samples. It seems like 10 is default from Bootstrap libaray whereas other sources use 1000 more commonly.
post631-comment2-reply1: the default should be fine, you can decide that on your own
post631-comment3: error
post632: Hello, I was just hoping to get clarification on the formatting of our submission, specifically for the cell outputs in the Jupyter Notebook. For HW1, I was deducted points because I had cleared all the cells' outputs and instead attached a PDF that showed all the outputs in HTML format. Is this wrong? I couldn't find anywhere explicitly in @40 where I can't submit it this way, but I just want to make sure. Thank you!
post632-comment1-reply1: You should not use a separate file, just use the notebooks as the instruction says and run all cells from the start before submission
post632-comment2: I used the same formatting for HW2, will this result in a 0? I thought that the "cell -> run all” function was more for proofreading the code because it states that the graders will be running the code anyways?
post632-comment2-reply1: you will prob be deducted points for this yes, and the run all is for the notebook that you are submitting as the rule states
post633: We have posted the grades for HW1 on DEN. We will accept regrade requests until 10/03/2023. Make a private post on Piazza if you need a regrading.
post633-comment1: Can't find anything in'Grades' section of DEN. Where to see the grade for hw1?
post633-comment1-reply1: Your grade is showing as published already
post633-comment1-reply2: I also cannot find my grades under the “Grades” tab. It shows “No items found”.
post633-comment1-reply3: Check DEN > My Tools > Assignments
post633-comment1-reply4: Check in Assignments tab
post633-comment1-reply5: Thanks! I have seen it.
post633-comment2: I am not able to see my grade. If I go to grade page, it says no item found. 
post633-comment2-reply1: See above student replies, Check DEN > My Tools > Assignments
post633-comment3: Hi, I am not able to see my grades under DEN > My Tools > Assignments. 
post633-comment3-reply1: Looks like your assignment was missed by the grader, it is being graded now
post633-comment3-reply2: Thanks for letting me know. I still can't see my grade though.
post633-comment3-reply3: it is still be graded, should be done by tmr
post633-comment4: Hello,I am not able to see my grades under DEN > My Tools > Assignments.
post633-comment4-reply1: Looks like your assignment was missed by the grader, it is being graded now
post633-comment4-reply2: Thank you for letting me know, however I still can’t see my grade.
post633-comment4-reply3: it is still be graded, should be done by tmr
post634: Are we supposed to complete the hw3 till Q2 as per the pdf. ans the hw4 pdf and hw3 is little different both has different allocation for question in both pdf. 
post634-comment1-reply3: Correct, but feel free to get started on HW4 on your own since it is longer than HW3
post634-comment2: so do we follow hw4 PDF from which part and for HW3 which part it's confusing. HW3 as per PDF only 1 and 2 is required right ? And for hw4 the parts are different
post634-comment2-reply1: HW3 is part 1, HW4 is part 2, the PDFs are essentially the same
post635: Hi, for page 120 of L3, it says "for Gaussian this means Σk are diagonal". If it is naive Bayes, features in class k are independent, then Σij = Cor(Xi, Xj) = 0 for i ≠ j, so Σk are diagonal. But is it still true for non-gaussian? Since when it is Naive Bayes, this property should always hold, right? Once independence holds, uncorrelation always holds, right? Then why it mentions "for Gaussian this means Σk are diagonal." Thanks. 
post635-comment1-reply1: <p>Independence means uncorrelatedness. But only for Gaussians, they are equivalent. The covariance matrix may be diagonal for non-Gaussians, but they may still be dependent.</p> <p></p> <p>Independence implies diagonal covariance matrix for all random variables.</p> <p></p> <p>Diagonal covariance matrix implies independence for Gaussians, but not necessarily for other random variables.</p>
post636: submitted my assignments on time but when check it in DEN-> my Tools -> Grades, I see HW 0 not submitted and HW1 and HW2 not listed only. Earlier I could see, I got full marks for HW 1.
post636-comment1-reply1: Grades are still being updated no need to worry
post636-comment2: I could see my HW1 grades earlier. Thats why I'm worried.
post636-comment2-reply1: please see response above
post636-comment2-reply2: Yes and I replied with additional concern
post636-comment2-reply3: Once again grades are still being updated no need to worry!
post636-comment2-reply4: Okay thankyou
post637: For c(iii) and c(iv), should we use the whole data (both test & train) to do the estimate sd, CI, and then feature selection, or should we use only the train data?
post637-comment1-reply4: @307, all data
post638: Hi, I created separate test and train data frames by looping through all folders and file names according to directions in 1B. can I validate that my data frames are the right shape? My training dataset has 33,119 rows and 8 columns, and testing has 9,120 rows and 8 columns. One of my columns is the name of the activity I grabbed from the dataset's folder, which is why I have 8 instead of 7.
post638-comment1-reply4: You can do a quick eatimation urself, there are total of 15*5&#43;6&#43;7=88 files, each has around 480 rows, thats estimated total of 42240 records
post638-comment2-reply4: <p>According to @300, all data sets should have 480 rows, except the sitting/dataset8.csv which has 479 rows</p> <p>so i guess</p> <p>- training one should have 69 files * 480 rows - 1 = 33119</p> <p>- the testing one should have 19 files * 480 rows = 9120</p>
post639: Hi, How do we show our reasoning for selecting the three most important time-domain features? Do we need to use plots or test some regression models? Is writing a paragraph to explain our reasoning enough for this question? Thanks!
post639-comment1-reply4: A paragraph along with the CI generated previously is enough
post640: how did we upload all those datasets -- I'm just confused about how the directory structure should be. The instructions mention which datasets to be for training and testing but I'm not sure how to split that up in the directory. Does each activity folder have separate folders for training datasets and test datasets? Are we supposed to then merge all of the datasets in the assignment?
post640-comment1-reply4: You don&#39;t have to edit the directories by hand at all, that is very tedious. Instead just use a column in your df to indicate train/test, or have two df (one train one test)
post640-comment2: Okay, thanks. And then also just confirming, datasets 1 and 2 in bending1 and bending2 folders are test data, and all the other datasets in those folders will be for training?
post640-comment2-reply1: that is what the instruction says
post640-comment3: We only use test and train datasets in (b), and we can just display the train/test datasets file paths. In (c) we don't need to split the datasets to training or testing because in (c)ii the table has 88 rows. So should we display the label(train/test) in the df in (c)?
post640-comment3-reply1: For the questions asking for all the data, you can either combine the tables or just read them together
post641: HW3 ciii states that we need to build a 90% bootstrap confidence interval for the standard deviation of each feature. How many bootstrap samples should we use?
post641-comment1-reply1: The only requirement is for it to be 90% CI, everything else is up to you
post642: Not sure if there was an easier way to read in the data, but I didn't get any "errors" and I didnt edit any of the files yet. By "errors" do we mean that the code should have thrown an error or is there a way to find out where those errors might be?/ what errors should I look for? Thank you!
post642-comment1-reply1: I did get errors for 2 files while reading csv.<div>You might also get errors (example - extra comma in csv file) in later parts of questions if no errors were encountered earlier. </div>
post642-comment2-reply1: You will 100% encounter errors, either as form of code terminating, or your data being incorrect, you will likely spot the second form of errors when you do the rest of the tasks
post643: Hi, For this problem, are we supposed to estimate the standard deviation of min_1, max_1, mean_1, median_1, ..., 1st quart_6, and 3rd quart_6 or we should estimate the standard deviation of min, mean, median, std, 1st quart, and 3rd quart for the 6 time series? Thanks!
post643-comment1-reply1: Its the stdev of of each of the columns for the df that you created in c.ii
post643-comment2: for question (iii) we have 88 standard deviation for each feature and the question ask us to make an estimate of the standard deviation for each feature. Is is best to report the mean?
post643-comment2-reply1: you can use the bootstrapped package or scipy as the question mentions
post643-comment2-reply2: So basically the standard deviation thing is done in previous question, we don't need to give an average of standard deviation for each features regarding the 88 files? We only have to figure out a confidence interval.
post643-comment2-reply3: iii does not say to find the average
post644: Just want to make sure i get the idea of the question: although in part b it asks us to split the dataset into training and testing, in part C ii, it wants us to find the features regardless of training or testing, just by dataset and column. For example, min1 at instance1 could mean the minimum value of bending1 dataset1 at column avg_rss12? Thank you
post644-comment1-reply3: Yes this would be on all the data, instance means for each file, and the number for each metric just needs to be consistent and meaningful (as in you know which number points to which measure)
post644-comment2: Should I display the result in the file order(from bending1 folder to walking folder, with dataset in increasing order) or random order is fine?
post644-comment2-reply1: That does not really matter
post644-comment3-reply1: <p>Another question -- is this an acceptable format for the columns in Part C ii? below are example column names. this just makes more sense in my mind versus the &#34;min1, max1...&#34; example given in HW </p> <table border="1"><thead><tr><th>min(avg_rss12)</th><th>max(avg_rss12)</th><th>mean(avg_rss12)</th><th>median(avg_rss12)</th><th>std(avg_rss12)</th><th>1st quart(avg_rss12)</th></tr></thead></table>
post644-comment3-reply1: <p>Another question -- in the table format that is presented on c ii, there is nowhere to identify which file/folder belongs to each instance. So we do not know which activity is related to which instance. Is that okay? Wouldn&#39;t it be useful for later questions to know which instance is for which activity? or is this features table just trying to get us familiar with the time-domain features and working with them rather than using it later on for modeling.</p> <p></p> <p>could i name my instance by dataset / activity instead of numbers? (i.e. &#34;bending1/dataset7.csv&#34; instead of instance 1?</p>
post644-comment3-reply1: Another question -- in the table format that is presented on c ii, there is nowhere to identify which file/folder belongs to each instance. So we do not know which activity is related to which instance. Is that okay? Wouldn&#39;t it be useful for later questions to know which instance is for which activity? or is this features table just trying to get us familiar with the time-domain features and working with them rather than using it later on for modeling.
post645: Could you tell me what the Final Project is? Is there any presentation? Do we need to be in class after Midterm 2?
post645-comment1-reply1: The final project will be after the midterm 2, it will be like another HW just slightly longer, more details to come
post645-comment2: No presentation. It's like an extended HW. It will be released after Midterm2, according to the syllabus.
post646: Hello, I notice that for Homework 3, there are several different activities that are being measured, bending, cycling, sitting, etc and the instructions suggest that we should combine parts of the datasets in each of these activities into one test dataset and one train dataset. My question is if there is some underlying part of the data that points to what category each measurement belongs to(for example if 'avg_rss23' is for cycling vs. sitting? If this isn't there, how would the model be able to differentiate between measurements belonging to a different class? Would it be necessary to add class labels for these datasets?
post646-comment1-reply1: all the datasets have the same features, if that is what you are asking, how you want to label them is up to you as long as they are meaningful
post646-comment2-reply1: I created an extra column named &#34;activity&#34; and just took the folder name and saved it for each dataset so I have a record of which activity belongs to which rows of data. I assume we will need to be able to differentiate between them for modeling later
post647: Hi, When I used read_csv to read the Cycling datasets 9 and 14, I got an error saying"ParserError: Error tokenizing data. C error: Expected 7 fields in line 485, saw 8". If I skipped the bad lines when reading the csv file, I got 479 rows instead of 480 in both datasets 9 and 14. I'm wondering if we can skip the bad lines in the dataset. Thanks!
post647-comment1-reply1: You can try to open dataset9.csv and dataset14.csv with text editor. And then you can find that there is a redundant comma at the end of the last line. <div>I tried with manually delete these commas in text editor , save the deletion, and ran the read_csv() again, then it worked.</div>
post647-comment2-reply1: @296
post647-comment3: Just to make sure, are we allowed to delete these commas in text editor ourselves? Or maybe we need to write code to skip bad lines? Thanks
post647-comment3-reply1: If you check the post @296, you can fix the error either manually on excel or python
post648: Has the 479 rows whereas the other dataset has 480. How do I handle this? Thank you!
post648-comment1-reply1: You can proceed with 479 rows as is
post649: Hello, May I know when the grades for HW1 will be released?
post649-comment1-reply1: @250
post649-comment2-reply1: Professor said that you gonna have your grade by this Wed 27th
post650: I submitted HW2 prior to the deadline but noticed that I had accidentally uploaded the incorrect version of my homework. Consequently, I made the necessary edits, modified some code, and resubmitted it today. I'm curious if this is acceptable, or if there will still be a 10% penalty imposed on my homework.
post650-comment1-reply1: It is graded based on the last submission, so there will be a deducton, but u can choose to use a late day
post650-comment1-reply1: It is graded based on the last submission, so there will be a deducton
post650-comment2: Thanks so much, Shawn, do I need to email to TA or mention that I want to use a one-day-late policy for this assignment?
post650-comment2-reply1: It should be automatic, but its always best to provide the info in the notebook when u make ur final submission. Or you can also open a private piazza post just to make sure
post651: What should we do if we encounter files that have incorrect/different formats? Should we try to fix them or skip them? An example is bending2 dataset4
post651-comment1-reply1: Please read the instructions carefully, footnote 1 says &#34;1Some of the data files need very minor cleaning. You can do it by Excel or Python&#34;
post651-comment2-reply1: There were 3 sheets that the data were separated by spaces instead of commas.
post651-comment3: are u sure there were 3 datasets? i see only 2 - cycling - dataset9 and 14
post651-comment3-reply1: Got the third file and the error also. 
post652: Hello I am having issues loading the data for Homework 3. When I try to use the read_csv function of Pandas, it will only display the task, frequency, and clock elements and not any of the columns that we actually need for the analysis. Is there a good way to try to address this in the code? 
post652-comment1-reply1: You might want to look deeper at the pandas documentation for skipping top rows<div><br /></div><div><a href="https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html">https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html</a></div>
post652-comment2-reply1: <md>Hi, parameter ```skiprows``` in ```pd.read_csv()``` can be helpful. Sample code is like: ```python # Define the row number from which you want to start reading start_row = 10 # for example, to start reading from the 11th row # Read the CSV file df = pd.read_csv('filename.csv', skiprows=range(0, start_row)) ``` In the homework 3 data, the first 4 rows (# Task: , # Frequency (Hz): 20, # Clock (millisecond): 250, # Duration (seconds): 120) can be skipped. Thus, you can try with setting the ```start_row=4``` in the above sample code.</md>
post652-comment3: Do we need to save the top 4 rows for anything?
post652-comment3-reply1: no
post653: Can SMOTE do down-sampling , did the teacher mean SMOTE-pro versions in the lecture?
post653-comment1-reply1: SMOTE can do down-sampling. As lesson 3 slide 61 mentions: &#34;SMOTE can down-sample cases from the majority class via random sampling&#34;. Slide 62 also gives some examples.
post653-comment2: Got it , Thank u! > [The combination of SMOTE and under-sampling performs better than plain under-sampling.](https://arxiv.org/abs/1106.1813)
post654:  Should we describe our findings in these questions (e,h,i)? or only plotting the results is enough? Thx
post654-comment1-reply1: I did a short description of my findings. 
post654-comment2-reply1: Describe your findings when the question asks you to
post655: If the sample size is small, can we still use Z-test in logistic regression / or T-test in linear regression ?
post655-comment1-reply1: The general principle is to keep sample size above 30
post655-comment2: It is known that if you draw both distribution, and you changed degree of freedom gradually, after 30 df then the difference became negligible, which is both trailing thickness of bell curve becomes pretty much the same.
post656: Hi, I ran my hw yesterday and it worked fine using the code as the pic shows but today when I tried to run it again, it pops an error says it not callable anymore. Can someone help me with it? Much thanks!
post656-comment1-reply1: Have you created another variable called &#34;range&#34; anywhere earlier in your code and initialized it as a list? That might be causing this issue.<div><br /></div><div>@268</div>
post656-comment1-reply1: Have you created another variable called &#34;range&#34; anywhere earlier in your code and initialized it as a list? That might be causing this issue.
post657: I want to avail one grace day for the submission of HW2. Is there any procedure which needs to be followed or is it okay if I just upload my code in the github repository after the regular deadline?
post657-comment1-reply1: I remember the prof saying that it will just automatically be deducted and to keep track of how many you have 
post657-comment2-reply1: There is grace day and late day, grace day is 10% off for 3 days, late day is what you are talking about.<div><br /></div><div>It is ok to just upload it, but just to be safe, put how many days you want to use on the top of the notebook.</div>
post658: This is regarding the regression model involving all possible interactions and the removal of insignificant terms using p-value analysis. In the case of original predictors (linear terms), if some of those predictors are statistically insignificant, should we remove the original predictors, or should we retain them? E.g., Suppose the (AT) column has a p-value greater than 0.05. Do we remove the original predictor (AT)?
post658-comment1-reply1: @233
post658-comment2-reply1: @233
post659: Hello, as we know that we could use p-value to get the statistically significant of one single predictor. 1) Does that mean if one predictor's p-value is large ( >0.05 ), we should not use this predictor in the Multiple Regression model ? 2) Does this conclusion also work in other models, like Random Forest, KNN or SVM ? I mean, if we use a single regression model to get a single predictor's p-value, can we use the conclusion for feature selection, like deciding whether or not using this predictor in other model ? 
post659-comment1-reply1: 1. If p-value is bigger than the pre-selected alpha (it can be whatever, but it has to be selected beforehand), then its insignificant and should be dropped<div><br /></div><div>2. No you should not select features this way in general</div>
post659-comment1-reply1: 1. If p-value is bigger than the pre-selected alpha (it can be whatever, but it has to be selected beforehand), then its insignificant and should be dropped<div><br /></div><div>2. No you should not select features this way</div>
post660: Can someone please explain the below slide? As in I am confused here with: 1) what is k and l here 2) Why are we taking the summation from 1 to k in the denominator 3) How come the probability for y=l/x = 1 here 
post660-comment1-reply1: <p>1. The class is the feature, and the l is the conditional marker. So it&#39;s solving for Pr(Y=k class, given X)</p> <p></p> <p>2. We take the sum because we want for all classes in the denominator formula.</p> <p></p> <p>3. The sum of probabilities is always equal to 1.</p>
post661: Hi all, I have some questions regrading the difference between flexible & inflexible models: In question(b): when the number of predictors p is extremely large, and the numberof observations n is small, I understand that a flexible model may overfit the data. While a inflexible model like linear regression may also underfit the data if the pattern is not linear. In which way should I compare the performance between them? 
post661-comment1-reply1: You just want to think of the general case which type of model would be better, you don&#39;t need a dataset to actually get the performance.<div><br /></div><div>You are correct on the thinking, but which one do you think would be better?</div>
post661-comment1-reply1: You just want to think of the general case which type of model would be better, you don&#39;t need a dataset to actually get the performance. 
post662: Hello I am trying to answer part j of the homework 2. I see that KNN Regression is performing better than the best Linear Regression model. I read that KNN Regression performs better when the data has high Signal To Noise ratio. Is this why we were asked to note down any outliers to remove in part c?Am I on the right track here? Is my understanding correct? Or is there a different reason for KNN to perform better that I am missing in my observation? 
post662-comment1-reply1: Maybe, just try your best at explaining it!
post663: Hi! I was wondering what the folder inside the data folder should be named as for HW2. The last time it was vertebral_column_data. Should we change it to CCPP for this HW? 
post663-comment1-reply1: yes thats ok
post664: Hello, I wonder if we should read in all the files (regardless of the type of activity) and combine them into big dataframe (two data frames, one for testing and one for training for all activities) , or should we read in files by activity? Ex: combine the csv files in bending and label it as bending_train bending_test… thank you 
post664-comment1-reply1: In the end its just up to what you prefer, as long as you feel comfortable and produces the correct results its fine
post665:  HIii all. I am confused about those dots in the plot. Are they supposed to be gathered together like this?
post665-comment1-reply1: Hello! This depends on what you are trying to plot. I recommend to check the sets that you are using for the plot.
post665-comment2-reply1: The dots are the real data points in scatterplot form, the line is the regression line. As the student answer points out, you might be using a wrong set of data here
post666:  In reference to the following "Test both models on the remaining points and report your train and test MSEs." Is that mean we will create two models: 1- The first one should be include all predictors. 2- Second one, should include all possible interaction terms and quadratic nonlinearities. but for the point regarding removing insignificant variables. Shall we remove them in model 2? I mean when creating the model itself based on p-value of the first model or after creating the model? 
post666-comment1-reply1: Hi! According to what I understood, we need to create two models, and based on the results from the first one, we need to remove insignificant terms for creating the second model!
post666-comment2-reply1: @237, @257
post667:  For the plotting, are we supposed to create 2 separate plots for each model (one for the normalized features, the other for the raw features)? 
post667-comment1-reply1: yes
post668: I accidentally added another folder, Homework 2, which includes data and notebook. I added the data and notebook separately again. It seems I cannot delete the homework 2 directory. Is it okay if I leave it like that? 
post668-comment1-reply1: Yeah that should be ok
post669: While training my knn model on normalized features, do we use normalized Y_train and Y_Test to predict values and eventually calculate MSE or do we use raw Y_train and Y_test?
post669-comment1-reply1: The instructions say you should normalize the features. That&#39;s why you only normalize the features and not Y.
post669-comment2-reply1: If you normalize Y then you lose the whole purpose of the data itself, there are no longer interpretable results
post669-comment2-reply1: If you normalize Y then you lose the whole purpose of the data itself, there are no longer inseparable results
post670: Facing a very weird error. Code snippet and error is below: predictor_names = ['AT', 'V', 'AP', 'RH']response = 'PE'n = len(predictor_names) interaction_df = df[predictor_names].copy()for i in range(len(predictor_names)): for j in range(i + 1, len(predictor_names)): TypeError Traceback (most recent call last) Cell In[19], line 6 3 n = len(predictor_names) 5 interaction_df = df[predictor_names].copy() ----> 6 for i in range(len(predictor_names)): 7 for j in range(i + 1, len(predictor_names)): 8 predictor1 = predictor_names[i] TypeError: 'Series' object is not callable I have tried to put n instead of len(predictor_names) or even hardcoded to 4 but I am still facing the same issue. I have tried restarting the notebook as well. Can someone help me.
post670-comment1-reply1: Refer Statmodels.formula.api for creating interaction terms, it is more simple than doing this. Though try using a debugger I guess the issue is in line 5 but it exits that line and shows error for line 6.
post670-comment2: Thanks for suggesting Statmodels.formula.api for creating interaction terms. 
post670-comment2-reply1: We love R
post671: Can anyone tell me where is the problem? The code is sometimes working while sometimes throwing out the error 
post671-comment1-reply1: Is this a particular cell in ipynb file. If you try re running that particular cell Everytime that may be the issue for getting this error.try running the cell after running the cells above.
post671-comment2-reply1: <p>If you are trying to make the range() iterator into a list you can just do list(range(x, y).</p> <p>And I recommend using something like <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" rel="noopener noreferrer">train_test_split</a> from sklearn for splitting train and test</p>
post671-comment3: Try using __builtin__.range() instead of range()
post671-comment4: I was struggling with the same error. I figured this could happen if we have a declared list with named keywords like 'list' or 'range'. I guess you might have a list 'range' for question 1 b) iii. This is also a reason why the code sometimes works and sometimes doesn't. The ideal way to avoid such errors is to not use named keywords for user-defined objects.
post672: Hello,In 1(i), do we have to perform KNN regression with the original predictors (AP, V, AT, RH), or should it also include the interaction terms and quadratic nonlinearities like in 1(h)?
post672-comment1-reply1: <p>Only the original predictors. Interaction terms and non-linearities from other parts of the homework only make sense in the context of linear regression. It&#39;s a way of handling deficiencies in the simple linear regression model. They should not be used in KNN since the KNN model is completely different and handles prediction in a different way.</p> <p></p> <p>This stack exchange might be relevant to your question: https://datascience.stackexchange.com/questions/97395/how-interacting-variables-known-in-statistics-as-moderating-variable-are-handl</p>
post673: As mentioned in the lecture, we can recover the class probability based on the $$\delta _{k}(x)$$ we get from LDA. And the formula is quiet related to the logistic regression function. So, in the perfectly separable case, will the parameter of x in $$\delta _{k}(x)$$ also go to infinity? If so, it means the data in class k will have a Gaussian distribution with close to 0 variance? In addition, does this similarity exist even we relax the assumption we made on the data distribution, say the data in class K is not necessarily Gaussian distributed?
post674: Hello, I know 1h asks for a model that uses 'all possible interaction terms and quadratic nonlinearities'. Does this mean that the equation would have interactions between the variable and the quadratic nonlinearities? For example, would our model also need to include 'V^2 * RH' or 'AT RH^2'? 
post674-comment1-reply1: @224
post675:  When we decide to remove insignificant variables using p-values, shall we remove it based on results in previous questions such as (g)?
post675-comment1-reply1: h and g are separate questions, you should be looking at the p-values for h
post675-comment2-reply1: I think we can use the backward selection to remove the insignificant variables. 
post676:  I'm little bit confused in (h) regarding the second model which contains all possible interaction terms and quadratic nonlinearities, shall we train the model using the same random train data in the base model? 
post676-comment1-reply1: I think so because the aim is to see if our new model is better or not. If you shuffle your data and not use the same random train data, then you can not compare them.
post677: Hello, In the textbook, in the Lab section, there is a package called "ISLP", are we supposed to use library in the HW? In addition, I figured out that the questions in mid-term exam sample are so different with the exercises in the textbook, but I am not good at Math >< Where can I find more exercises to plan for the mid-term exam? 
post677-comment1-reply1: ISLP is not required for any of the HWs, in terms of reviewing just look at textbook questions and past exams provided @12
post678:  Should we remove insignificant variables from both models?
post678-comment1-reply1: I believe the instruction say to use first one is using all predictors, but second one is to use all interaction terms and quadratic nonlinearities then remove the insignificant variable by inspecting the p-value. Then train the predictors without the insignificant variable again. The first one is just a baseline model for us to compare.
post678-comment2-reply1: Just the second model is ok
post678-comment3: Hi, here's another concern from me to remove the interaction terms. Let's say if our formula would be C = A + A^2 + B + B^2 + AxB I saw the P value for A and A^2 is really large, while A x B is extremely small. Can I keep the term AxB in the formula, or I have to delete them as well?
post678-comment3-reply1: @233 you can try removing them
post678-comment3-reply2: Thanks for your reply, just want to double confirm so it is legal to have the predictor in some interactions while there isn't any basic term of it in the formula? ex: C = B + B^2 + AxB (there is no A and A^2 in the formula) It looks kind of creepy compared with some of examples I saw lol.
post678-comment3-reply3: Once again the principle of keeping the base feature is ad doc, you can experiment around
post679: On Linear Regression slides, when comparing the model performance with interaction term and without an interaction term, we compare R2 values using this calculation: Interpretation—continued•This means that(96.8 − 89.7)/(100 − 89.7) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction. Can someone explain what's happening here? I get that the difference between R2 is in the numerator and the 100 in the denominator comes from R2 being a percentage out of 100, but why would we subtract both top and bottom by 89.7, from science when things change we look at new - old / old or 96.8 - 89.7 / 89.7 to measure the percent change, I've never seen this calculation before
post679-comment1-reply3: <p>The model without interactions had an R^2 of 89.7%, which leaves 100-89.7=10.3% of the variability unexplained.</p> <p>The model with interactions had an R^2 of 96.8%, which improved 96.8-89.7=7.1% of the variability.</p> <p></p> <p>Thus 7.1/10.3=69% of the remaining variability was explained.</p>
post679-comment1-reply3: <p>The model without interactions had an R^2 of 89.7%, which leaves 100-89.7=10.3% of the variability unexplained.</p> <p>The model interactions had an R^2 of 96.8%, which improved 96.8-89.7=7.1% of the variability.</p> <p></p> <p>Thus 7.1/10.3=69% of the remaining variability was explained.</p>
post679-comment2: Awesome 
post680: There's a non-linear association between AT^2 and AT^3. Now while building the 2nd model ( as an alternative to baseline model), when the feature is same here, should I include both AT^2 and AT^3 ? or either of them. Also please explain the reasoning as well. 
post680-comment1-reply3: Question h is only asking for &#34;quadratic nonlinearities&#34;, are you asking for this or something else?
post680-comment2: Yes I was asking the same. Thankyou.
post681: I was not able to get a lecture for normalized method. Can any T.A. elaborate the theory and how to implement to code? Would it be clearer just go for a T.A. office hour? Thank you very much.
post681-comment1-reply3: <a href="https://piazza.com/class/lll6cacyxjfg3?cid=219">@219</a> provides a method of normalization, check the sklearn documentation for more info on implementation
post681-comment1-reply3: @219 provides a method of normalization
post681-comment2-reply3: <p>Here is a quick guide on normalization techniques from Google: <a href="https://developers.google.com/machine-learning/data-prep/transform/normalization" target="_blank" rel="noopener noreferrer">https://developers.google.com/machine-learning/data-prep/transform/normalization</a></p> <p></p> <p>MinMaxScalar essentially uses the scaling to a range method.</p>
post682: what does 'Plot the train and test errors in terms of 1/k' mean?
post682-comment1-reply3: Same plot as we did in hw1, except this time plot 1/k instead of just k (which will have the effect of reversing the x-axis)
post682-comment1-reply3: sorry need to be answer, submit my follow up question in the wrong place
post682-comment1-reply3: <p>also what does &#39;best fit&#39; means in the same question?</p> <p>Are we suppose to find lowest MSE for training data and testing data separately? or just one model?</p>
post682-comment2: also what does 'best fit' means in the same question? Are we suppose to find lowest MSE for training data and testing data separately? or just one model?
post682-comment2-reply1: After plotting all of the k values, you're supposed to select a k* like we did in hw1. That k* is the best fit. For each k, you should train the KNN model with the training data. Then predict and find the MSE of the training data, and then using the same model do the same thing with the test data for the MSE of the test data. These two values are plotted in the graph for the given k.
post682-comment2-reply2: But k* (best fit) should be chosen from best test error
post683: Do we need to perform backward selection to drop the predictors? or can we just drop all the predictors that are insignificant?
post683-comment1-reply2: @198 followups has the answer
post683-comment1-reply2: Backwards selection is just an algorithm to drop insignificant predictors. Whatever method or algorithm you use, I would make a note if it.
post684: I was wondering if we would get some feedback somehow soon. Thanks!
post684-comment1-reply2: The current ETA for HW1 grading is next Wed, 09/27
post685: Hi, Do we need to find the number of rows and columns of each sheet, or just sheet one? Wasnt sure if "data set" meant all data available or the data that we are focusing on in sheet 1. Thank you!
post685-comment1-reply2: <p>@185</p> <p></p> <p>The sheets are all the same data just arranged differently. I think we can just use sheet 1 to get all the data.</p>
post685-comment1-reply2: The sheets are all the same data just arranged differently. I think we can just use sheet 1 to get all the data.
post686: if standard Error is the standard deviation of the distribution of the estimator, what does the equation on pg 16 of annotated slides mean? 
post686-comment1-reply2: nvm I&#39;m still not sure what to make of this equation, why wouldn&#39;t we just calculate the std deviation of the distribution of betas
post686-comment1-reply2: oh wait I think I get it so it&#39;s saying total variance of predictions / variance of the predictors = variance of our estimator, and if we square root that variance we get the standard deviation of our estimators 
post686-comment2: but now I see it's some measure of the variance of beta, which is mean squared error, with respect to how far a feature variable is from it's mean and that makes sense sorry for typing alot
post687: I'm trying to fit a linear regression model for each predictor and PE value. Getting p values 0.000 for all models. I'm using import statsmodels.api as sm and the model = sm.OLS(y, X).fit() to fit for each predictor against Y as PE. Since I'm getting p values 0.000 for all, am I doing something wrong here ?
post687-comment1-reply2: @241
post688:  Regarding "Train the regression model on a randomly selected 70% subset of the data with all predictors." Is that mean we select 70% of the data randomly to be train data and the rest considered as test data? 
post688-comment1-reply2: correct
post689: Hello, in my opinion, 「 Coefficient is huge 」and「 P-Value is small ( like smaller than 0.05) 」both indicate that this feature is statistically significant. But what if the co-efficient and the P-value is both almost zero ? Like the red box in the below picture ? 
post689-comment1-reply2: Coefficient only means how much the prediction changes for one unit of change by the predictor. Statistical significance is only for p-value, nothing to do with the coefficient
post689-comment2-reply2: I believe this is something the Professor mentioned when he was talking about how a doctor and statistician got into an argument. p-values show if something is &#34;statistically significant&#34; or not. Coefficients show how much change is happening.
post690: Hi,I'm doing hw2, 1(c) and I'm getting zero p-values for all the variables and I suspect if there's something wrong with that. In my understanding, if the variable is really significant, the P-value should infinitely approximate 0 but never be exactly 0. (I checked whether it's exactly 0 with if model.pvalues[col] == 0: print('zero') and 'zero' was printed.) I'm attaching my code here. y = raw['PE']results = []for col in raw.columns[:-1]: X = raw[col] X = sm.add_constant(X) model = sm.OLS(y,X).fit() print(model.summary()) results.append({ 'Predictor': col, 'Coefficient': model.params[col], 'p-value': model.pvalues[col], 'R_squared':model.rsquared }) 
post690-comment1-reply2: This just means that the p values are very very close to zero, and it is rounding them of to zero
post691: I am not sure why statsmodels is not working for me when trying to build the linear regression model. I keep getting the error below. Any help would be great. 
post691-comment1-reply2: Python is case sensitive, you might wanna try OLS<div><br /></div><div><a href="https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS">https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS</a></div>
post692: Hi, I am kinda confused about the question setting of 1h. I want to check if my understandings are right. In this question, we generate 2 models. One is using the interactions OR nonlinear associations (based on the results we have in previous questions? e.g we find AT:RH is significant, so we include that in our model). The other model, we include everything (interactions & nonlinear associations) first, then, we exclude the insignificant terms based on p-values to have our second model. At last, we use these two models to get our train and test MSEs. 
post692-comment1-reply2: Its two models, one being with all predictors (base features), and second with all possible interaction terms and quadratic nonlinearities, and remove insignificant variables using p-values
post692-comment2: Got it, so the first one is just like y = a+ b*x1 + c*x2. And the purpose of this question is to see if adding quadratic or interaction will improve the performance of model or not.
post692-comment2-reply1: correct, the first is an baseline model
post692-comment2-reply2: So the first one is just regular line regression model without interaction terms, not polynomial?
post692-comment2-reply3: from my understanding, first one, Second one, 
post692-comment2-reply4: I think the first one shouldn't include the interaction?
post692-comment2-reply5: If you talking about 1.(d) I did not include the interaction, but let see what T.A. will say. 
post692-comment2-reply6: The base model in 1h should be the same model as 1d, just trained with the 70% dataset instead of the whole thing.
post692-comment3: Should we include base features in 1g and the second model in 1h?
post692-comment3-reply1: of course
post693: Hi, Normally when we conduct regression if p-value is smaller that 0.0001 (which would be shown as 0.000 in python result) we will reject the hypothesis. But in 1h if we set the criteria as 0.0001, we are likely to remove too many predictors. Could we make the criteria of p-value a little larger, like 0.05? Thanks.
post693-comment1-reply1: You can set the p-value as whatever you like, BUT make sure to set it BEFORE you conduct your tests. It would be biased if you do it afterwards. And generally people use 0.05.
post694: Hi, In 1h we are asked to remove factors that are not statistically significant. I found that some basic factors (i.e. V, AT) can be not significant in some random circumstance. Should we remove the basic factors? I wonder if this would affect the explanatory ability of the model?
post694-comment1-reply1: <div>You should try dropping it to see if your performance gets better, the principle itself of wether to keep the base feature while and interaction exists is ad hoc.</div>
post694-comment2: I remember Professor saying that it's advisable to keep the base predictor(main effects) to keep the model more interpretable
post694-comment2-reply1: That is called the hierarchical principle and only relevant when interaction terms are involved. It is also more of a choice than a rule.
post694-comment2-reply2: Yes, keeping it could lead to better interpretable. But if we only care about the performance, there is no answer only after we do another experiment. Is that right ?
post694-comment3-reply2: You might need to check your backwards selection algorithm for removing features. Sometimes the base features have statistically insignificant p-values on the first iteration, but they are not the highest p-value of all the features. Removing the highest and running again may lower the p-values of the basic features.
post694-comment3-reply2: You might need to check your backwards selection algorithm for removing basic features. Sometimes the base features have statistically insignificant p-values on the first iteration, but they are not the highest p-value of all the features. Removing the highest and running again may lower the p-values of the basic features.
post695: Hi, May I confirm that "the linear regression model" stated in question(j) refers to which question previously? Does it refer to question(d)?Thanks!
post695-comment1-reply2: It refers to 1h, since you were never asked to test the errors before that
post696: Hi. HW2 1 c asks us to find the outliers which we would like to remove from the data for each of these regression tasks. Once we find the outliers, are we supposed to remove the outliers from data, and fit the LR model again? Also, recalculate the statistically values such as P values and analysis the statistically significant assocation between the predictor and the response again? Thanks.
post696-comment1-reply2: @120
post697: Hi. It says "Make pairwise scatterplots of all the varianbles in the data set including the predictors (independent variables) with the dependent variable ". Are we supposed to generate pairwise scatter plots of all variables in the dataset, but these plots not be color-coded based on the dependent variable? (5*5 figures) Or, figures with data points in the scatter plots being color-coded based on the dependent variable (4*4 figures)? Or either will be fine? Thanks.
post697-comment1-reply2: As you pointed out it says &#34;all the varianbles in the data set including the predictors (independent variables)&#34;, so it would be 5x5 here
post697-comment2-reply2: <p>I believe pair-wise means a vs. b, b vs. c, a vs c etc, so to ansewr your question it would be AT vs V, AT vs RH, AT vs PE, vice versa. These will yield a 4 X 4 figures. some function will produce a distribution when they have encounter situation of pairwise of themselves. eg: AT vs AT. with the distribution it will produce a 5 x 5.</p> <p></p> <p>In terms of color-coded, im not sure what you mean. in the last hw we only color-code because we have different labels of y (0,1), here the y is quantitative so color-code is no need. </p>
post697-comment2-reply2: <p>I believe pair-wise means a vs. b, b vs. c, a vs c etc, so to ansewr your question it would be AT vs V, AT vs RH, AT vs PE, vice versa. These will yield a 4 X 4 figures. some function will produce a distribution when they have encounter situation of pairwise of themselves. eg: AT vs AT. </p> <p></p> <p>In terms of color-coded, im not sure what you mean. in the last hw we only color-code because we have different labels of y (0,1), here the y is quantitative so color-code is no need. </p>
post697-comment2-reply2: <p>I believe pair-wise means a vs. b, b vs. c, a vs c etc, so to ansewr your question it would be AT vs V, AT vs RH, AT vs PE, vice versa. These will yield a 4 X 4 figures. some function will produce a distribution when they have encounter situation of AT vs AT. </p> <p></p> <p>In terms of color-coded, im not sure what you mean. in the last hw we only color-code because we have different labels of y (0,1), here the y is quantitative so color-code is no need. </p>
post697-comment2-reply2: <p>I believe pari-wise means a vs. b, b vs. c, a vs c etc, so to ansewr your question it would be AT vs V, AT vs RH, AT vs PE, vice versa. These will yield a 4 X 4 figures. some function will produce a distribution when they have encounter situation of AT vs AT. </p> <p></p> <p>In terms of color-coded, im not sure what you mean. in the last hw we only color-code because we have different labels of y (0,1), here the y is quantitative so color-code is no need. </p>
post697-comment2-reply2: I believe pari-wise means a vs. b, b vs. c, a vs c etc, so to ansewr your question it would be AT vs V, AT vs RH, AT vs PE, vice versa.
post698: 1h is the only one that explicitly mentions removing insignificant variables, but I am seeing in some of the posts it being mentioned in relation to other questions. Any clarification is appreciated, thanks!
post698-comment1-reply2: Yes 1h is the only explicit one, the other post is another student asking if they could do it with an earlier question (i assume as additional practice), which of course they are welcome to do
post699:  for example of above values, it is obivous that this predictor have high association according to R^2 value. This also tells that each coefficients have quite low P-values thus it also proves the reasonable strong association. However, this predictor AP does not have strong R^2 value, yet has all the low P-values for each coefficient. How do I interprete these values. Do R^2 valeus overide interpretability regardless of each P-values? Thanks in advance. 
post699-comment1-reply2: My understanding is that the p value is what we&#39;re looking at to determine whether there is a meaningful relationship between the independent variable and the dependent variable and when looking at a nonlinear relationship we look at the highest term.
post699-comment2: If you check the above screenshots, AP has all the low P-values. so let's say we don't count R^2 then P-values suggests significance. isn't it?
post699-comment2-reply1: yes p-value suggests significance
post699-comment2-reply2: Then, wounldn't they be significant meanwhile the R^2 still very low? Thanks!
post699-comment2-reply3: Yeah thats totally possible, it just means ur model explains not much of the variation, but it can still be significant
post699-comment2-reply4: Base on my understanding, the p-value is used to determine whether a predictor has statistically significant impact. For example, in a study examining the relationship between exam scores and study time, the R-square for study time may not be high since it fails to account for factors like "prior knowledge" or "learning efficiency." Therefore, a model that only includes "study time" cannot fully capture the relationship between X and Y. Certainly. A low R2 value doesn't necessarily mean that the predictor variable (study time, in this example) is not important for the response variable (grades). If the p-value for study time is very low, it usually suggests that study time is statistically significant in predicting grades. A low R2 might simply mean that other unconsidered variables also impact grades, or that the model itself is not perfect. A low R2 does not negate the importance of a predictor variable if its p-value indicates statistical significance.
post700: Are we also supposed to include the interaction of linear and non-linear features in the model in question 1h?
post700-comment1-reply4: no quadratic interactions are needed
post701: Hi, just wanna check, which file that we need to use for HW2. Is the ods one or the excel sheet?
post701-comment1-reply4: Its the same as HW1, you can use either (but ofc you will find excel to be easier to import)
post702: Thanks！ 
post702-comment1-reply4: correct
post703: What is recommended to use to normalize our data set,Min max scalar or sklearn.Normalize?
post703-comment1-reply4: Use minmaxscalar, Normalize performs row wise normalization. That does not do feature scaling. We want to achieve feature normalization.
post703-comment2: Can we use StandardScalar since it is also column-wise or is it recommended to use minmax? 
post703-comment2-reply1: We don't really know if our dataset is Gaussian, so i recommend using minmaxscaler
post703-comment2-reply2:  Will they be right way of doing it?
post703-comment2-reply3: that looks good
post704: Hello, as we know, the standard error is , which is the [ sqrt(variance) / sqrt(number of samples) ] , So the [ SE^2 = variance / number of samples ], but why in the Lecture 2 PPT, Page 16, it is : May I ask what does the Denominator mean here ? Or maybe I have a bad understanding ?
post704-comment1-reply3: <p>$$\sigma/\sqrt{n}$$ is the standard error for the sampling distribution of the sample mean, according to the Central Limit Theorem.</p> <p></p> <p>Here, $$\beta_i$$&#39;s are linear regression coefficient estimates, and those formulas can be proven.<br /><br />It feels like you asked ChatGPT &#34;what is the standard error&#34; and got a &#34;typical&#34; answer that is not relevant.</p>
post705:  Hi Based on what we specify the degree of Polynomial Features? Thanks
post705-comment1-reply3: I am not getting what you are asking here. But include both polynomial and cubic as the instruction shown.
post705-comment2-reply3: Cubic is degree 3
post706: Hi! For plotting the statistical significance of predictors, isn't the plot for 1bii (pairwise scatterplot) enough to show correlation between response and predictors? If not, do you have any suggestions? Thank you
post706-comment1-reply3: For example, you can plot a scatterplot with the regression line
post707: Hi Class, There are no rooms for our midterm exams at USC. According to the syllabus, I will change the hours of the midterm exams to 8-9:50 AM, but the dates stay the same. This decision is a last resort, and it makes sure that we can have the exam for all students at the same place. Regards, M R Rajati
post707-comment1: Would you please specify if it’s in the morning or in the evening. Thank you.
post707-comment1-reply1: Morning.
post707-comment2: If there are no rooms, does this mean the test will be administered online? Or just that the test will be in the lecture rooms, but at 8:00am?
post707-comment2-reply1: The test will be in person
post707-comment2-reply2: So the exam is at 8 am or 8 pm?
post707-comment2-reply3: 8 am
post708: Why is Sxy written as sum of (xi - xhat) * (yi - yhat) and not in square root form like the denominator? Pg 50 for R^2, pg 14 for B1 linear regression Annotated slides, What exactly is S referring to? 
post708-comment1-reply3: S means the sum of the product of the differences between the two, in this case x and y
post709: I noticed that the column names in the dataset are ['AT', 'V', 'AP', 'RH', 'PE'], but the homework refers them as ['T', 'V', 'AP', 'RH', 'EP']. Should we change the data to reflect the homework variable names or just leave them as they are? Just want to clarify whether this is a test on "data cleanup" or just a naming error on the homework.
post709-comment1-reply3: <div>You can do what you feel comfortable with, it really does not affect our models.</div><div><br /></div>It is just how the UCI documentation names the variables, Temperature (AT), Exhaust Vacuum (V), Ambient Pressure (AP), Relative Humidity (RH), and Energy Output (PE).
post710: Is this question based on the original regression model or the one where we included higher dimension terms? In other words, are we creating interactions with just the linear terms or also the quadric and cubed terms?
post710-comment1-reply3: @148
post711: I am wondering how should we split the training set and testing set here, is it mentioned in previous part? Or these are all decided by ourselves, like the size of each set.
post711-comment1-reply3: @189
post712:  I'm wondering about finding the outliers, I tried to use both of z-score and bonferroni outlier test. As a result, each one gives diffrent number of outliers. Which one should I use, is there any specific guides of choosing the best method? Thanks 
post712-comment1-reply3: @204
post713: For questions after 1(c) do we need to use a cleaned dataset, i.e. with outliers removed?
post713-comment1-reply3: No, you only need to show the outliers. But if you want to experiment with a cleaned dataset go ahead as well!
post713-comment2: I completed the assignment using the cleaned dataset, but my results might differ. Will that affect the grading?
post713-comment2-reply1: As long as you did all the other steps that are asked you will be fine
post714: On slide 14, there is a 1 / n -1 written in the numerator and the denominator of Beta1 = Sxy / Sx^2 , what is that referring to? 
post714-comment1-reply1: The standard deviation calculated with a divisor of 𝑛−1<br /> is a standard deviation calculated from the sample as an estimate of the standard deviation of the population from which the sample was drawn. Because the observed values fall, on average, closer to the sample mean than to the population mean, the standard deviation which is calculated using deviations from the sample mean underestimates the desired standard deviation of the population. Using 𝑛−1<br /> instead of 𝑛<br /> as the divisor corrects for that by making the result a little bit bigger.
post715: I'm encountering some issues c in the outlier analysis. While exploring potential solutions, I've come across two common approaches for outlier removal that are widely discussed in online sources. The first method involves utilizing scatter plots to visually inspect the data. The second method employs statistical tests, specifically the Z-Score method, to identify and remove outliers. However, I've been contemplating whether the outliers identified through the Z-Score method are truly equivalent to the outliers that we might pinpoint on a scatter plot. My main question is the effectiveness of our statistical approach in removing outliers that may not be relevant in the context of linear regression.
post715-comment1-reply1: <div>Of course the different methods will net you different results. You can use any of them for this HW, just state which one you are using, it shouldn&#39;t ultimately matter too much for this HW. In the real world, NO FREE LUNCH.</div><div><br /></div>There are many ways to find outliers, you can use one that you see fit. Some examples are Cooks distance, studentized reisduals, influence, leverage, IQR, and box plot
post715-comment2: Can I answer more narrative-like statement by visually looking from the plots for that question?
post715-comment2-reply1: You don't have to list out every individual outlier point if that is what you are asking, but you should find them and say how many there are.
post716: Hi, in the Homework instruction, the abbreviation of the dependent variable net hourly electrical energy output is "EP", but it is "PE" in the source data file. Which name should we use? Just for the sake of using consistant name when describing the analysis.
post716-comment1-reply1: As long as you are consistent within your code and analysis, it will be fine
post717: Hi!I just wanted to ask whether we should split the dataset to train and test to evaluate the performance of the linear regression model. In the dataset study, and README.txt file, it says they split the dataset with 2-CV folds. Do we need to do this too?Thanks!
post717-comment1-reply1: There is no need for 1c, you are only looking at significance, you will be instructed to split train and test at a later part of the HW
post718: do we need all interaction terms in one equation or we do it like linear variables+ one of the interaction terms and loop until all interaction terms are covered. I did it both ways and results are almost same !! lemme know which one is correct 
post718-comment1-reply1: You are asking about using between forward selection (start with null) vs backward selection (start with full), which both are fine
post718-comment2: yaa so actually, without interaction terms it's a normal simple linear regression and then for part g I looped over every interaction terms to see it's significance individually and finally I combined each and every interaction terms or we can say a full equation with all interaction terms .
post718-comment2-reply1: In that case you should not test significance "individually", instead test the new interaction term with the rest of the terms. You can refer to lecture lecture 2 slides for detailed steps, also @148
post718-comment2-reply2: so should I do a forwards chaining of interaction terms until it gets a full equation or is it fine if I directly do the full equation only.
post718-comment2-reply3: Yes of course that is ok, that is backward selection as the answer said above
post718-comment2-reply4:  ok great thanks 🙌
post719: Are we just supposed to regress on the pairwise interaction terms? Or, should we include anything else in the equation as well? 
post719-comment1-reply4: If you are asking if you should include the individual factors as well as the interactions, then yes. The base factors of the interactions should be present.
post719-comment2: Is it only the individuals and the pairwise or can we look at the pairwise terms resulting from quadratic or cubic polynomial linear regression to prove significance
post719-comment2-reply1: @148
post719-comment3: When it says to remove terms with an insignificant p-value, should we use backwards selection? Or can we just remove any variables with p < .05 after the first regression? Just want to make sure I am answering the problem in the intended way. 
post719-comment3-reply1: Please use the proper techniques for backward selection
post720: For the reading, should we read intro to statistical learning & elements of statistical learning or either is fine?
post720-comment1: are u referring to the HW?
post720-comment1-reply1: In the syllabus, under each week's topic it says the chapters to read for ISLR and ESL? Where can we find ESL
post720-comment1-reply2: Oh yes, in that case ESL is indeed "The Elements of Statistical Learning", and yes you should read it if its on the syllabus!
post720-comment2-reply2: marked as resolved
post721:  Hi Regarding to describe the findings of pairwise scatterplot, shall we describe the relationship between independent variables and dependent variable? Thanks
post721-comment1-reply2: You should point out all findings from the pairplot, if you see some relationship between dependent variables, do point them out as well
post721-comment2: Follow-up question: Shall we read all the sheets into one data frame? --I found the answer but can't delete this comment sorry!
post722: There're a lot of methods to find outliers like IQR method, Cook's distance, Z-score method, etc. Which one should we use?
post722-comment1-reply2: I think we can use any one. We might have to specify in the notebook which method is being used to find outliers. @Instructors, could you please confirm this?
post722-comment2: Do we need to graph out outliers or just return a list of them?
post722-comment2-reply1: I would suggest graphing them out to be more visual, otherwise its kinda hard looking at a bunch of numbers
post723: For KNN Regression, are we supposed to train 2 sets of knn regression models with normalized data and raw data separately?
post723-comment1-reply1: Yes, you should have one model trained with normalized data and one model trained with raw data. Putting them together could produce incorrect results.
post724: Is the train & test set in 1(i) referring to the ones that we defined in 1(h)?
post724-comment1-reply1: yes
post724-comment2: Do we only use the base features in this case?
post724-comment2-reply1: KNN should be base features
post725: Do we also need to plot outliers using a different color, or just plot what we get from the prediction?
post725-comment1-reply1: How many number of outliers are you getting for 1(c) ?
post726: For Question 2 & 3, these are exercises in ISLR, which are not part of programming question. Should we include the answers to Q2 & Q3 as markdown cells in notebook of Q1? Or should we include another document (pdf/doc)? By the way, just for confirmation that the ISLR we applied is the version in below link: https://hastie.su.domains/ISLRv2_website.pdf
post726-comment1-reply1: Please either use markdown to answer the questions, or attach pictures of the answers in the notebook.<div><br /></div><div>And yes that looks correct</div>
post726-comment1-reply1: Please either use markdown to answer the questions, or attach pictures of the answers in the notebook.
post726-comment2:  Just to make sure, are these the problems we're solving? 
post726-comment2-reply1: yes
post727: Hi,I remember the professor said we should use statsmodels in class? Just want to confirm if we use statsmodels or sklearn linear regression.Thanks
post727-comment1-reply1: You can use either
post727-comment2: Statsmodels is highly preferred though.
post728: Hi, Folds5x2.xlsx contains multiple sheets, and I am not sure what the difference is between them. I was wondering if it's okay to just focus on the sheet1 for the homework 2. Thanks!
post728-comment1-reply1: <md>Hi, you can find in the footnote of Homework 2 that *all of these 5 sheets are shuffled versions of the same dataset. **Work with Sheet 1**.*</md>
post729: Hello, I am working on Homework 2 question i and it is asking us to "Perform k-nearest neighbor regression for this dataset". But all the data for this dataset is continuous and from my knowledge, KNN is a classification algorithm. So how would we apply KNN here? What would the classifier be? In addition, what is the difference between normalized and raw features?
post729-comment1-reply1: <p>KNN can be used for both classification and regression. The classification version predicts a class, while the regression version predicts a value. There are separate models for each version if you are using sklearn. KNN is not as accurate for regression as it is for classification due to the curse of dimensionality as was discussed in lecture lesson 1.</p> <p></p> <p>For the second part of your question, raw features are when you use the unmodified input data to train the KNN regressor. Normalized features is when normalization is applied to the data before using it to train the KNN regressor. You are supposed to try both and compare for the assignment.</p>
post729-comment1-reply1: <p>KNN can be used for both classification and regression. The classification version predicts a class, while the regression version predicts a value. KNN is not as accurate for regression as it is for classification due to the curse of dimensionality as was discussed in lecture lesson 1.</p> <p></p> <p>For the second part of your question, raw features are when you use the unmodified input data to train the KNN regressor. Normalized features is when normalization is applied to the data before using it to train the KNN regressor. You are supposed to try both and compare for the assignment.</p>
post730: When I compute the (multiplicative) inverse of a matrix using np.linalg.inv(V), it gives different result with another student even though we have the same covariance matrix, V. This difference further causes a bigger test errors. My min test error is 0.24 while other students usually have 0.1. I wonder why and how to fix. 
post730-comment1-reply1: <p>if you are using knn from sklearn then check what are the values of arguments in the constructor call…maybe see if there is any difference, also i am although sure that the version of numpy/sklearn does not matter but would be good if check them as well. </p> <p></p> <p>also try to debug using ipdb or some other tools to see the values of error for each k iterations.</p> <p></p> <p>it would be nice if you can post some small snippet that reproduces the above differences.</p>
post731: What is ISLR: 2.4.1 and ISLR: 2.4.7 ?
post731-comment1-reply1: &#34;An Introduction To Statistical Learning&#34;, aka the textbook
post732: Hi, I have recently turned in my homework. I wonder whether it is correct to upload my files and include the code's file path in the way as shown in the attached image. Thank you. 
post732-comment1-reply1: As long as you are following @40 and ur code runs as is you are fine
post733: My first name has multiple words. So how should I title the file? Should I put ‘_’ in the middle of my first names or just capitalise each word without ‘_’.Please let me know soon since today is the submission deadline.
post733-comment1-reply1: yeah thats fine (either is fine lol)
post733-comment1-reply1: yeah thats fine
post733-comment2: Which one should I do?????
post733-comment2-reply1: lmao sorry
post734: when i try to clone the hw1 repo, I always get this error: remote: Repository not found fatal: repository 'https://github.com/DSCI-552/homework-1-username.git/' not found I don't know what could be wrong
post734-comment1-reply1: there shouldnt be the .git thing at the end
post734-comment2: still getting this error remote: Repository not found. fatal: repository 'https://github.com/DSCI-552/homework-1-username/' not found
post734-comment2-reply1: I would suggest going to an OH, if you can't make it to any of them today, use the upload function on the github website instead for now
post734-comment3: Can you try this instead: git clone https://username:password@github.com/NAME/repo.git ? Or alternatively: git clone https://username@github.com/NAME/repo.git Could be an authentication issue.
post734-comment3-reply1: I fixed it by using GitHub desktop
post734-comment3-reply2: then indeed it was an auth issue
post734-comment4-reply2: <p>It is most likely an issue with the credentials your laptop/pc has already stored and thus, might not have prompted you to enter it again. Try following the steps to delete the stored credentials and then try cloning it again. You will most likely be prompted to enter your credentials.</p> <p></p> <p><a href="https://stackoverflow.com/questions/37813568/git-remote-repository-not-found">https://stackoverflow.com/questions/37813568/git-remote-repository-not-found</a></p>
post735: When I try to get the relative path in Jupyter Notebook and print it, I get C:/home instead of something like C:/home/DSCI552/HW1/. I tried manually moving the file from "home" to the folder where the data is, but it gets deleted from jupyter notebook and when I upload it again, it appears again in the home folder. What should I do?
post735-comment1-reply2: <div>Its likely that your kernal is not initiated properly or smt, such as if you are not running the terminal startup command inside your HW1 folder. </div><div><br /></div><div>Using vscode or similar @146 also should automatically get rid of this issue.</div><div><br /></div><div><br /></div>If you keep running into this issue you can try to run<div><br /></div><div>import os</div><div>import sys</div><div>os.chdir(sys.path[0])</div><div><br /></div><div>sys.path[0] gets the directory of the current script, so theoretically it should work.</div>
post735-comment1-reply2: <div>Its likely that your kernal is not initiated properly or smt, such as if you are not running the terminal startup command inside your HW1 folder.</div><div><br /></div><div><br /></div>If you keep running into this issue you can try to run<div><br /></div><div>import os</div><div>import sys</div><div>os.chdir(sys.path[0])</div><div><br /></div><div>sys.path[0] gets the directory of the current script, so theoretically it should work.</div>
post735-comment2: Thank you. Using vscode fixed the issue. Just to confirm, because we use the structure given in homework rules, this is the relative path, according to me. Is this correct, or would it be considered absolute since I entered the data and vertebral column folder?
post735-comment2-reply1: yeah using ../ is only possible for relative paths
post736: Hi professor, I am wondering which fomula we should use to calculate error."1-knn_classifier.score(X_test, Y_test)"or "1-accuracy_score(y_test, y_pred)"
post736-comment1-reply1: They should give the same result. Directly using the classifier score function just saves a step since you don&#39;t have to store the predictions.
post736-comment1-reply1: They should give the same result. Using the &#34;score&#34; function just saves a step since you don&#39;t have to store the predictions.
post737: I noticed that for almost every KNN test I ran, the error rate is smallest when k = 1. How are we supposed to choose the optimal k? Simply by selecting the one giving minimum error, or use cross-validation method?
post737-comment1: I’m not sure if that’s what is supposed to happen, are you sure you’re using test error and not training error ?
post737-comment2-reply1: @127
post737-comment3: There is a reason why the training error is always smallest for k=1 and you are supposed to answer why in (f).
post737-comment4-reply1: Ideally, we need a k&gt;1, which yields the least test error as our optimal k. In this particular assignment, we need not think about using cross-validation. You will be using that strategy going forward. 
post738: Just to confirm :Our notebook should be inside notebook/KNN folder right ?
post738-comment1-reply1: That structure is what is shown in @40, although the additional &#34;KNN&#34; directory is not necessary according to @40_f1
post738-comment1-reply1: That structure is what is shown in @40, although the additional &#34;KNN&#34; directory is probably not necessary if you don&#39;t want to double nest.
post738-comment2: So just to confirm, we don't need another "KNN" directory under the "notebook" directory as shown in the structure and instead we just directly put our ipynb file under the "notebook" directory, correct? 
post738-comment2-reply1: yeah
post738-comment2-reply2: Is it okay if I submitted it under notebook/KNN anyways ?
post738-comment2-reply3: ofc course, once again we are lenient when it comes to grading
post739: Is hw due at 11:59pm the day of?
post739-comment1-reply3: Yes. So HW1 is due tomorrow night. 
post739-comment2-reply3: I would say submit the homework before 11:59 pm just to be safe. You can find the due dates on the Syllabus. <a href="/class/lll6cacyxjfg3/post/7" target="_blank" rel="noopener noreferrer">@7</a>
post739-comment2-reply3: I would say send it before 11:59 pm just to be safe. You can find the due dates on the Syllabus. <a href="/class/lll6cacyxjfg3/post/7" target="_blank" rel="noopener noreferrer">@7</a>
post739-comment3: I looked in the syllabus but I only found the project due time
post739-comment3-reply1: scroll down its towards the end
post739-comment4: Another good way to check and confirm the deadline is by opening up your repository and clicking on the hyperlink in README Click on Review the assignment due date and you will see 
post740: I am new to using relative paths and the GIT notebook and I do not know how to establish a relative file path for my Jupyter Notebook. Based on the other posts here and from some research, I imported os and used os.getcwd before trying to do pd.readcsv with '../dateset.dat' and it is still saying that the file does not exist. Is there a good guide to setting up the relative file paths correctly? I am not sure what I am doing wrong.
post740-comment1: I assumed you already took a look at @84. So what does it say ur current directory is when you do os.getcwd()?
post740-comment1-reply1: Yes I did, My current directory is the Homework folder where I have the data and the actual ipynb file
post740-comment1-reply2: May I see a screenshot of your file structure?
post740-comment1-reply3:  Is this what you wanted?
post740-comment1-reply4: I wanted to see the folder as in the file explore set up. But just from this have you tried using "/" instead of "\\"?
post740-comment1-reply5:  This is the file explorer setup. When I try to do the absolute path, it works with /. But when I do the relative path, even doing / instead does not work. 
post740-comment1-reply6: well you see, your data and the notebook files are in the same directory, so with you current setup you dont actually need any prefix to the filename for it to work.
post740-comment1-reply7: I see, so as long as your working directory has the files you need, you don't need the prefix? But when I submit, will the TA still be able to run the code with a similar setup? 
post740-comment1-reply8: Sure it will work, but currently it is not following the guidelines @40 in the picture shown, I suggest you follow that structure
post740-comment1-reply9: I see, I will try to make it match that structure before submitting
post740-comment2-reply9: resolved
post741: Our hw1 repository should be private or public?
post741-comment1-reply9: You should use the submission repo from the link in @42.
post741-comment2: The syllabus also says not to post course materials and homeworks publicly.
post741-comment3-reply9: Its suppose to be private, or all the other students will be able to see it :O
post742: Just want some clarification on what use all of your training data means? Does this mean we are using our training data that we set up in problem (b)? 
post742-comment1-reply9: yes
post743: I find that there are corresponding functions in sklearn that can be used to calculate the data that needs to be calculated for this question. Can I call them directly? Or should I write the code myself to do the calculation?
post743-comment1-reply9: @125 same idea for all functions
post744:  Should we return the Best log10(p) for test and train? 
post744-comment1-reply9: please read the instruction carefully, it says &#34;Summarize the test errors&#34; and nothing else
post744-comment2: in b. the question asks about what's the best log10(p) 
post744-comment2-reply1: correct, only for test errors
post745: Is this a correct submission format for hw1?
post745-comment1-reply1: Yeah that looks like what is described in @40. The most important thing is that your code is using relative paths so that graders can run without issues. You can check that it runs by git cloning the repo into a different directory on your computer and trying to run all.
post746: How come we are using the set of values of k(1, 6, 11, ... , 196) instead of the prior k(208, 205, ... , 3, 1)? Wouldn't that make more sense because we are using the full set size again?
post746-comment1-reply1: Its not a mistake, simply follow the instruction will do fine in this case
post747: In what scenario are we using log10 p = [0.1 ... 1] I'm getting the best minimum test error with Minkowski p = 1 or Manhattan, Is the point that Manhattan is the simplest and the best? Cheers
post747-comment1-reply1: No that is not the point, the point is NO FREE LUNCH, and you should try different things
post747-comment2: I see, thank you very much. Is there any experimental application of the log10 of the p values? 
post747-comment2-reply1: It is essentially a set of numbers that cover a good range of values
post748:  Hello Shall we check the predicted label in each mitrics for train data and compare it to train data labels to get the best k? I am a little bit confused here 
post748-comment1-reply1: Just go through all your error rates and choose the lowest, explain why. It&#39;s asking for lowest error rate not best k. 
post748-comment2-reply1: You just have to identify and report the lowest train error rate you could achieve so far in different sections.
post748-comment2-reply1: You just have to identify the lowest test error rate you could achieve so far in different sections and report that. 
post748-comment3: Just to confirm. So, it is the lowest test error rate from previous sections not the lowest training error rate as mentioned in the question.
post748-comment3-reply1: No. Test error rate is the error rate on a test set. Training error rate is the error rate on the training sample. 
post748-comment3-reply2: Sorry, my bad. You must report the lowest 'training' error rate you have achieved so far. 
post748-comment3-reply3: No worries, is there any hint on how to identify the training error rate?
post748-comment3-reply4: You've already calculated it in previous parts! You just have to go through your doucment in this part, no coding required! 
post749:  Hi I compared the results of test errors for k in Mahalanobis Distance with another student, but it seems our results are a bit different, for example, my best k is 1 but her best k is 6. is that normal? Thanks
post749-comment1-reply4: yes its normal
post749-comment2-reply4: @152 might help explain why they are different
post750: In d) if multiple k values give best test error, should we list all of them in the table?
post750-comment1-reply4: <md>@36</md>
post751: Do we need to specify which class is negative and which is positive in the confusion matrix? 
post751-comment1-reply4: The confusion matrix should just have labels for class 0 and class 1, as well as axes labels to indicate which is predicted and which is true. Positive and negative are implied between the two classes. It&#39;s standard that class 0 is negative and class 1 is positive, especially since in our homework they stand for &#34;normal&#34; and &#34;abnormal&#34;.
post751-comment1-reply4: The confusion matrix should just have labels for class 0 and class 1, as well as axes labels to indicate which is predicted and which is true. Positive and negative are implied between the two classes.
post751-comment1-reply4: The confusion matrix should just have labels for class 0 and class 1, as well as labels to indicate which is predicted and which is true. Positive and negative are implied between the two classes.
post751-comment2: Would it be fine to just output the confusion matrix without any labels? Since its fairly standard?
post751-comment2-reply1: I think the labels are necessary. The order of the rows and columns in the confusion matrix is not standardized. For example, sometimes TP is in the top left and sometimes it's in the bottom right, so you need to label the classes to show which is which.
post752: Working on c.iii and my graph is showing a higher error in training than testing for higher values of N. Intuitively, this doesn't make much sense, so is it most likely that my code is wrong?
post752-comment1-reply1: <p>It can happen if your test set has a sampling bias. Quoting an example from the internet here</p> <p></p> <blockquote> <p>A simple example can explain this. If you are a student studying for an exam, and you understood only 40% of your syllabus. Fortunately for you, the examiner asks you questions only on the things you learned, and you get a 100% result. This does not mean that you know the whole subject, just that the test was ‘biased’ for you.</p> </blockquote> <p></p> <p>You can try reshuffling your train and test sets (use a different seed value if using already) and then re-train your model to check the performance. </p>
post753: When I pushed my homework on github, it created some miscellaneous files like .DS_Store or checkpoint folder.Should I delete all those unnecessary files? 
post753-comment1-reply1: There is no need to delete them, but if you wish too that is fine too
post753-comment2: Alternatively, you can add those files & folders to '.gitignore' for them to be ignored by git before committing. This can help you in future assignments.
post754: I implemented the Mahalanobis distance according to footnote 6 but I am getting inconsistent error values when I re-run the exact same code. From the two graphs produced by identical code run multiple times, the lines are quite different with even the k* not being the same. Has anyone else run into this? Which k* should I select for the summary table? 
post754-comment1-reply1: For anyone else who experiences this, I seem to have solved it by specifying the algorithm as &#34;force&#34; for the sklearn classifier. It seems that it was automatically choosing the &#34;ball_tree&#34; algorithm which was not deterministic for some reason. After switching to the brute force algorithm, the results are the same every time.
post755: In order to find optimal K, do we have to perform kFold cross-validation for a specified N? 
post755-comment1-reply1: There is no need here, just simply follow the instruction and compare the error for each k and select the lowest one
post756: What is the purpose of having it be in reverse order, and does it change the results? I didnt think it did and so I was wondering if maybe I was missing something. thank you!
post756-comment1-reply1: There is no purpose, its just so you know to graph it in the normal order, hence reversing the reverse, its a wording thing
post756-comment2: should we also be inverting our axes for ciii as well? also is it it always that when plotting error that typically you plot in the reverse order? 
post757:  Hi, Just to make sure, is the output for point d (i), a table contains k value and corresponding test error? Thanks,
post757-comment1-reply1: Sure that works
post757-comment2: for summarizing the test errors, is there a preference between using KNeighborsRegressor vs Distance Metric when calculating the Minkowski distance
post757-comment2-reply1: Sorry, I didn't get your question completely. But if you are asking whether you should use KNeighborsRegressor(with varying metrics) or KNearestNeighbors with different distance metrics, I would say there is no preference. Let me know if my understanding is completely wrong 😅.
post758: Should we use the third degree polynomial linear regression from 1f to show pairwise interaction terms or a second degree polynomial linear regression to show pairwise interaction terms? 
post758-comment1-reply1: question f and g are separate, so no need!
post758-comment2: g only needs a linear model with interactions. Don't include quadratic and cubic terms.
post758-comment2-reply1: Thanks prof
post759: Is anyone using Jupyter books in vscode for hw1? 
post759-comment1: sure, thats completely fine (id say its preferred actually cause its just easier)
post759-comment1-reply1: oh and just to add, vscodium is superior ;)) 
post759-comment1-reply2: can you provide a step by step on how to use jupyter books in vscode?
post759-comment1-reply3: I am struggling to set up my environment in vscode so just been using Jupyter for now
post759-comment1-reply4: Its as simple as open vscode -> go to extensions -> install jupyter notebook plugin -> follow the prompts -> enjoy :Dhttps://youtu.be/9V7AoX0TvSM?si=quOPhRSRL3dxBJq3
post759-comment1-reply5: [Here](https://code.visualstudio.com/docs/datascience/jupyter-notebooks) is a guide provided by vs code. You'll also need to install the [python](https://marketplace.visualstudio.com/items?itemName=ms-python.python) extension and the [jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) extension
post759-comment1-reply6: after doing all the tutorials, all my code runs except for :"from sklearn.neighbors import DistanceMetric" and it gives the error: " ImportError: cannot import name 'DistanceMetric' from 'sklearn.neighbors' (/Users/jacy/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/__init__.py) " do you have any recommendation on how to import that specific package? 
post759-comment1-reply7: It depends on what version of sklearn is installed in the python kernel that you selected for the vs code workspace. I believe that DistanceMetric is in sklearn.metrics for newer versions of sklearn (https://github.com/scikit-learn/scikit-learn/discussions/23591) 
post759-comment1-reply8: that was very helpful thank you! i was importing as sklearn.neighbors
post759-comment2-reply8: <md>Yeah, I use vscode for all my python projects including jupyter notebooks. The extensions and linting in vscode can be really helpful if you have the right settings, and it's much easier than manually starting a jupyter server.</md>
post760: As title
post760-comment1-reply8: .gitignore is an autogenerates file by git to track the files, there is no need to modify it. You can put anything you think is meaningful about what you have done in your readme file, there is no strict requirment.<div><br /></div><div>Adding, if you want any files that you do not want to be tracked and reflect on your remote git branch you add them to the .gitignore </div>
post760-comment1-reply8: .gitignore is an autogenerates file by git to track the files, there is no need to modify it. You can put anything you think is meaningful about what you have done in your readme file, there is no strict requirment.<div><br /></div><div>Adding, if you want any files that you do not want to be tracked and reflect on your remove git branch you add them to the .gitignore </div>
post760-comment1-reply8: .gitignore is an autogenerates file by git to track the files, there is no need to modify it. You can put anything you think is meaningful about what you have done in your readme file, there is no strict requirment.
post761:  Hi, I need a clarification on c,iii 1- Shall we create a new classifier for the subset training set? 2- What is the output for this question, is it only a graph show the error rate v. N? Thanks
post761-comment1-reply8: 1. What do you mean by a new classifier? You will still be using KNN. If you are asking if you should retrain it, then yes, you should have been doing so since c.ii<div><br /></div><div>2. Yes please read the instruction carefully, it says to construct learning curve</div>
post762: For HW1, I utilized matplotlib and seaborn libraries to create all my plots as such was recommended for HW0. For future homework assignments, is it okay to use different plotting libraries such as plotly?
post762-comment1-reply8: I think you can take a look @38
post763: In the second slide, is the intercept row stats of β0？Its p-value is very small, does it mean anything? (I remember in the lecture we used the fact that β1's p-value is large to conclude gender is not statistically significant.) Thanks. 
post763-comment1-reply8: It just means that the intercept is statistically different than 0, there is no real world meaning and we don&#39;t usually care about it
post764: Is it okay to use or (should use) for getting, Train errors from training data set (210 rows) and Test errors (100rows) from the fit from training data set(210 rows)? Thank you.
post764-comment1: can you rephrase this, I am not really understanding
post764-comment1-reply1: I'm fitting KNN from training set (210rows) To get the training error, I used the same training set(I used for training) to calculate the errors in different K values. Am I doing correctly? Because in this case I always get 0 error rate at k=1 for any kind of method which is so trivial.
post764-comment1-reply2: Which question are you referring to? 
post764-comment1-reply3: Im referring (f) since I had to check all different methods' training errors.
post764-comment1-reply4: please refer to @24 and @81, dont over think
post764-comment1-reply5: I've already looked them up. i trying to double check which I did fine or wrong. So back to original question, am I doing correctly? Thank you so much.!
post764-comment1-reply6: Follow the instructions and answer the question, just don't overthink it! (I shouldn't give the answer straight up)
post764-comment2-reply6: marked as resolved
post765: For HW 1, part biii, where it says to select the first 70/140 rows as training and the rest as training, what does it mean when we say select? does this mean to create new dataframes for training and testing data sets? thanks!
post765-comment1-reply6: <md>Yes, it means to split the data up into two distinct dataframes or arrays, one for training and one for testing accuracy. You can do this manually or with something like [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).</md>
post765-comment2-reply6: Well you can do this anyway you like, split it into two dfs or add a column indicating, as long as it works for you its all good
post766: do we need to change the column of AB and NO to 0 and 1? or is it ok to have solved the homework with it as AB and NO? Thank you
post766-comment1-reply6: On the footnotes, the instructions specifically say: “Make sure that you convert labels to 0 and 1, otherwise you may not obtain correct answers.”
post766-comment2: I think it's valuable to look into "one hot encoding"
post767: I noticed I only pushed notebook directory to the repository and when I tried to "commit to main" it showed the pic below. It seemed that the main branch already has data directory... 
post767-comment1-reply6: Empty folders are not tracked with git, add something in ur folder
post767-comment2: It's not empty. It contains a subfolder. Don't know why it shows it's empty
post767-comment2-reply1: thats still empty u need a file, a folder is just a path, its like you are putting air inside air its still air
post767-comment2-reply2: And the subfolder contains files. Isn't that ok? 
post767-comment2-reply3: From the screenshot it seems like you only have the data folder and no files, which is why I said what I said.You can either delete those files and add them again while github desktop is open, or use git and run the suggested command
post767-comment2-reply4: Ok. Thanks
post767-comment3: I feel that you have not added the files to be commited to your branch Run git add .And then commit . You might not have run the previous command and that’s why it says untracked files
post767-comment3-reply1: Solve it. Thank u.
post768: For questions in the homework, such as which is the best k? Do we just output it in a print statement? or include it in a markdown comment? Thank you
post768-comment1-reply1: I did a markdown comment. Not sure if we needed to have a code for that 
post768-comment2-reply1: Mark down is preferred for word analysis, but if its like an output just printing is fine
post768-comment2-reply1: please notice the requirments.txt file in @40
post768-comment3: I think the current instructor response is meant for @134. I don't think there is any harm to do it both ways to be safe, but am instructor can confirm.
post768-comment3-reply1: aw looks like piazza glitched again oops
post769: Hi, I used several packages (e.g. prettytable) in HW1, do I need to keep those installation commands in the notebook? Thanks! 
post769-comment1-reply1: please notice the requirement.txt file in @40
post769-comment2-reply1: <md>To elaborate for those unfamiliar, a requirements.txt file is typically standard for describing any dependencies in a project, although optional for our homework according to @40. You can run the command `pip freeze > requirements.txt` to automatically generate it (as long as the command is run in the same Python environment as your homework). An even better way to automatically generate requirements.txt is to use [pipreqs](https://stackoverflow.com/a/31684470). All dependencies can then be installed in a new environment by running `pip install -r requirements.txt`</md>
post769-comment3: Can we assume the commonly used packages (such as numpy, pandas, sklearn., matplotlib..) have been installed in the grader's device, so we don't need to do the pip installation command?
post769-comment3-reply1: I think that is a fair assumption since the requirements.txt file is optional. If you want to be safe, @40 recommends that all imports be organized in the first cell so that they can easily find packages that they need to install if necessary.
post769-comment3-reply2: yes common packages should be installed
post770: Hi To make sure that my understanding is correct related to c-ii, are the below points the needed outputs of this point? 1- Plot train and test errors vs k 2- Answer this question: Which k∗ is the most suitable k among those values? 3- Calculate the confusion matrix, true positive rate, true negative rate, precision, and F1-score Thanks
post770-comment1-reply2: That is correct. I believe we need confusion matrices and metrics for both the test and training errors.
post771: Hello all, when encoding the labels from NO, AB to 0, 1 I can't seem to get them to match. What I mean is, I'm using sklearn label encoder, but it's making NO = 1, and AB = 0. Not sure I can reverse this order? I tried to edit the data file and moving things around but that didn't work.
post771-comment1-reply2: <p>I believe you are using the column_2C.dat for the assignment. In that case, you write custom code to manipulate the column in the data frame similar to this. </p> <pre> data[&#39;col&#39;] = data[&#39;col&#39;].apply(lambda x: 1 if x==&#39;AB&#39; else 0)</pre> <p></p>
post771-comment2-reply2: The sklearn label encoder makes the first instance it finds as 0 and the next one as 1. So try manually encoding them to 0 and 1.
post772: Hi all, I just had a discussion with another guy about HW1's output. (Only compare the pictures, not coding section.)Wondering if it's normal that our output pictures are not the same? Probably due to the dataset shuffled reason(after extraction)?And another question is that how to make sure the trained model is good enough? Is there any criterion such as error score need to below some threshold, or could we judge by the plot curve? Any response would be helpful, thx a lot.
post772-comment1-reply2: Its normal for graphs to look different, but they should generally have similar trends. <div><br /></div><div>There is no need to worry about specifically how well ur model needs to perform since we do not grade on that, just make sure you achieve everything the instruction ask you to do</div>
post773: For ci, we write knn code and train it using training data. For cii, we test this model. Is my understanding right. Also, what do you mean by "Test all the data in the test database with k nearest neighbors. Take decisions by majority polling. Plot train and test errors in terms of k for k ∈ {208,205, . . . ,7,4,1,}(in reverse order)."
post773-comment1-reply2: @127
post774: For this subpart, do we need to re-compute k* or use the same as part A as we did in part B?
post774-comment1-reply2: Please find a new k*, since it does not say to keep using the one from part A
post775: When calculating the confusion matrix and other metrics for k*, do we use only the test set or should it be calculated with both test and training set? Since there is some training error for the model's k*, I am not sure if it should be included in the metric calculations.
post775-comment1-reply2: correction: k* is obtained through the test data, but you should calculate both the test and train error here
post775-comment1-reply2: k* is obtained through testing with training data (assuming you are talking about k nearest neighbor)
post775-comment1-reply2: k* is obtained through cv with training data (assuming you are talking about k nearest neighbor)
post775-comment2: Thank you for your reply. As a followup, should they be combined into one confusion matrix and metrics? Or should there be a confusion matrix for test data and a confusion matrix for training data?
post775-comment2-reply1: please do it for both separately 
post775-comment2-reply2: Hi, I searched online and found "A confusion matrix is used to measure the performance of a classifier." So my understanding is confusion matrix is just for test data. Why do we need that for training data as well? 
post775-comment2-reply3: Do it for both to eliminate any chance of points being taken off
post775-comment2-reply4: to display confusion matrix for training data, do we have to use k* obtained from test data?
post775-comment2-reply5: yeah use the same one
post776: For the table of test errors when k = k* in question d, is it right to have a column for k, and a column for the test errors for each metric when k=k*? Should the test errors for part B with with log10(p) {0.1, 0.2, 0.3;..., 1} for the best k for the manhattan distance be a part of the table? Thank you!
post776-comment1-reply5: You just need to use the same k* found in A for all log10(p), then find which log10(p) performs the best
post776-comment2-reply5: .
post776-comment2-reply5: <p>I have the same doubt. What should the table comprise ? It says to summarize all the test errors for which k = k*. There are multiple k* (for each part), for ex: should it be like this?</p> <table cellpadding="6px" border="1px" cellspacing="0"><tbody><tr><td>K*</td><td>metric</td><td>test error</td></tr><tr><td>1</td><td>Manhattan</td><td>0.05</td></tr><tr><td>5</td><td>Minkowski (log10(p))</td><td>0.12</td></tr></tbody></table>
post776-comment2-reply5: <p>I have the same doubt. What should the table comprise ? It says to summarize all the test errors for which k = k*. There are multiple k* (for each part), should it be like this:</p> <table cellpadding="6px" border="1px" cellspacing="0"><tbody><tr><td>K*</td><td>metric</td><td>test error</td></tr><tr><td>1</td><td>Manhattan</td><td>0.05</td></tr><tr><td>5</td><td>Minkowski (log10(p))</td><td>0.12</td></tr></tbody></table>
post776-comment3: I have the same doubt. What should the table comprise ? It says to summarize all the test errors for which k = k*. There are multiple k* (for each part), for ex: should it be like this? K*metrictest error1Manhattan0.055Minkowski (log10(p))0.12 (apologies for posting again, I edited my response as it got posted as an answer)
post777: Hi, I was working on homework cii and it asked to calculate the f1-score, but I came across different answers for different approaches (sklearn.metrics.f1_score and hand calculation from the documentation ). sklearn doc: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html which would be the correct approach? thank you. 
post777-comment1-reply5: using the pacakge function is fine
post777-comment2-reply5: <md>Hi, it seems that the difference between your result calculated by equation and by sklearn package is due to the **wrong assignment** of each elements in confusion matrix output by sklearn.metrics.confusion_matrix(). You can refer to the sample code in the document of sklearn.metrics.confusion_matrix(). https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html ```python tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel() (tn, fp, fn, tp) ``` In your code, you assigned each element in the sequence of (tp, fn, fp, tn) while the sequence should be **(tn, fp, fn, tp)**.</md>
post778: Hi, just a general question, Whenever we use the weighted decision in kNN classification, is it expected to always get a zero training error, as the highest weightage will be given to the same data point?? if yes, how do we gauge the fit of our model whether it has overfit or underfit? Thank you!
post778-comment1-reply5: It is not expected to always have zero training error. KNN means K-Nearest-Neighbors, you will be using neighbors of a point and every point has different neighbors so idk what you mean by &#34;the same data point&#34;.
post778-comment2: If I understand your question correctly, I would imagine that the answer would depend on the value of $$k$$ and the weight function being used. For e.g, imagine a case where $$k=3$$ and the weight function is given as: $$\frac{1}{distance\_to\_neighbor + 1} (\text{the +1 to avoid division by zero})$$. You have a test data point $$X$$ and you find its three closest neighbors: $$\{X, Y, Z\}$$ (of course, $$X$$ has to be one of the closest neighbor to itself since we are dealing with training error) with distances from the reference point: $$\{0, 0.5, 0.5\}$$. Furthermore, we know these data points' classes are: $$\{A, B, B\}$$. Then, using the weight function, we would assign a score of $$\frac{1}{0+1} = 1$$ to class $$A$$ and a score of $$\frac{1}{0.5+1} + \frac{1}{0.5+1} = \frac{4}{3}$$ to class $$B$$. Since $$\text{score of class }B > \text{score of class }A$$, we would label the test data point as class $$B$$, which is not the class of the reference data point itself. This is my interpretation, at least. You can discuss your query with the professor after class as well.
post778-comment3: Even in weighted KNN, it is possible that multiple points that are further away could "override" the vote of the closest neighbor by the way. Just because the most weighted neighbor votes one way, does not mean that the overall result will be the same as the vote. In the US electoral system, California has the most electoral votes, but we do not always elect the same president that California votes for!
post779: What if only when k=1 has the lowest testing error? Should we ignore k=1? and pick the second lowest testing error?
post779-comment1-reply5: I hope this helps:<a href="/class/lll6cacyxjfg3/post/36" target="_blank" rel="noopener noreferrer"> @36</a>
post779-comment2: but what if there is only one minimum test error which is k*=1. Should we ignore it and take the second lowest value of k?
post779-comment2-reply1: * the k of the second lowest error
post779-comment2-reply2: Use the smallest error if its unique
post779-comment2-reply3: isn't it trivial and obvious escpecially from the question of (f)? It is going to be zero for every different distance method.
post779-comment2-reply4: @24
post779-comment2-reply5: Following up on: * the k of the second lowest error So, when k = 1 gives the minimum test error, can we report the second lowest test error and its corresponding k? (may or may not have the same test error, but I'm checking if k!=1 when updating min test error) Is this alright?
post780:  Is this a correct submission format for hw1? 
post780-comment1-reply5: <p>Also, i did not add anything on the README file! is that okay?</p> <p></p>
post780-comment2-reply5: this is fine
post781: Are we supposed to remove outliers in the pre-processing step?
post781-comment1-reply5: There is no need to as the instruction does not say to do so
post781-comment1-reply5: There is no need to as the instruction does not say to do so (also if u do, you will end up with less samples than needed which is bad)
post781-comment2: I notice that there is a data point whose value for a specific column deviates from the distribution of other data points hugely. In this case, should I just leave it there or do some processing like manually change its value?
post781-comment2-reply1: There is no need to do so, but if u want you are always welcome to give it a try
post781-comment2-reply2: Fine. Thanks
post781-comment3: Based on my personal experience, removing outliers are crucial when data preprocessing involves standardization/normalization. For instance, let's say before removing outliers, feature x falls in the range [0, 100]; however, 100 is an extreme outlier, and removing them adjusting x range to be [0, 10] - do you realize the difference? If I want to normalize feature x and meet some x1, without outliers removal will scale x1 to (x1 - 0)/(100 - 0) = x1/100, while removing outliers will scale x1 to (x1 - 0)/(19 - 0) = x1/10. The range of feature is important for KNN, or any distance-based models in general. It's too complex to talk it here. But if you are interested, you can come to my office hour so that we may expand this topic a little deeper
post781-comment3-reply1: Thank u so much~
post782: Hello, Just wanting to confirm what k-value should be used when we plot N vs. test/train error rate. Is the k changed dynamically? As in, for each value of N, we plot k vs error rate and choose which k was most optimal and use that error rate for that specific N, or do we use the same k-value as N increases (like using the one from c(ii)). Thank you
post782-comment1-reply1: K can change for each N
post782-comment2: is it normal behavior to see k go up as N goes up, but around N=70-90, k=1? I see this behavior in my knn loop, and also a somewhat constant test error rate before it finally decreases for large N
post782-comment2-reply1: it totally depends on many other factors such as data size, noise, density and distribution of data points etc, there is no direct correlation with N and k although you should keep in mind that if k is very small compared to N then the mode might overfit…similarly if k is large then it might underfit. hence there is no strict linear relation between N and k, because of which we use ML :p
post783: In HW1 question (e), do we have to show the best test errors per each distance metric (Euclidean, Manhattan, and Chebyshev) or the best test error overall?
post783-comment1-reply1: I showed best test error per distance metric including the respective k value.
post783-comment2-reply1: Show them for each metric.
post783-comment3: Just to clarify -- do we only *one* test error per metric, reported when k = k*? Or, should we report a full set of test errors for each metric and each k, where k = {1, 6, ..., 196}
post783-comment3-reply1: please read the instruction carefully, it says "report the best test errors" and does not say anything else, but ofc we should see that you actually tried all ks
post784: When i try log10(p) ∈ {0.1,0.2,0.3,...,1}, it gives me an error that p cannot be a negative number. "ValueError: p must be greater than 1" I am using the DistanceMetric.get_metric() function. Any ideas?
post784-comment1-reply1: log10(p) = {0.1, ... ,1}, solve for p.
post784-comment2: Is there a particular reason why it is written as "$$ log_{10}(p) ∈ \{0.1, 0.2, 0.3, . . . , 1\} $$" in the question instead of as "$$ p ∈ \{ 10^{0.1}, 10^{0.2}, 10^{0.3} . . . , 10^{1} \} $$"?
post784-comment2-reply1: I don't think there is a particular reason, but it does save some time when writing this way in latex dont u think :)) 
post784-comment2-reply2: Ah gotcha, yeah it was a bit tedious to type that out. I was trying to google if there was a relationship between $$ log_{10} (p) $$ and the Minkowski Distance and couldn't find any result so I figured I would ask. Thanks!
post785: Hello, I was looking at the syllabus again and just noticed that exam 1 takes place on Friday October 20th, which is not the usual class time. I have another class from 9:30-4:00 pm on October 20th. Is it possible to take the midterm at a different date? If so, who should I reach out to in order to make that possible?
post785-comment1-reply2: <strong attention="iyb66l86qwb64f">@Mohammad Reza Rajati</strong> 
post785-comment2: This was CLEARLY MENTIONED in the first lecture. There will be one exam at the specified time and that’s it. And it was made clear that anyone who cannot make it to the exam must drop the course.Solution for you: talk to the instructor of the other class to excuse you for half of one lecture or petition to take another course instead of DSCI 552.
post785-comment2-reply1: I apologize professor, I was not able to attend the first lecture because it was online and I did not have the D-Clearance. I only got approval to enroll in the course last week, at which point I was mainly focused on finishing homework 0 and 1 and didn't think to look at the syllabus. I have reached out to my other professor to see if she will allow this. If she does not allow me to, I'll drop the course. I apologize for any inconvenience this may have caused you. 
post786: Based on my understanding of the question, the sklearn KNeighborsClassifier has a parameter called weight which we are meant to set to 'distance' in order to award greater influence to closer points, but when I applied that with the Euclidian distance, I got a test error rate of 0. I tested my code with and without that weight parameter and without the weight parameter, I got a more reasonable value for best k and for test error rate. Any idea why this could be? I am not sure whether or not this is an error on my end. 
post786-comment1-reply1: If you are always getting a flat 0 no matter what other parameters you choose while weight=&#39;distance&#39;, then there is something wrong yes. I suggest going to an OH for a better look at your specific problem if you can&#39;t figure it out.
post786-comment2: I just saw this and did what what the original question had stated and am getting the same issue Are we not supposed to use weight parameter?
post787: Hi, I got this warning while running the HW1, and I tried to upgrade the packages, but the warning is still there (the graph result can be shown). Would this affect grading or the execution of the program on the TA's device? Thanks! 
post787-comment1-reply1: Warnings won&#39;t deduct points, as long as your results show its fine. You can also try to suppress warnings<div><br /></div><div><pre><code style="margin:0px;font-style:inherit;vertical-align:baseline">import warnings warnings.filterwarnings(<span style="margin:0px;font-style:inherit;vertical-align:baseline">&#34;ignore&#34;</span>)</code></pre></div>
post788: Hello, I am a bit confused about how to incorporate the Chebyshev Distance into my KNN code. when I tried to add it at the distance metric, it gave me an error. Are we supposed to try to maximize the p feature as much as we can? Is there a best way to do that? Thanks in advance
post788-comment1-reply1: If you put metric as chebyshev, it should work (assuming you are using sklearn) What kind of error is it?
post788-comment2: For Chebyshev distance, as it is given in the HW, you can also use the minkowski distance with p -> infinity. To make p -> infinity you can use the float function. You can refer google as well!
post788-comment2-reply1: I was able to rerun and get an answer. Thank you for your help!
post789: Hi! When I did the c(ii) and wanted to use classification to find out k value, I had the error like this. I am not sure how can I solve the error. 
post789-comment1-reply1: Its due to version use scikit version 1.0.2
post789-comment1-reply1: Its a version error use scikit version 1.0.2
post789-comment2: When I got this error, it was due to my xtrain and xtest dataframes. I was able to get the code to run by adding .values to my xtrain and xtest. I got some warnings while doing it, but the code ran. I hope that is helpful!
post789-comment2-reply1: Same worked for me too, it was tough to debug this issue though 😬.
post789-comment2-reply2: this worked for me as well
post789-comment3: Update Navigator then it’s working 
post789-comment4: This is an issue with sklearn version 1.3.0.Use .values on x_train and x_test or downgrade to sklearn version 1.2.2 You can refer the below thread!https://github.com/scikit-learn/scikit-learn/issues/26768 
post790: Hi, during the lecture & the below slide, I understand with small p, we intend to reject the null hypothesis. But, before this slide, we also said that Alpha is probability of rejecting null hypothesis. And also with small p, that means Alpha will be small as well, but if alpha is small then rejecting null would be difficulty, isn't it? Then how come with small p, it's getting easier to reject the null? Thanks in advance :)
post790-comment1: Also if we see this diagram below, we can infer that if p is small then the area on extreme end will be small, so more prob to not reject the H0, isn't it? 
post790-comment1-reply1: I think you are mixing up alpha and p, alpha determined the extreme end areas, not p
post790-comment1-reply2: Just to add, as mentioned above, alpha is pre-determined before the experiment (you have to decide the confidence interval beforehand). For example, if your confidence interval is 95%. The non-shaded region under the curve represents this confidence interval (0.95), and the shaded regions together represent the rejection region (0.05). Now, you will compute the p-value using z-statistic / t-statistic depending on 'n'. Based on the location where it falls on the above graph (i.e. whether it falls inside the shaded region/ non-shaded region), we either reject or not reject the null hypothesis. I hope this helps. 
post790-comment2-reply2: Alpha is a metric/number that DOES NOT change, in a well conditioned setting Alpha should be determined before you start any experimentation.<div><br /></div><div>p however, is something that you obtain through calculation, the smaller p is, the more unlikely for the situation we are testing to happen, thus making it easier to reject the null. Alpha DOES NOT change along with p.</div><div><br /></div><div>Also @53 please make questions public whenever possible</div>
post790-comment2-reply2: Alpha is a metric/number that DOES NOT change, in a well conditioned setting Alpha should be determined before you start any experimentation.<div><br /></div><div>p however, is something that you obtain through calculation, the smaller p is, the more unlikely for the situation we are testing to happen, thus making it easier to reject the null. Alpha DOES NOT change along with p.</div>
post790-comment3: This is a PUBLIC question.
post790-comment4: Let me expand this question a little bit. Alpha is predefined. Yes, a bigger Alpha makes it easier to reject the null hypothesis during experiment execution - but Alpha = The probability of Type I Error (i.e., rejecting a true null hypothesis). The underline logic is that every observed experiment value is random / by chance - it may not reflect the true data distribution well. Thus, even if we observed a p-value < Alpha, this p-value may be "by chance to be so extreme". That's why we usually set Alpha at some small value, e.g., 5% to make it " more difficult to observe a value more extreme than Alpha". Let's do an example - H0: person A is innocent; H1: person A is not innocent. Do you see why we need a small Alpha? Because we are fearful of rejecting a true H0 (we try not to harm the innocent)
post790-comment4-reply1: This answers my question. Thanks to all the instructors for explaining it :)
post791: Hi, I have this small doubt about the t-statistics formula which has "- 0" in the numerator. Can you help me explain what's the use of putting "- 0" with B(hat)1? Thanks! 
post791-comment1-reply1: <p>The t-statistic is the ratio of the difference between the estimated value of a parameter and its hypothesized value to its standard error. In this case, our hypothesized value of $$\beta$$ is zero. i.e., we are trying to find if $$\beta$$ has some significance or not.</p> <p></p> <p>Also, please consider posting general questions like these publicly as it will help other students as well. You can change the visibility of this post to public if you are comfortable. </p>
post791-comment1-reply1: The t-statistic is the ratio of the difference between the estimated value of a parameter and its hypothesized value to its standard error. In this case, our hypothesized value of $$\beta$$ is zero. i.e., we are trying to find if $$\beta$$ has some significance or not. 
post791-comment2: Also @53 please make ur questions public whenever possible
post791-comment2-reply1: Yes, this is a PUBLIC question. @Changxun (Shawn) Li you probably have the permission to make it public yourself. I am doing it now.
post792: Hi, In this problem, I used the KNeighborsClassifier in Sklearn to fit the data using Mahalanobis distance. My training data and test data are in the type of np.ndarray. When I fit my training independent and dependent variables, I got the error saying that 'numpy.ndarray' object has no attribute '_validate_params'. I also tried to convert the training data into DataFrame but it still doesn't work. How can I fix it? Thanks! 
post792-comment1-reply1: To what I learn from your description, you need to pass an instance of the model to the kneightborclassifier, you might be passing the class itself.
post792-comment2: Could you please clarify what "pass an instance of the model to the kneightborsclassifier" means?
post792-comment2-reply1: You need to do something similar to this. neigh = NearestNeighbors(n_neighbors=2) neigh.fit(X) # where X is your training dataframe/ numpy array
post792-comment2-reply2: I figured it out by using a callable function instead of using a string to indicate the metric distance. Thanks! 
post793: Part (e) says that we need to report the best test errors. Can we just report them with a print statement or should we make a table similar to part (d) where it specifically asks us to use a table?
post793-comment1-reply2: Either is fine, as long as you have the needed information the format is not critical
post794: D2 on homework 1 asks us to use the Mahalanobis Distance metric on the KNN classifier. There's what I believe to be a description of how to calculate this, but I don't understand a word of it: Could someone tell me what the Mahalanobis Distance is, why it's important, and how we calculate it?
post794-comment1-reply2: TLDR it is just a distance formula to be used with KNN, it can be good for multivariate situation or with outliers, I believe professor goes more in detail about the math during lecture. In terms of the footnote, it is giving you a hint on what to pass in for Mahalanobis.<div><br /></div><div>The inverse of the co-variance matrix is the Mathemtical inverse of the matrix. The inverse of a matrix is another matrix, which leads to an identity matrix when the matrix is multiplied by its inverse. Here the footnote lets you know which of these two to use.</div><div><br /></div><div>Same as all the other distances that the HW wants you to try out, you don&#39;t have to implement it by hand. There is preset formulas within sklearn for this, and numpy can also help you with obtaining the needed inputs.</div>
post794-comment1-reply2: TLDR it is just a distance formula to be used with KNN, I believe professor goes more in detail about the math during lecture. In terms of the footnote, it is giving you a hint on what to pass in for Mahalanobis.<div><br /></div><div>The inverse of the co-variance matrix is the Mathemtical inverse of the matrix. The inverse of a matrix is another matrix, which leads to an identity matrix when the matrix is multiplied by its inverse. Here the footnote lets you know which of these two to use.</div><div><br /></div><div>Same as all the other distances that the HW wants you to try out, you don&#39;t have to implement it by hand. There is preset formulas within sklearn for this, and numpy can also help you with obtaining the needed inputs.</div>
post794-comment2: Hi. For HW d ii. Mahalanobis Distance. As it is mentioned in the HW that when the covariance matrix is singular or ill-conditioned, the features have to be transformed into a reduced feature set in the linear subspace, which is equivalent to using a pseudoinverse instead of an inverse. Given this, are we supposed to check if the covariance matrix is singular or ill-conditioned in this homework and then perform corresponding methods to deal with it? Or is it just for giving us the knowledge and we are not required to do it in this homework, and will not give us extra points by doing so?
post794-comment2-reply1: Hi, i will perform the check and perform corresponding methods. I found it wasn't too difficult to implement.
post794-comment2-reply2: Yes please perform the check to your best ability!
post794-comment3: Hi, I have a followup question. Does the covariance matrix mentioned here refer to the covariance matrix composed of data for the 6 attributes? Just to confirm. Thanks!
post794-comment3-reply1: If in this way, what data should we use to calculate the covariance matrix? I mean, should we only use the trainning set, or the whole dataframe?
post794-comment3-reply2: yes its data composed of the 6 variables and and yes only use training data for it as your model should be trained only with training data and the matrix is just a param induced on top of the model…fyi, the model is being built upon the training data, so it make sense that the matrix also should be made up from training data
post795:  Hello, For question (C), part (i), I will use KNeighborsClassifier from sklearn, for n_neighbors parameter, shall we use the default value which is equal to 5 or to change it? Thanks, 
post795-comment1-reply2: <div>Shawn: c.i is not actually asking you to do use KNN, you just need to find what package you should be using, or implement it yourself</div><div><br /></div>As question C part ii states, set k ∈ {208,205, . . . ,7,4,1,} (in reverse order). You can combine the two questions, they are not mutually exclusive.
post795-comment1-reply2: As question C part ii states, set k ∈ {208,205, . . . ,7,4,1,} (in reverse order). You can combine the two questions, they are not mutually exclusive.
post795-comment1-reply2: c.i is not actually asking you to do use KNN, you just need to find what package you should be using, or implement it yourself
post795-comment2: To make sure, Is that mean in part c.i, I just need to know which package should I use, then I implement it within part c.ii ? 
post795-comment2-reply1: yes
post796: For the learning curve, what are we supposed to plot on the axes? Should we plot N on X-axis and the smallest training error of the optimal k on Y-axis? Thanks!
post796-comment1-reply1: correct, but not just train also test error, please read the instructions carefully
post796-comment2: So are we suppose to plot both train and test error on the same graph?
post796-comment2-reply1: You can do either that or separate its up to you
post796-comment3:  The question specifically asks us to plot the best test error rate. Are we supposed to do both or just what is asked in the question. Pls confirm. Thanks!
post796-comment3-reply1: Plot both train and test error on the same graph. Either ways you will answer the question.
post796-comment3-reply2: yeah its just nice to have both, you won't get points taken off if you only do test tho
post797: Do we have to submit anything on D2L because it gives us an option to upload files for HW0?
post797-comment1-reply2: No thats not needed
post797-comment2: Okay. Thanks
post798:  Getting this message on GitHub. File runs successfully on local machine.
post798-comment1-reply2: Github is showing at the top that the file is 0 bytes. It may not have uploaded correctly. You can also clone the repo to another location to see what is uploaded to the repo and if it runs.
post798-comment2: Thanks for the explanation!Reuploading the file worked
post799: Hi. I notice there is a "Package imports" part in the beginning of the homework temlpate, is it required to include all the Package imports codes here? Or we can place them wherever they will be used? Thanks.
post799-comment1-reply2: @61 nothing is required, its just a good general practice
post800: Hi. Is feature scaling needed in processing the data before training the model? In my opinion it is needed, but it is not exactly mentioned in the homework. For my understanding, feature scaling is often crucial by using KNN because the algorithm relies on the distance between samples to classify. If different features are on different scales or units, the distance computation could be disproportionately influenced by certain features. Thx.
post800-comment1-reply2: No free lunch, give it a try if you think it might help in this situation, but if its not mentioned the instructions its not required.
post800-comment2-reply2: The professor spoke about this in lecture. He said that feature scaling assumes that all features are equally important but for the HW1 data set this is not the case and feature scaling will cause worse results. But he encouraged trying it out and commenting on your findings in the homework if you are interested, although it is not necessary.
post801: Hello, There are 2 data files for Homework 1 (2C.dat and 2C_weka.arff). Does it matter which file we use to read data? I used the weka.arff file since labeling data was easier but later saw that in the "Homework Submission Rules" post, 2C.dat was used for the path example. So I was wondering if it matters which file we use? Thank you!
post801-comment1-reply2: @71. If they are the same data set it should not matter which one is used as long as you include it in your github submission and the relative paths work.
post801-comment2: Agree.Choose the most suitable one 
post802: Would it be better to create separate environments for each HW or is it ok to have one environment for the class?
post802-comment1-reply2: I don&#39;t really know what you mean by &#34;environment&#34;, is it like a IDE specific thing?<div><br /></div><div>I guess you can do whatever you like, but each HW is basically just notebook&#43;data</div>
post802-comment2: i guess i meant like for when we do conda create ... I usually just do one environment for a class but on the intro zoom video it was mentioned that we can create one environment for each HW 
post802-comment2-reply1: ah conda i see, i dont use conda so don't have much input, but yeah follow whatever the zoom video says you should be in good hands!
post802-comment3-reply1: I think it is up to you. Keeping a clean environment for each homework is not essential for the submission since the requirements.txt file is optional according to @40. Also, each homework will probably need the same or many of the same packages, so if you keep separate environments you&#39;ll probably just end up installing the same packages every time for each homework. But it could be good to practice creating environments for each homework since having separate environments for different projects is typically considered best practice.
post803: Where and the Assignment 1 link and definition files? 
post803-comment1-reply1: Not really understanding what you need, but everything about HWs are in @10
post803-comment2-reply1: Professor will share the GitHub rink for the hw1 later after the hw0 I guess. And for the file such as template or dataset, you can download from the shared Dropbox 
post804: Hi Class, As I promised last week, we will have two consecutive lectures today and both classes will have joint sessions. The first lecture will be 12-1:50 in SGM and on DEN. The second lecture will be 3:30-5:20 in THH and on DEN. If you are planning to attend the second lecture, you must attend the first lecture. Both lectures will be recorded and made available on DEN. Regards, M R Rajati
post804-comment1: SGM 123 and THH 201.
post804-comment2: Will they be made available to attend online?
post804-comment2-reply1: Yes. -------------------------------------------------------------- Mohammad Reza Rajati, PhD Senior Lecturer, Thomas Lord Department of Computer Science University of Southern California 
post804-comment2-reply2: Will the link work on DEN work for both lectures? (I am an on campus student) I had just woke up so I cannot make the noon in person.
post804-comment2-reply3: I have a class during the noon session Would it be better to just watch the recordings in order or would attending the 2nd lecture and then watching the first be fine?
post804-comment2-reply4: I see now there is a new link for the joint lecture. I assume it will work for both classes?
post804-comment2-reply5:  Yes, Both classes will be available to all students. M R Rajati’s phone 
post804-comment2-reply6:  Just watch the recordings as these are related lectures. M R Rajati’s phone 
post804-comment3: Has the 12 noon Zoom meeting started? DSCI 552 - Rajati (Joint Meeting for 9/6)
post804-comment3-reply1: Yes use the link before that one, DSCI 552 - Rajati Virtual Link
post804-comment3-reply2:  You must use the usual link. The second link is for afternoon. M R Rajati’s phone 
post805: Hello,Could you please confirm the timings and the rooms for the two joint sessions today? Thanks 
post805-comment1-reply2: @93
post806: Hi, I pushed the HW0 Jupyter Notebook to the submission link via GitHub Desktop. And what I got is this. Could you help to confirm whether this is a valid submission? After this step, are no more actions required? Thanks! 
post806-comment1-reply2: The data set is not included in repo so the TAs would have issues running your notebook (not for hw0 since it&#39;s not graded but that would be a problem for future hw). You can clone the repo to a different location on your computer and try to run all as a way to verify that the repo has everything that&#39;s needed for grading.
post806-comment1-reply2: This is good
post806-comment2-reply2: Close but not yet, remember to follow the file structure in <a href="https://piazza.com/class/lll6cacyxjfg3?cid=40"></a><a href="https://piazza.com/class/lll6cacyxjfg3?cid=40"></a><a href="https://piazza.com/class/lll6cacyxjfg3?cid=40"></a><a href="https://piazza.com/class/lll6cacyxjfg3?cid=40">@40</a><div><br /></div><div>(Also its DSCI not INF anymore, and its a HW not a lab, but thats just the minor details)</div>
post806-comment2-reply2: Close but not yet, remember to follow the file structure in <a href="https://piazza.com/class/lll6cacyxjfg3?cid=40"></a><a href="https://piazza.com/class/lll6cacyxjfg3?cid=40"></a><a href="https://piazza.com/class/lll6cacyxjfg3?cid=40">@40</a><div><br /></div><div>(Also its DSCI not INF anymore, and its a HW not a lab, idk where you are getting that from kinda sus)</div>
post806-comment2-reply2: Close but not yet, remember to follow the file structure in <a href="https://piazza.com/class/lll6cacyxjfg3?cid=40"></a><a href="https://piazza.com/class/lll6cacyxjfg3?cid=40">@40</a><div><br /></div><div>(Also its DSCI not INF anymore, idk where you are getting that from kinda sus)</div>
post806-comment2-reply2: Close but not yet, remember to follow the file structure in <a href="https://piazza.com/class/lll6cacyxjfg3?cid=40">@40</a>
post806-comment2-reply2: Close but not yet, remember to follow @40
post806-comment3: The template for Homework 0 from Dropbox is named INF-552 Lab0.ipynb. Is there a preferred file name for submission?
post806-comment3-reply1: Please refer to @40 bullet point 6
post806-comment3-reply2: Also bullet point 2 in @40 has the instructions for the desired directory, including relative paths to the data set included within the repo.
post807: The 330pm section is still on as per usual today, correct?
post807-comment1-reply2: @93
post808: For（ b） i. & ii., since there are six independent variables, are we supposed to get six subplots, or we need to make it into one plot?
post808-comment1-reply2: I think it should be a pairwise plot.
post808-comment2-reply2: @73
post809: Hi, I would like to make sure in regards to this "In this exercise, we only focus on a binary classification task NO=0 and AB=1". Is that mean that we dropped other classifications (DH, SL) from the data? Thank you, 
post809-comment1-reply2: <p>@71 </p> <p>Yes, the 2C datasets you are using have only NO or AB as labels</p>
post810:  Hello, Are we supposed to import all the files in the Vertebral Column Data Set? Thx,
post810-comment1-reply2: @71
post811: Are there limits on what packages we're allowed to use in assignments? I see there's an arff package that can be used to import arff data into python, but it's not something that's been mentioned before, and I don't know which of the mentioned packages can be used to do this, if any.
post811-comment1-reply2: You can use the arff package to import your data. That should be fine
post812: As we are allowed to finish our homework with Colab, I get some questions about this. Do I need to specify that I finish it with Colab? And can I expect that my code will be tested with Colab as well? And what is the proper way to import data? Currently, I'm using google.drive package to mount the workspace to my google drive and access the data by relative path. But this means the data file has to be uploaded to the google drive, which I'm not sure if TA will do so when testing. Any suggestion?
post812-comment1-reply2: Just keep ur notebook as how you have finished it, remember to always RUN ALL before submitting as instructed
post813: Hello, I am not sure how to import data using a relative path. Should we only use a relative path when submitting the homework, or can we use absolute path to finish the homework on our laptops?
post813-comment1-reply2: <p>Put things in the same folder or different folder, then use a path like following</p> <p></p> <p>same folder -&gt; &#34;./filename.arff&#34;</p> <p>a folder inside your code folder -&gt; &#34;./foldername/filename.arff&#34;</p> <p>a folder outside your code folder -&gt; &#34;../folderwithcode/foldername/filename.arff&#34;</p> <p></p> <p>hope this helps!</p>
post813-comment1-reply2: <p>Put things in the same folder or different folder, then use a path like following</p> <p></p> <p>same folder -&gt; &#34;./filename.arff&#34;</p> <p>a folder inside your code folder -&gt; &#34;./foldername/filename.arff&#34;</p> <p>a folder outside tour code folder -&gt; &#34;../folderwithcode/foldername/filename.arff&#34;</p> <p></p> <p>hope this helps!</p>
post813-comment1-reply2: <p>Put things in the same folder, then use a path like following</p> <p></p> <p>same folder -&gt; &#34;./filename.arff&#34;</p> <p>a folder inside your code folder -&gt; &#34;./foldername/filename.arff&#34;</p> <p>a folder outside tour code folder -&gt; &#34;../folderwithcode/foldername/filename.arff&#34;</p> <p></p> <p>hope this helps!</p>
post813-comment2-reply2: To be more precious on top of the student answer,<div><br /></div><div>&#39;.&#39; means from the perspective of the current folder,</div><div>&#39;..&#39; means from the perspective of the parent folder,</div><div>&#39;../..&#39; is grand parent folder and so on</div>
post813-comment3: Thanks. But can the paths to be different when doing the homework on our laptops and when uploading the files to github?
post813-comment3-reply1: Its relative path, so its relative to the notebook file, so it should be the same
post814: If our training set is size N, what should we select as our testing set? Is the testing data all other data points other than those selected for the training set (and thus changes depending on what N is) or is it a fixed set? If the latter, which specific data points are we selecting?
post814-comment1-reply1: You can just use the same test set as you have constructed in the previous questions
post815: According to Dr. Rajati's recent lecture, it seems k around 11~50 is all looking suitable. in this case would you recommand use highest k value or just pick the least error point (in this plot k=16)? Thank you.! 
post815-comment1-reply1: @36
post815-comment2: then it would be k=3? but it doesn't align with lectures, though. Thank you very much.
post815-comment2-reply1: well its always a no free lunch situation, there is never a definite number for all problems
post815-comment2-reply2: You will learn more robust methods of testing your models later in the course, but given what you've learned so far, I’m sure you can come up with a defensible justification for k=3! Just think about the tradeoffs you are making wrt the lecture materials as you adjust the parameter of k. edited for clarity. 
post815-comment2-reply3: Can you elaborate more about a justification for k regarding bias variance trade off? it has 6 dimension data so I thought taking only small k values will end up less meaningful training (which mentioned by Dr. Rajati) or it is fine because it is classification problem?
post815-comment2-reply4: Don’t worry too much about that for now, just go with a good k given what you’ve learned in the lectures so far. Just remember as Changxun said, there is no free lunch. There are trade offs to everything.
post816: Part (f) of the homework asks us to report the lowest training error rate. Is this a typo? I thought reporting the lowest testing error rate would make more sense.
post816-comment1-reply4: not a typo @24
post817: The majority polling decision can be replaced by weighted decision, in which theweight of each point in voting isinversely proportionalto its distance from thequery/test data point. In this case, closer neighbors of a query point will havea greater influence than neighbors which are further away. Use weighted votingwith Euclidean, Manhattan, and Chebyshev distances and report the best testerrors whenk∈ {1,6,11,16, . . . ,196}. I guess this is yet to be taught buy can anyone explain !!! this, as in what exactly we have to do.
post817-comment1-reply4: The voting method decides how the prediction is made, the simplest majority voting just means that the prediction is whichever result that has the most occurrence (votes) in the neighbors.<div><br /></div><div>The question is asking you to try out other weighted voting methods, do you own research on what the weights listed means and implement them as required.</div>
post817-comment2: so for this do I need to create KNN from scratch or something because sklearn doesn't give a direct param input for this
post817-comment2-reply1: Are you sure it doesn't? Always read through the documentation!https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html
post817-comment2-reply2: I did it without creating KNN from scratch. I suggest looking up libraries that implements KNN for you and which parameters they take.
post817-comment2-reply3: but of course you are always welcome to implement it yourself for extra practice
post817-comment2-reply4: my bad 😬 missed that part !! Thanks though
post818:  Do we need to write k-nearest neighbors code from scratch for this particular question & then use sklearn KNN for remaining questions OR can we use sklearn KNN for this & remaining questions? Thanks!
post818-comment1-reply4: You can do either
post818-comment2-reply4: I used sklearn KNN for this &amp; remaining questions
post818-comment3: For training and testing set do we use what we defined on b)iii) or a new test-train set?
post818-comment3-reply1: use the same unless instruction says otherwise
post819: Hi, I created a new github account with usc email and created a github classroom repo for HW0. But I already had a github account (not usc email), but after creating one with (usc email) it seems like I have to consistently change auth credentials to use both accounts. I was wondering if I can use my original (non usc email) github account starting with HW1. Thank you!
post819-comment1-reply1: It doesn&#39;t matter if you github is usc email or not, just make sure to use the same one you put on the github username sheet @41
post820: I am trying to create data frame from dictionary(in it keys are headers such as 'yearID' and values are a list of column values). But when I use the Dataframe() it says cannot assign columns when orient="columns". I cannot find another way to assign headers at the time of creating dataframe from a dictionary.
post820-comment1-reply1: <p>From your description, it seems like you do not need to use the &#34;orient=columns&#34; argument. Have you tried creating the data frame without using it?</p> <p></p> <p>Shawn: Have you looked into from_dict()? https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_dict.html</p>
post820-comment1-reply1: From your description, it seems like you do not need to use the &#34;orient=columns&#34; argument. Have you tried creating the data frame without using it?
post820-comment2: Yeah. But that assigns column values as row values.
post820-comment2-reply1: you said dictionary where keys are headers, and value are the column values, that is exsactly what from_dict does by default, check out the first example in the documentation
post820-comment2-reply2: I know. K am using that method but then I need to rename headers in a separate step not while.creating the dataframe.
post821: Does it mean plotting by 1dimensional or pairwise scatters (using seaborn)? or ? Thank you.
post821-comment1-reply2: It should be a pairwise plot, you can use any popular library
post821-comment1-reply2: It should be a pairwise plot, you can use any library
post821-comment1-reply2: It should be a pairwise plot
post821-comment2: which seemed the later question part(c) ii understood as pairwise train and test error plots. Am I understanding correctly (total 6C2 = 30 plots)? Again, thank you very much.
post821-comment2-reply1: c.ii is just two plots, one for train and one for test
post821-comment2-reply2: I put train and test in c.ii in one plot and included a legend, is that ok?
post821-comment2-reply3: yeah sure thats fine!
post821-comment2-reply4: So then by just doing KNN from .fit(x_tr, y_tr) then .predict() method of scikit library, is it using KNN of all 6 features ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis'] of Euclidean space? Meaning, computing SQRT(a^2+b^2+c^2+d^2+e^2+f^2) then fit from 6D geometry? Thank you for your elaboration. I wasn't sure I'm fully understood that Scikit does everything for me with some simple codes.
post821-comment2-reply5: well first of all you should be plotting in terms of k not 1/k. As for your question, yes you are asked to train KNN with all six features, and the error rate is just as @70 explains, 1-accuracy
post822:  For part c of HW 1, it says to "plot train and test errors in terms of k for k ∈ {208, 205, . . . , 7, 4, 1, } (in reverse order)". However, my test dataset only has 100 samples, therefore I cannot use KNN classification for k>100. How should we address this issue? 
post822-comment1-reply5: Use try and except block and where the Knn doesn&#39;t work set error to 1, otherwise for all other k value let it calculate the k value and then use that to plot the graph with respect to K values.
post822-comment2: Nevermind, I figured out what I was doing wrong, but thank you anyway! As a follow-up question, is the method "score" in sk-learn a sufficient way to get the train and test errors for different k's?
post822-comment2-reply1: Personally I didn't use score I calculated everything using confusion matrix and ravel() function.
post822-comment2-reply2: @70, the error is 1-accuracy score, and accuracy score has a function in sklearn
post822-comment2-reply3: What did u figure out though may I know like I had the same issue!!!.
post822-comment3-reply3: You are not training on the test set, you should train on the train set, which has 210 samples in total
post823: Hi, The vertebral column dataset meant for HW1 has 2 datasets, 2c and 3c. Which one are we supposed to use? Or are we required to merge these both for the HW? Thanks for your time!
post823-comment1-reply3: For hw1 we only require 2C data,as of now 3C is not required and there is no need to merge the data.
post823-comment2: We can use 2C data.
post823-comment3: Got it! Thank you for responding!
post823-comment4: As others have pointed out, I believe we are suppose to use 2C. When it comes to .dat and weka.arff, choose whichever you want. However, for me working with the weka.arff was easier.
post824: Hi,I wanted some clarification, does the confusion matrix itself already contain the true negative & true positive values? (NVM I found this that confirms my question, going to share it if other students would like to see it, https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#:~:text=By%20definition%20a%20confusion%20matrix,positives%20is%20C%200%20%2C%201%20.) Also for question Cii, it asks us to "Plot train and test errors in terms of k for k∈ {208,205, . . . ,7,4,1,}(in reverse order)", what does is mean??What are the test and train errors? Is that similar to false positive and false negative?Any explanation would be appreciated!Thank you for your time
post824-comment1-reply3: It means plot the Test / Train Error on the Y - axis and the k values in descending order on the x axis. The Test / Train error is 1 - accuracy where accuracy is TP &#43; TN / (TP &#43; TN &#43; FP &#43; FN) otherwise known as correct predictions / total number of predictions 
post824-comment1-reply3: You can use ravel() function in confusion metrics to get all tp,tn,fp,fn. Then calculate accuracy and error.
post824-comment2: Are the errors considered the False positives and false negatives?
post824-comment2-reply1: Thank you!
post824-comment2-reply2: Accuracy (all correct / all) = TP + TN / TP + TN + FP + FNMisclassification (all incorrect / all) = FP + FN / TP + TN + FP + FNPrecision (true positives / predicted positives) = TP / TP + FPSensitivity aka Recall (true positives / all actual positives) = TP / TP + FNSpecificity (true negatives / all actual negatives) =TN / TN + FP
post824-comment2-reply3: I think u forgot some ( )s here :))
post824-comment2-reply4: 🥲 realized it later
post824-comment2-reply5: Accuracy (all correct / all) = (TP + TN) / (TP + TN + FP + FN) Misclassification aka Error (all incorrect / all) = (FP + FN )/ (TP + TN + FP + FN)Precision (true positives / predicted positives) = TP / (TP + FP)Sensitivity aka Recall (true positives / all actual positives) = TP / (TP + FN)Specificity (true negatives / all actual negatives) =TN / (TN + FP)
post824-comment3-reply5: 1. As you have already found with the documentation, the confusion matrix does indeed contain such information! But if you want other things such as rates then further calculation is needed.<br />2. Test and train errors are just (1 - accuracy score) since this is a classification problem. In terms of the plotting, you will want k to be on the x-axis and your error to be on the y-axis. Just plot it in the regular small-to-large order, since it is the reverse of k∈ {208,205, . . . ,7,4,1,} (IK the wording here might be confusing)
post824-comment4: I also have similair but more fundamental question while I was rewatching the recs of August 28. I do have an idea of that Tr data called "training" per se which you'd get from the real world. But can anyone elaborate what is "Test Data" in general terms and why they are becoming bigger MSEs along with more features? Thank you! 
post824-comment4-reply1: Test data as the name implies, is used to test the accuracy of your model. You cannot just use the training data you initially used to generate your model for testing because that is inaccurate, test data is essentially some data that your model has not seen before so it is fair to assess the accuracy with it.
post824-comment4-reply2: Thanks, Changxun, I understood test data mean now. Then would test data usually real data or random manipulated data? and how does it intpreted as bigger the MSE than the ones from Training Data, intuitively? Thank you so much.
post824-comment4-reply3: Testing data mostly a part of the full dataset itself, it can be manipulated data also but since manipulating data takes time we split the training data into parts of 80-20 where 20% is the testing data. This ratio can change as per the size of data. Even if your model has a good MSE value it is not necessary that the testing data will have the same as testing data helps us predicts if the model over fitted or underfitted on the training set. If the MSE of testing data set is larger the training data you might be able to understand by plot the graph for y_pred values vs the y_test values, this might give you a better knowledge about how your model performed.
post824-comment4-reply4: Intuitively, it makes sense for you model to recognize something that it has seen before (train) better than something is has not (test)
post825: I can see the (a) Download the Vertebral Column Data Set from: ~~ in the homework1 statement. But, I could get the data from the homework dropbox. Should I make a code for this separately in the jupyter notebook? If not, can I start the code from (b)? Thank you in advance.
post825-comment1-reply4: You can just download the data from the dropbox and import manually (this code should still be in part a), but if you want to get it from the web thats fine too
post825-comment1-reply4: You can just download the data from the dropbox and import manually, but if you want to get it from the web thats fine too
post826: Hello all, Due to a conflicting training I cannot hold my office hours this week. So sorry for the late notice! I hope you all have a wonderful labor day weekend. Grace
post827: I watched tutorials by Woojeong for a couple of days. and thank you very much. However, it seems to me that manually uploading homeworks to repositary of my Github via website is way simpler method to me. Is it okay (as long as follow the submission rules)? Thanks.
post827-comment1-reply4: That is ok, but we highly encourage you to learn how to use git since it is essential for future jobs and projects that you might have
post828: Do we need to use the HW template for each homework? Thanks!
post828-comment1-reply4: No you can use your own, they are just there for your and the grader&#39;s convenience.<div><br /></div><div>But if you do decide to use your own, please clearly label each question.</div>
post828-comment2: what template are you referring to?
post828-comment2-reply1: @10
post829: Hi I finished the homework0 and git pushed the data directory and notebook directory. In my notebook directory, I only had HW0.ipynb file pushed but see that there is also HW0-checkpoint.ipynb file in this directory in GitHub that I did not push.. Is this supposed to be there?
post829-comment1-reply1: Jupyter creates a checkpoint file every-time for backup purposes, since the file is hidden on your desktop you can&#39;t see it (unless you turn on visibility ofc).<div><br /></div><div>It&#39;s fine either way, if you wanna remove it or not.</div>
post830: Do we need the column of 1s in B1 if we have B0? I understand that B0 has to do with the y intercept in linear regression and so does the column of 1s in B1, my question is do we need both? Or if when we have B0 then we don't have the column of 1s in B1 / when we have the column of 1s in B1 then we don't need B0? 
post830-comment1-reply1: what do you mean by column of 1s here? Can you give an example
post830-comment2: IMG_4479_3.HEIC In my old linear algebra book they write y = X B where X is the design matrix with a column of 1s and B = [ B0 and B1 ] . 
post830-comment2-reply1: I guess I'm asking which way we will be writing regression, as y = B0 + X * B1 or y = B * x where B = [B0 , B1] and X, the design matrix, has a column of 1s
post831: Hi, can someone please help me understand how the variance of epsilon is formed here as discussed in lecture. I know the variance formula which is the differences between each number in the data set and the mean. Would be great if someone can explain it please. Thanks!
post831-comment1-reply1: <p>As you mentioned, one equation for variance is the average squared difference between each number in the data set and the mean. I think the equation that you are referring to is something like this:</p> <p><img src="https://d4y70tum9c2ak.cloudfront.net/contentImage/f1xr7mgXgAxHoy9D6TKL9HHx7W6LJF_xG%2BFzrqftA5E/resized.png" width="160" height="65" /></p> <p>In the case of the lecture, epsilon is a random variable instead of a data set. Consequently the variance of epsilon can be treated as an expected value, which is described by the following formula:</p> <p></p> <p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ad35c4161b9cf52868e879d457d8d796094ff02" /></p> <p></p> <p>The last bit of the derivation is the definition that the professor used in the lecture.</p>
post831-comment1-reply1: <p>As you mentioned, one equation for variance is the average squared difference between each number in the data set and the mean. I think the equation that you are referring to is something like this:</p> <p><img src="https://d4y70tum9c2ak.cloudfront.net/contentImage/f1xr7mgXgAxHoy9D6TKL9HHx7W6LJF_xG%2BFzrqftA5E/resized.png" width="160" height="65" /></p> <p>In the case of the lecture, epsilon is a random variable instead of a data set. It can be treated as an expected value, which is described by the following formula:</p> <p></p> <p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ad35c4161b9cf52868e879d457d8d796094ff02" /></p> <p></p> <p>The last bit of the derivation is the definition that the professor used in the lecture.</p>
post831-comment2: But as per your calculation, it should have ben E[X2] + E[X]2 right? @Anonymous Poet
post831-comment2-reply1: $$ E[X^2]-E[X]^2 $$ is correct. Or in the case of lecture, $$ Var(\varepsilon)=E[\varepsilon^2]-E[\varepsilon]^2 $$. To expand a bit further on the formula derivation above: $$ Var(X)=E[(X-E[X])^2]=E[X^2-2XE[X]+E[X]^2]=E[X^2]-2E[X]E[X]+E[X]^2=E[X^2]-2E[X]^2+E[X]^2=E[X^2]-E[X]^2 $$
post831-comment2-reply2: Got it! Thank you :)
post831-comment3-reply2: <p dir="ltr">or $$ Var(X)=Cov(X, X) = E[X * X] - E[X] * E[X] = E[X^2]-E[X]^2 $$, notice that $$Var(X) \geq 0$$ by Jensen’s Inequality</p>
post831-comment3-reply2: <p dir="ltr">or $$ Var(X)=Cov(X, X) = E[X * X] - E[X] * E[X] = E[X^2]-E[X]^2 $$, notice that Var(X) $$\geq$$ 0 by Jensen’s Inequality</p>
post831-comment3-reply2: <p dir="ltr">or $$ Var(X)=Cov(X, X) = E[X * X] - E[X] * E[X] = E[X^2]-E[X]^2 $$, notice that Var(X) \geq 0 by Jensen’s Inequality</p>
post831-comment3-reply2: or $$ Var(X)=Cov(X, X) = E[X * X] - E[X] * E[X] = E[X^2]-E[X]^2 $$
post831-comment3-reply2: or $$ Var(X)=COV(X, X) = E[X * X] - E[X] * E[X] = E[X^2]-E[X]^2 $$
post832: Do we have 2 lectures today? 
post832-comment1-reply2: Hi! If you are talking about the joint session, professor mentioned that it will be on Wednesday after Labor day.
post833: Hi all, due to technical delays, I cannot hold my Office Hours this week (Thursday, 08/31/23 - 2 PM - 4 PM). Sorry for the inconvenience! Regards, SG
post834: Hi! I don't have my own laptop (have been using a tablet for school) and so I typically use the USC virtual desktop for any software needs. Fortunately it has anaconda installed, however it looks like you need admin privledge to edit the path environment variables on homework 0. I am fairly new to coding/ using anaconda and was wondering if not being able to set up the path environment would affect my ability to submit future homeworks/projects. Thank you!
post834-comment1: Maybe try using google collab instead, that way you shouldn't need to struggle with any kind of setup
post834-comment2-reply2: You can setup virtual Linux desktop where you not needed to setup path variable and you can work on it normally. Though Google co lab is the best option.
post834-comment2-reply2: You can setup virtual Linux desktop where you not needed to setup path variable and you can work on it normally. Though Google co lab is the best option.
post834-comment2-reply2: You can setup virtual Linux desktop where you not need to setup path variable and you can work on it normally. Though Google co lab is the best option.
post834-comment3: Just to confirm, we can use google collab for our homeworks instead of setting up ide and environment on local system & then submit the notebook on github repo, correct?
post834-comment3-reply1: yes
post835: Hi everyone! I had registered for DSCI552 last Friday. Can anyone please direct me towards where can I access the HW problem statements? Appreciate your help. Thank you!
post835-comment1-reply1: <a href="https://piazza.com/class/lll6cacyxjfg3?cid=10">@10</a><div><br /></div><div>There are more posts with dropbox links that include slides, syllabus, exams, etc.</div>
post835-comment1-reply1: @10
post835-comment2: yes，all hw in @10
post835-comment2-reply1: Thanks for reaching out!
post836: Although hw0 is ungraded and given for us to practice, will we receive feedback on our submission? 
post836-comment1-reply1: HW0 is not graded thus no feedback will be given, but if you wish you can go to an OH after that has been published later.
post836-comment1-reply1: HW0 will not be submitted thus no feedback can be given, but if you wish you can go to an OH after that has been published later.
post837: Hello, Homework 0 introduced many ways to process and visualize/plot data. I was wondering if there is a specific method we should use for Homework 1 when we make scatterplots, boxplots, etc. ? (E.g. using Pandas vs Matplotlib vs Seaborn) I feel like each of them have their advantages/disadvantages and should be used where they best fit but I am not sure if we are required to use a specific one for the homework.
post837-comment1-reply1: There is no specific requirements, use whatever package you find convenient at that time! (If you decide to use some lesser known packages please remember to note that down too)
post837-comment1-reply1: There is no specific requirements, use whatever package you find convenient at that time!
post837-comment2-reply1: I&#39;ve been pondering the same topic, and it seems that the choice between using Matplotlib or Seaborn for plotting isn&#39;t all that crucial. I&#39;ve noticed that Seaborn often automates tasks like labeling and enhancing the visual appeal of plots, while achieving the same with Matplotlib would require writing more lines of code.
post838: Hello, I’m enrolled in DEN section of the class. I was wondering how will the exams be for DEN students? Thank you 
post838-comment1-reply1: <p>Based on my experience in past semesters, if you’re in the LA area you’ll have to come to campus to take the exam. If you’re not close to LA, you’ll have the option to come to campus<span style="text-decoration:line-through"> or find a proctor if that’s a better. The proctor will print the exam and then administer the test. DEN should be sending you an email in the next few weeks to set that up but here’s a link for what it would look like: https://denawstools.uscden.net/mydentools/students/denpetition/proctor.pdf</span></p> <p></p>
post838-comment1-reply1: <md>Based on my experience in past semesters, if you're in the LA area you'll have to come to campus to take the exam. If you're not close to LA, you'll have the option to come to campus or find a proctor if that's a better. The proctor will print the exam and then administer the test. DEN should be sending you an email in the next few weeks to set that up but here's a link for what it would look like: https://denawstools.uscden.net/mydentools/students/denpetition/proctor.pdf</md>
post838-comment2: Can the instructor please confirm the format for the midterms for DEN students? Thank you
post838-comment3-reply1: <p>DEN exams will be proctored on Zoom by a TA, a link will be sent before the exam</p> <p></p> <p>M R Rajati: DEN Students in LA area must attend the exams in person. Others will be proctored online. On campus students must take the exam in person and there&#39;s no exception to this rule whatsoever, even if the student has an emergency.</p>
post838-comment3-reply1: DEN exams will be proctored on Zoom by a TA, a link will be sent before the exam
post839: Hello everyone, I'm working on HW1. In Question c-ii, if we get multiple k values for which we are getting the minimum error. Can we take any one of them to be k* ?
post839-comment1-reply1: You can list all of them if you are not sure. You can select the smallest K &gt; 1, if the minimum errors are matching. 
post840: I just registered for this class last Friday. The class doesn't appear on my den yet. Where can I watch the recording of this class?
post840-comment1-reply1: @22, @29
post840-comment2-reply1: Maybe you can&#39;t see the class on your DEN because you just registered the class, you can wait a little bit or you can contact DEN to ask. dentsc@usc.edu.
post841: As new to any type of coding, I guess HW0 teaches getting basics about data processing using Python. I was able to do upto iterrows() part and spend so much time on understanding basics but it would be much effiecient to get solution code so that I can reverse-engineer the codes. Since it's zero scoring anyone would like to share the solution? Thanks in advance.
post841-comment1:  The whole point is to do it on your own and because it is zero grade, efficiency is not of a big concern. M R Rajati’s phone 
post841-comment1-reply1: Thank you Professor. I will do my best.
post841-comment2-reply1: <p>Hey, you can still post questions !! I can help you in any way possible.</p> <p></p> <p>python: <a href="https://www.w3schools.com/python/default.asp" target="_blank" rel="noopener noreferrer">https://www.w3schools.com/python/default.asp</a></p> <p></p> <p>GitHub: <a href="https://education.github.com/git-cheat-sheet-education.pdf" target="_blank" rel="noopener noreferrer">https://education.github.com/git-cheat-sheet-education.pdf</a></p> <p></p> <p>for pandas, seaborn, and many other libraries look out for: <a href="https://www.kaggle.com/learn" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/learn</a></p> <p></p> <p>For setting up Python on the local machine:</p> <p>windows: <a href="https://medium.com/@jaipawar113/beginners-guide-to-virtual-environments-in-windows-using-miniconda-daef6a42063b" target="_blank" rel="noopener noreferrer">https://medium.com/@jaipawar113/beginners-guide-to-virtual-environments-in-windows-using-miniconda-daef6a42063b</a></p> <p>mac: <a href="https://arpitrana.medium.com/installing-miniconda-on-mac-osx-ac4557a715f7" target="_blank" rel="noopener noreferrer">https://arpitrana.medium.com/installing-miniconda-on-mac-osx-ac4557a715f7</a></p> <p></p> <p>The best option connect Github to Google co-lab.</p>
post841-comment2-reply1: Hey, you can still post questions !! I can help you in any way possible.
post841-comment3:  There you go… M R Rajati’s phone 
post841-comment4: Thanks! for infor. I'm just not sure i'm doing it right. 
post841-comment4-reply1: The playeID column is indexed and since it is you can't change the name for it unless you reset the index for df and then try using rename
post842: can someone let me know where to find Salaries.csv file that was mentioned in HW0 ?
post842-comment1-reply1: Hi! It is under the folder Homework 0 Data!
post842-comment2: Can not find the folder Homework 0 Data
post842-comment2-reply1: @10 scroll down
post843: Hello, I'm looking at the part of HW 0 that is about setting up a Github Repo. Is it recommended to do this in Google Collab or in a local environment? 
post843-comment1-reply1: You can do either, some students don&#39;t have a capable local environment so its better for them to use google collab.
post843-comment2: okay, thanks!
post844: As title.
post844-comment1-reply1: Hi! According to the Syllabus, they will be introduced here!
post844-comment2-reply1: We are still deciding on that since some of the CPs are still getting settled in! But we will likely have OH basically every hour of the week (business hours)
post844-comment2-reply1: We are still deciding on that since some of the CPs are still getting settled in! But we will likely have OH basically every hour of the week
post845: Hi, I can see the lecture recording posted on D2L for both lectures of this week but I am unable to play the content. It says that I don't have access to view this section. EDIT: Resolved by logging off and logging back in because sometimes D2L doesn't work if Blackboard was used to sign in earlier. "When you log back in, you should get access. This issue can be caused by being logged into a Blackboard account before logging onto D2L."
post845-comment1: Can you maybe try using Panopto directly?https://uscviterbi.hosted.panopto.com/
post845-comment1-reply1: Tried. This is the link https://uscviterbi.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=bb9cc7b0-8dda-4220-8966-b0650139e0eb 
post845-comment1-reply2: I don't think there is much I can do on my end? Maybe try reaching out to DEN staff, they usually respond pretty quick.
post845-comment1-reply3: I see! What helped me fix it was logging out of Panopto specifically as it turns out it doesn't work when you log into Blackboard first and Panopto next... "When you log back in, you should get access. This issue can be caused by being logged into a Blackboard account before logging onto D2L." D2L mentioned this somewhere. But thank you for the quick response. Helped in troubleshooting :P
post845-comment1-reply4: Lol the classic turn it off and back on again, didnt think this was even possible for DEN 😭
post845-comment1-reply5: xD
post845-comment2-reply5: <md>That is a common issue that they post a help article every semester. It seems like they haven't made the post for this semester yet so I'll attach a screenshot to the post from this summer. ![image.png](/redirect/s3?bucket=uploads&prefix=paste%2Fkdscium7ywh4i8%2Fbe1ed7233906ec6a3f5632b77f85cc5697d821cdab0590b515bc4d1a00a84677%2Fimage.png)</md>
post845-comment3: deleted
post845-comment3-reply1: lol yes xD
post846: Hi, I lost a black pencil bag with white but smudged patterns on the outside. It was lost after the THH 201 class yesterday near the first row of the rightmost seats. If anyone found it or left it in a lost-and-found box, please reply to this post. Any help in finding it is greatly appreciated!!! Thank you :)
post847: Hi there, Shiva here working on hw1. On 1f, is it ok to use the k = 1 trivial solution where the training error rate is 0? Or should I be analyzing the minimum from situations where k > 1 ? Cheers
post847-comment1-reply1: <p>That is ok :) But please also provide an explanation on why this is.</p> <p></p> <p>Also, it&#39;s ok to post publicly so students with similar problems can reference the post! If you do not want your name to show up you can enable anonymous posting.</p>
post847-comment2: Hi, To follow up, can you please clarify if it is okay to use k=1 and provide more explanation for it? Or did you mean if we should be analyzing values of k>1? Thanks.
post848: Hello, I remember hearing that it is a good idea to brush up on probability/stats last lecture. Should we also brush up on the basics of linear algebra? Thanks.
post848-comment1-reply1: Yea of course, be my guest!
post848-comment2-reply1: Don&#39;t forget that professor shared a lot of good resources via Dropbox. The material under &#34;Handouts&#34; was very useful for me to review both Linear Algebra and Probability.
post848-comment3: Following up on this, what about brushing up on calculus? It's mentioned in the prerequisite portion of the syllabus but the professor didn't highlight it like he did for linear algebra and probability.
post848-comment3-reply1: Calculus is basically the pre-req for anything math related, it is kinda assumed that you are already proficient, and if you are not then for sure review it. 
post849: I had trouble finding the recording of Monday lecture? Anyone knows where we can access it? 
post849-comment1-reply1: All recordings will be uploaded to DEN/D2L, when it is uploaded depends on the speed of the DEN staff, but usually its within a few days of the lecture
post850: 
post850-comment1-reply1: You can either<div>1. Talk to professor Rajati after class</div><div>2. Ask your questions on piazza</div><div>3. Attend OHs once we have that finalized. </div>
post850-comment2-reply1: <md>I believe he said in lecture yesterday to just walk up to him after class. If your questions are more than he can answer then, he said you can work something out.</md>
post850-comment2-reply1: <md>I believe he said in lecture last night to just walk up to him after class. If your questions are more than he can answer then, he said you can work something out.</md>
post851: I was going through a paper and was wondering what is the meaning of ℝ?
post851-comment1-reply1: <p>What paper/reading are you talking about, I am going to assume that R means &#34;real numbers&#34; as in mathematics we use that symbol to define real numbers.</p> <p></p> <p></p> <p>Real numbers include all positive and negative numbers (fractions, integers, whole numbers, zero, etc.)</p>
post851-comment1-reply1: What paper/reading are you talking about, I am going to assume that R means &#34;real numbers&#34; as in mathematics we use that symbol to define real numbers.
post851-comment1-reply1: What paper/reading are you talking about, I am going to assume that R means &#34;real numbers&#34; as in mathematics we use that symbol to define real numbers.
post851-comment2: It's either all real numbers as mentioned above or the name of a programming language. 
post851-comment3: I would like to add on that, in data science, we refer that as "dimension" as well. For example, there is a data set of points in $$R^2$$ that means there is a set of points that have 2 dimensions such as: (1,2) (3,0) (5,1) (-3,9)
post851-comment3-reply1: Good point! It is essentially the same meaning as all real numbers, since R2 is just a vector in 2 real dimensions (instead of 1 for R)
post852: Hi Guys, My name is Chung En Pan, or Shawn Pan. Currently majoring in Applied Data Science, I also graduated from UCSD as an undergraduate in Data Science. Feel free to contact me to discuss on hw, or just chat. Also looking for a work out buddie :) email: chungenp@usc.edu Wechat: Shawn088716 Line: shawn0880716
post852-comment1: you can also use @5 for teammate search
post852-comment1-reply1: I get a "post cannot be found" error when I click on that link, I think it may be private
post852-comment1-reply2: thanks for catching that, it should be available now!
post853:  Piazza is a Q&A platform designed to get you great answers from classmates and instructors fast. We've put together this list of tips you might find handy as you get started: Ask questions! The best way to get answers is to ask questions! Ask questions on Piazza rather than emailing your teaching staff so everyone can benefit from the response (and so you can get answers from classmates who are up as late as you are). Edit questions and answers wiki-style. Think of Piazza as a Q&A wiki for your class. Every question has just a single students' answer that students can edit collectively (and a single instructors’ answer for instructors). Add a followup to comment or ask further questions. To comment on or ask further questions about a post, start a followup discussion. Mark it resolved when the issue has been addressed, and add any relevant information back into the Q&A above. Go anonymous. Shy? No problem. You can always opt to post or edit anonymously. Tag your posts. It's far more convenient to find all posts about your Homework 3 or Midterm 1 when the posts are tagged. Type a “#” before a key word to tag. Click a blue tag in a post or the question feed to filter for all posts that share that tag. Format code and equations. Adding a code snippet? Click the pre or tt button in the question editor to add pre-formatted or inline teletype text. Mathematical equation? Click the Fx button to access the LaTeX editor to build a nicely formatted equation. View and download class details and resources. Click the Course Page button in your top bar to access the class syllabus, staff contact information, office hours details, and course resources—all in one place! Contact the Piazza Team anytime with questions or comments at team@piazza.com. We love feedback!